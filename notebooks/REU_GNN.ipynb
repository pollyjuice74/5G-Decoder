{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8gzotOWu7Hbzy9yK25I/d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/REU-LDPC-Project/blob/main/REU_GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main imports\n",
        "\n",
        "!pip install sionna\n",
        "import sionna as sn\n",
        "from sionna.utils import BitErrorRate, BinarySource\n",
        "from sionna.mapping import Mapper, Demapper\n",
        "from sionna.channel import AWGN\n",
        "from sionna.fec.ldpc import LDPCBPDecoder\n",
        "from sionna.fec.ldpc.encoding import LDPC5GEncoder\n",
        "from sionna.fec.ldpc.decoding import LDPC5GDecoder\n",
        "\n",
        "!pip install torch torch-geometric\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GPSConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3JuIWKhBDLou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 15\n",
        "k = 12\n",
        "\n",
        "enc = LDPC5GEncoder(k, n)\n",
        "dec = LDPC5GDecoder(enc)\n",
        "# print(dec.pcm)\n",
        "\n",
        "pcm, rm = generate_pruned_pcm_5g(dec, n)\n",
        "# pcm, rm\n",
        "enc.k_ldpc, enc.n_ldpc, enc.z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e610891-622b-4da3-b995-798038abf59a",
        "id": "0ODSgbdMPrwO"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using bg:  bg2\n",
            "# information bits: 12\n",
            "CW length after rate-matching: 15\n",
            "CW length without rm (incl. first 2*Z info bits): 20\n",
            "# punctured bits: 77\n",
            "# pruned nodes: 76\n",
            "# parity bits 8\n",
            "# shortened bits 8\n",
            "pruned pcm dimension: (8, 20)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 104, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = dec\n",
        "enc = decoder._encoder\n",
        "\n",
        "# transmitted positions\n",
        "pos_tx = np.ones(n)\n",
        "\n",
        "# undo puncturing of the first 2*z information bits\n",
        "pos_punc = np.concatenate([np.zeros([2*enc.z]),pos_tx], axis=0)\n",
        "\n",
        "# puncturing of the last positions\n",
        "# total length must be n_ldpc, while pos_tx has length n\n",
        "# first 2*z positions are already added\n",
        "# -> add n_ldpc - n - 2Z punctured positions\n",
        "k_short = enc.k_ldpc - enc.k # number of shortend bits\n",
        "num_punc_bits = ((enc.n_ldpc - k_short) - enc.n - 2*enc.z)\n",
        "pos_punc2 = np.concatenate(\n",
        "            [pos_punc, np.zeros([num_punc_bits - decoder._nb_pruned_nodes])])\n",
        "print(k_short, num_punc_bits,)\n",
        "\n",
        "# shortening (= add 0 positions after k bits, i.e. LLR=LLR_max)\n",
        "# the first k positions are the systematic bits\n",
        "pos_info = pos_punc2[0:enc.k]\n",
        "print(pos_info, )\n",
        "\n",
        "# parity part\n",
        "num_par_bits = (enc.n_ldpc-k_short-enc.k-decoder._nb_pruned_nodes)\n",
        "pos_parity = pos_punc2[enc.k:enc.k+num_par_bits]\n",
        "pos_short = 2 * np.ones([k_short]) # \"2\" indicates shortened position\n",
        "print(num_par_bits, pos_parity, pos_short)\n",
        "\n",
        "# and concatenate final pattern\n",
        "rm_pattern = np.concatenate([pos_info, pos_short, pos_parity], axis=0)\n",
        "print(rm_pattern)\n",
        "\n",
        "# and prune matrix (remove shortend positions from pcm)\n",
        "pcm_pruned = np.copy(decoder.pcm.todense())\n",
        "idx_short = np.where(rm_pattern==2)\n",
        "idx_pruned = np.setdiff1d(np.arange(pcm_pruned.shape[1]), idx_short)\n",
        "pcm_pruned = pcm_pruned[:,idx_pruned]\n",
        "num_shortened = np.size(idx_short)\n",
        "\n",
        "print()\n",
        "for i in pcm_pruned:\n",
        "  print(i)\n",
        "\n",
        "print()\n",
        "for i in decoder.pcm:\n",
        "  print(i)\n",
        "\n",
        "rm_pattern[idx_pruned],\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSY4v_LhWbao",
        "outputId": "fb5879a3-c7fa-49d7-f6f9-4d998c0cb6b3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 77\n",
            "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "8 [1. 1. 1. 1. 1. 1. 1. 0.] [2. 2. 2. 2. 2. 2. 2. 2.]\n",
            "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0.]\n",
            "\n",
            "[0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "[1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "[1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "[0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
            "[1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
            "[0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "[0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "\n",
            "  (0, 1)\t1.0\n",
            "  (0, 3)\t1.0\n",
            "  (0, 4)\t1.0\n",
            "  (0, 6)\t1.0\n",
            "  (0, 13)\t1.0\n",
            "  (0, 19)\t1.0\n",
            "  (0, 20)\t1.0\n",
            "  (0, 22)\t1.0\n",
            "  (0, 0)\t1.0\n",
            "  (0, 2)\t1.0\n",
            "  (0, 5)\t1.0\n",
            "  (0, 7)\t1.0\n",
            "  (0, 12)\t1.0\n",
            "  (0, 18)\t1.0\n",
            "  (0, 21)\t1.0\n",
            "  (0, 23)\t1.0\n",
            "  (0, 1)\t1.0\n",
            "  (0, 6)\t1.0\n",
            "  (0, 9)\t1.0\n",
            "  (0, 11)\t1.0\n",
            "  (0, 12)\t1.0\n",
            "  (0, 14)\t1.0\n",
            "  (0, 16)\t1.0\n",
            "  (0, 18)\t1.0\n",
            "  (0, 22)\t1.0\n",
            "  (0, 24)\t1.0\n",
            "  (0, 0)\t1.0\n",
            "  (0, 7)\t1.0\n",
            "  (0, 8)\t1.0\n",
            "  (0, 10)\t1.0\n",
            "  (0, 13)\t1.0\n",
            "  (0, 15)\t1.0\n",
            "  (0, 17)\t1.0\n",
            "  (0, 19)\t1.0\n",
            "  (0, 23)\t1.0\n",
            "  (0, 25)\t1.0\n",
            "  (0, 1)\t1.0\n",
            "  (0, 2)\t1.0\n",
            "  (0, 6)\t1.0\n",
            "  (0, 8)\t1.0\n",
            "  (0, 16)\t1.0\n",
            "  (0, 21)\t1.0\n",
            "  (0, 24)\t1.0\n",
            "  (0, 26)\t1.0\n",
            "  (0, 0)\t1.0\n",
            "  (0, 3)\t1.0\n",
            "  (0, 7)\t1.0\n",
            "  (0, 9)\t1.0\n",
            "  (0, 17)\t1.0\n",
            "  (0, 20)\t1.0\n",
            "  (0, 25)\t1.0\n",
            "  (0, 27)\t1.0\n",
            "  (0, 2)\t1.0\n",
            "  (0, 4)\t1.0\n",
            "  (0, 8)\t1.0\n",
            "  (0, 10)\t1.0\n",
            "  (0, 13)\t1.0\n",
            "  (0, 14)\t1.0\n",
            "  (0, 16)\t1.0\n",
            "  (0, 18)\t1.0\n",
            "  (0, 20)\t1.0\n",
            "  (0, 26)\t1.0\n",
            "  (0, 3)\t1.0\n",
            "  (0, 5)\t1.0\n",
            "  (0, 9)\t1.0\n",
            "  (0, 11)\t1.0\n",
            "  (0, 12)\t1.0\n",
            "  (0, 15)\t1.0\n",
            "  (0, 17)\t1.0\n",
            "  (0, 19)\t1.0\n",
            "  (0, 21)\t1.0\n",
            "  (0, 27)\t1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 0.]),)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKuT2fT4WPmL"
      },
      "outputs": [],
      "source": [
        "# GENERAL NOTES #\n",
        "\n",
        "# Turn pcm into a graph using Pytorch Geometric\n",
        "\n",
        "# Have the edges describe relations between cn and vn\n",
        "  # Also have other edges for relation between nodes that are not necessarily directly linked for Transformer model\n",
        "\n",
        "# Message Passing embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "# Cell includes: E2Emodel, DecoderGNN, PCM_generator #\n",
        "######################################################\n",
        "\n",
        "\n",
        "class E2EModel(tf.keras.Model):\n",
        "    \"\"\"End-to-end model for (GNN-)decoder evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    encoder: Layer or None\n",
        "        Encoder layer, no encoding applied if None.\n",
        "\n",
        "    decoder: Layer or None\n",
        "        Decoder layer, no decoding applied if None.\n",
        "\n",
        "    k: int\n",
        "        Number of information bits per codeword.\n",
        "\n",
        "    n: int\n",
        "        Codeword lengths.\n",
        "\n",
        "    return_infobits: Boolean\n",
        "        Defaults to False. If True, only the ``k`` information bits are\n",
        "        returned. Must be supported be the decoder as well.\n",
        "\n",
        "    es_no: Boolean\n",
        "        Defaults to False. If True, the SNR is not rate-adjusted (i.e., Es/N0).\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "        batch_size: int or tf.int\n",
        "            The batch_size used for the simulation.\n",
        "\n",
        "        ebno_db: float or tf.float\n",
        "            A float defining the simulation SNR.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        (c, llr):\n",
        "            Tuple:\n",
        "\n",
        "        c: tf.float32\n",
        "            A tensor of shape `[batch_size, n] of 0s and 1s containing the\n",
        "            transmitted codeword bits.\n",
        "\n",
        "        llr: tf.float32\n",
        "            A tensor of shape `[batch_size, n] of llrs containing estimated on\n",
        "            the codeword bits.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, k, n, return_infobits=False, es_no=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self._n = n\n",
        "        self._k = k\n",
        "\n",
        "        self._binary_source = BinarySource()\n",
        "        self._num_bits_per_symbol = 2\n",
        "        self._mapper = Mapper(\"qam\", self._num_bits_per_symbol) #\n",
        "        self._demapper = Demapper(\"app\", \"qam\", self._num_bits_per_symbol) #\n",
        "        self._channel = AWGN() #\n",
        "        self._decoder = decoder\n",
        "        self._encoder = encoder\n",
        "        self._return_infobits = return_infobits\n",
        "        self._es_no = es_no\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def call(self, batch_size, ebno_db):\n",
        "\n",
        "        # no rate-adjustment for uncoded transmission or es_no scenario\n",
        "        if self._decoder is not None and self._es_no==False:\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, self._k/self._n)\n",
        "        else: #for uncoded transmissions the rate is 1\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, 1)\n",
        "\n",
        "        b = self._binary_source([batch_size, self._k])\n",
        "        if self._encoder is not None:\n",
        "            c = self._encoder(b)\n",
        "        else:\n",
        "            c = b\n",
        "\n",
        "        # check that rate calculations are correct\n",
        "        assert self._n==c.shape[-1], \"Invalid value of n.\"\n",
        "\n",
        "        # zero padding to support odd codeword lengths\n",
        "        if self._n%2==1:\n",
        "            c_pad = tf.concat([c, tf.zeros([batch_size, 1])], axis=1)\n",
        "        else: # no padding\n",
        "            c_pad = c\n",
        "        x = self._mapper(c_pad)\n",
        "\n",
        "        y = self._channel([x, no])\n",
        "        llr = self._demapper([y, no])\n",
        "\n",
        "        # remove zero padded bit at the end\n",
        "        if self._n%2==1:\n",
        "            llr = llr[:,:-1]\n",
        "\n",
        "        # and run the decoder\n",
        "        if self._decoder is not None:\n",
        "            llr = self._decoder(llr)\n",
        "\n",
        "        if self._return_infobits:\n",
        "            return b, llr\n",
        "        else:\n",
        "            return c, llr\n",
        "\n",
        "def export_pgf(ber_plot, col_names):\n",
        "    \"\"\"Export results as table for for pgfplots compatible imports.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ber_plot: PlotBER\n",
        "        An object of PlotBER containing the BER simulations to be exported\n",
        "\n",
        "    col_names: list of str\n",
        "        Column names of the exported BER curves\n",
        "    \"\"\"\n",
        "    s = \"snr, \\t\"\n",
        "    for idx, var_name in enumerate(col_names):\n",
        "        s += var_name + \", \\t\"\n",
        "    s += \"\\n\"\n",
        "\n",
        "    for idx_snr,snr in enumerate(ber_plot._snrs[0]):\n",
        "        s += f\"{snr:.3f},\\t\"\n",
        "        for idx_dec, _ in enumerate(col_names):\n",
        "            s += f\"{ber_plot._bers[idx_dec][idx_snr].numpy():.6E},\\t\"\n",
        "        s += \"\\n\"\n",
        "    print(s)\n",
        "\n",
        "###################################################################################################\n",
        "\n",
        "def generate_pruned_pcm_5g(decoder, n, verbose=True):\n",
        "    \"\"\"Utility function to get the pruned parity-check matrix of the 5G code.\n",
        "\n",
        "    Identifies the pruned and shortened positions.\n",
        "    Hereby, '0' indicates an pruned codeword position\n",
        "    '1' indicates an codeword position\n",
        "    '2' indicates a shortened position.\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    decoder: LDPC5GDecoder\n",
        "        An instance of the decoder object.\n",
        "\n",
        "    n: int\n",
        "        The codeword lengths including rate-matching.\n",
        "\n",
        "    verbose: Boolean\n",
        "        Defaults to True. If True, status information during pruning is\n",
        "        provided.\n",
        "    \"\"\"\n",
        "\n",
        "    enc = decoder._encoder\n",
        "\n",
        "    # transmitted positions\n",
        "    pos_tx = np.ones(n)\n",
        "\n",
        "    # undo puncturing of the first 2*z information bits\n",
        "    pos_punc = np.concatenate([np.zeros([2*enc.z]),pos_tx], axis=0)\n",
        "\n",
        "    # puncturing of the last positions\n",
        "    # total length must be n_ldpc, while pos_tx has length n\n",
        "    # first 2*z positions are already added\n",
        "    # -> add n_ldpc - n - 2Z punctured positions\n",
        "    k_short = enc.k_ldpc - enc.k # number of shortend bits\n",
        "    num_punc_bits = ((enc.n_ldpc - k_short) - enc.n - 2*enc.z)\n",
        "    pos_punc2 = np.concatenate(\n",
        "               [pos_punc, np.zeros([num_punc_bits - decoder._nb_pruned_nodes])])\n",
        "\n",
        "    # shortening (= add 0 positions after k bits, i.e. LLR=LLR_max)\n",
        "    # the first k positions are the systematic bits\n",
        "    pos_info = pos_punc2[0:enc.k]\n",
        "\n",
        "    # parity part\n",
        "    num_par_bits = (enc.n_ldpc-k_short-enc.k-decoder._nb_pruned_nodes)\n",
        "    pos_parity = pos_punc2[enc.k:enc.k+num_par_bits]\n",
        "    pos_short = 2 * np.ones([k_short]) # \"2\" indicates shortened position\n",
        "\n",
        "    # and concatenate final pattern\n",
        "    rm_pattern = np.concatenate([pos_info, pos_short, pos_parity], axis=0)\n",
        "\n",
        "    # and prune matrix (remove shortend positions from pcm)\n",
        "    pcm_pruned = np.copy(decoder.pcm.todense())\n",
        "    idx_short = np.where(rm_pattern==2)\n",
        "    idx_pruned = np.setdiff1d(np.arange(pcm_pruned.shape[1]), idx_short)\n",
        "    pcm_pruned = pcm_pruned[:,idx_pruned]\n",
        "    num_shortened = np.size(idx_short)\n",
        "\n",
        "    # print information if enabled\n",
        "    if verbose:\n",
        "        print(\"using bg: \", enc._bg)\n",
        "        print(\"# information bits:\", enc.k)\n",
        "        print(\"CW length after rate-matching:\", n)\n",
        "        print(\"CW length without rm (incl. first 2*Z info bits):\",\n",
        "                                    pcm_pruned.shape[1])\n",
        "        print(\"# punctured bits:\", num_punc_bits)\n",
        "        print(\"# pruned nodes:\", decoder._nb_pruned_nodes)\n",
        "        print(\"# parity bits\", num_par_bits)\n",
        "        print(\"# shortened bits\", num_shortened)\n",
        "        print(\"pruned pcm dimension:\", pcm_pruned.shape)\n",
        "    return pcm_pruned, rm_pattern[idx_pruned]\n",
        "\n",
        "###################################################################################################\n",
        "\n",
        "class SimpleGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, gps_conv_params):\n",
        "        super(SimpleGNN, self).__init__()\n",
        "        self.gps_conv = GPSConv(**gps_conv_params)\n",
        "        self.linear = torch.nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.gps_conv(x, edge_index, batch)\n",
        "        x = self.linear(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Example usage:\n",
        "# Assume the input has 16 features and we want 32 output features\n",
        "in_channels = 16\n",
        "out_channels = 32\n",
        "gps_conv_params = {\n",
        "    'channels': in_channels,\n",
        "    'conv': None,  # You can pass a specific MessagePassing layer here if needed\n",
        "    'heads': 1,\n",
        "    'dropout': 0.0,\n",
        "    'act': 'relu',\n",
        "    'norm': 'batch_norm',\n",
        "    'attn_type': 'multihead',\n",
        "}\n"
      ],
      "metadata": {
        "id": "uWe52iBBWX4O"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}
