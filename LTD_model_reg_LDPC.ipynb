{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODC8id9KN4m1CtMnAN3K8q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/5G-Decoder/blob/main/LTD_model_reg_LDPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5q1VAmIeUKIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d0b741f-50cd-4034-b232-2f5bb4e5645d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '5G-Decoder'...\n",
            "remote: Enumerating objects: 1467, done.\u001b[K\n",
            "remote: Counting objects: 100% (1467/1467), done.\u001b[K\n",
            "remote: Compressing objects: 100% (527/527), done.\u001b[K\n",
            "remote: Total 1467 (delta 929), reused 1462 (delta 926), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1467/1467), 1.65 MiB | 6.94 MiB/s, done.\n",
            "Resolving deltas: 100% (929/929), done.\n",
            "Collecting sionna\n",
            "  Downloading sionna-0.19.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting tensorflow<2.16.0,>=2.13.0 (from sionna)\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sionna) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.5.3 in /usr/local/lib/python3.10/dist-packages (from sionna) (3.8.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sionna) (1.13.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from sionna) (6.4.5)\n",
            "Collecting mitsuba<3.6.0,>=3.2.0 (from sionna)\n",
            "  Downloading mitsuba-3.5.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pythreejs>=2.4.2 (from sionna)\n",
            "  Downloading pythreejs-2.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from sionna)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting ipydatawidgets==4.3.2 (from sionna)\n",
            "  Downloading ipydatawidgets-4.3.2-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting jupyterlab-widgets==3.0.5 (from sionna)\n",
            "  Downloading jupyterlab_widgets-3.0.5-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: traittypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipydatawidgets==4.3.2->sionna) (0.2.1)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->sionna)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.0.4->sionna)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: pip is looking at multiple versions of ipywidgets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting ipywidgets>=8.0.4 (from sionna)\n",
            "  Downloading ipywidgets-8.1.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (5.5.6)\n",
            "  Downloading ipywidgets-8.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "INFO: pip is still looking at multiple versions of ipywidgets to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading ipywidgets-8.0.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (2.8.2)\n",
            "Collecting drjit==0.4.6 (from mitsuba<3.6.0,>=3.2.0->sionna)\n",
            "  Downloading drjit-0.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.68.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16.0,>=2.13.0->sionna) (0.45.1)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.2.2)\n",
            "Downloading sionna-0.19.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipydatawidgets-4.3.2-py2.py3-none-any.whl (271 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_widgets-3.0.5-py3-none-any.whl (384 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.3/384.3 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.0.5-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mitsuba-3.5.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (40.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading drjit-0.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pythreejs-2.4.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pollyjuice74/5G-Decoder\n",
        "!pip install sionna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy.sparse import issparse, csr_matrix\n",
        "\n",
        "from sionna.fec.utils import generate_reg_ldpc, load_parity_check_examples, LinearEncoder, gm2pcm\n",
        "from sionna.utils.plotting import PlotBER\n",
        "from sionna.fec.ldpc import LDPCBPDecoder\n",
        "\n",
        "import os\n",
        "# os.chdir('../..')\n",
        "if os.path.exists('5G-Decoder'):\n",
        "  os.rename('5G-Decoder', '5G_Decoder')\n",
        "os.chdir('5G_Decoder/adv_nn')\n",
        "\n",
        "from dataset import *\n",
        "from attention import *\n",
        "from channel import *\n",
        "from args import *\n",
        "from model_functs import *\n",
        "from models import *"
      ],
      "metadata": {
        "id": "U5U5qUUVUeRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading LDPC code\")\n",
        "pcm, k, n, coderate = generate_reg_ldpc(v=3,\n",
        "                                        c=6,\n",
        "                                        n=10,\n",
        "                                        allow_flex_len=True,\n",
        "                                        verbose=True)\n",
        "\n",
        "# pcm = tf.cast(pcm, dtype=tf.int32)\n",
        "encoder = LinearEncoder(pcm, is_pcm=True, dtype=tf.int32)\n",
        "\n",
        "batch_size = 1  # For multiple codewords\n",
        "b = tf.random.uniform((batch_size, k), minval=0, maxval=2, dtype=tf.int32)\n",
        "c = encoder(b)\n",
        "\n",
        "pcm @ tf.reshape(c, (-1, batch_size)) % 2"
      ],
      "metadata": {
        "id": "6RF7dBDwWg0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for e2e model\n",
        "from sionna.utils import BinarySource, ebnodb2no\n",
        "from sionna.mapping import Mapper, Demapper\n",
        "from sionna.channel import AWGN\n",
        "# from sionna.fec.ldpc import LDPC5GDecoder, LDPC5GEncoder\n",
        "from tensorflow.keras.layers import Layer, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Args():\n",
        "    def __init__(self, model_type, code_type='LDPC', n_look_up=121, k_look_up=80, n=400, k=200,\n",
        "                       n_rings=2, ls_active=True, split_diff=True, sigma=0.1,\n",
        "                       t_layers=1, d_model=128, heads=8, lr=5e-4,\n",
        "                       batch_size=160, batch_size_eval = 150,\n",
        "                       eval_train_iter=5, save_weights_iter=100,\n",
        "                       ebno_db_eval=2.5,\n",
        "                       ebno_db_min=0., ebno_db_max=4., ebno_db_stepsize=0.25,\n",
        "                       traindata_len=500, testdata_len=250, epochs=1000):\n",
        "        assert model_type in ['gen', 'dis'], \"Type must be: 'gen', Generator or 'dis', Discriminator.\"\n",
        "        assert code_type in ['POLAR', 'BCH', 'CCSDS', 'LDPC', 'MACKAY', 'LDPC5G', 'POLAR5G'], \"Invalid linear code type.\"\n",
        "\n",
        "\n",
        "        # model data\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.split_diff = split_diff\n",
        "        self.n_rings = n_rings # ring connectivity of mask\n",
        "        self.sigma = sigma\n",
        "        self.t_layers = t_layers\n",
        "        self.ls_active = ls_active\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "\n",
        "        # training data\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.traindata_len = traindata_len\n",
        "        self.testdata_len = testdata_len\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.ebno_db_min = ebno_db_min\n",
        "        self.ebno_db_max = ebno_db_max\n",
        "        self.ebno_db_stepsize = ebno_db_stepsize\n",
        "\n",
        "        self.ebno_db_eval = ebno_db_eval\n",
        "        self.eval_train_iter = eval_train_iter\n",
        "        self.save_weights_iter = save_weights_iter\n",
        "        self.batch_size_eval = batch_size_eval\n",
        "\n",
        "        # code data\n",
        "        self.code_type = code_type\n",
        "        self.code = self.get_code(n_look_up, k_look_up) # n,k look up values in Get_Generator_and_Parity\n",
        "\n",
        "        # if self.code_type not in ['LDPC5G', 'POLAR5G']:\n",
        "        #     self.n, self.m, self.k = self.code.n, self.code.m, self.code.k\n",
        "        # else:\n",
        "        #     self.n, self.m, self.k = n, n-k, k\n",
        "\n",
        "        # self.n_steps = self.m + 5  # Number of diffusion steps\n",
        "\n",
        "    def get_code(self, n_look_up, k_look_up):\n",
        "        code = type('Code', (), {})() # class Code, no base class, no attributes/methods, () instantiate object\n",
        "        # code.n_look_up, code.k_look_up = n_look_up, k_look_up\n",
        "        # code.code_type = self.code_type\n",
        "\n",
        "        # if self.code_type not in ['LDPC5G', 'POLAR5G']:\n",
        "        #     G, H = Get_Generator_and_Parity(code)\n",
        "        #     code.G, code.H = tf.convert_to_tensor(G), csr_matrix( tf.convert_to_tensor(H) )\n",
        "\n",
        "        #     code.m, code.n = code.H.shape\n",
        "        #     code.k = code.n - code.m\n",
        "\n",
        "        return code\n",
        "\n",
        "\n",
        "class MHAttention(Layer):\n",
        "    def __init__(self, dims, heads, mask_length, linear=False, dropout=0.01):\n",
        "        super().__init__()\n",
        "        assert (dims % heads) == 0, 'dimension must be divisible by the number of heads'\n",
        "        self.linear = linear\n",
        "        self.dims = dims\n",
        "        self.heads = heads\n",
        "        self.dim_head = dims // heads\n",
        "\n",
        "        if linear:\n",
        "            self.k_proj = self.get_k_proj(mask_length) # n+m\n",
        "            self.proj_k = None\n",
        "            self.proj_v = None\n",
        "\n",
        "        self.to_q, self.to_k, self.to_v = [ Dense(self.dims, use_bias=False) for _ in range(3) ]\n",
        "        self.to_out = Dense(dims)\n",
        "        self.dropout = Dropout(dropout) # to d-dimentional embeddings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Creates shape (n,k_proj) proj matrices for key and\n",
        "        n_value = input_shape[1]\n",
        "        if self.linear:\n",
        "            self.proj_k = self.add_weight(\"proj_k\", shape=[n_value, self.k_proj], initializer=GlorotUniform())\n",
        "            self.proj_v = self.add_weight(\"proj_v\", shape=[n_value, self.k_proj], initializer=GlorotUniform())\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        out_att = self.lin_attention(x, mask) if self.linear else self.attention(x, mask)\n",
        "        return out_att\n",
        "\n",
        "    def get_k_proj(self, mask_length):\n",
        "        # gets dimention for linear tranformer vector projection\n",
        "        for k_proj in range(mask_length // 2, 0, -1): # starts at half the mask length TO 0\n",
        "            if mask_length % k_proj == 0:\n",
        "                return tf.cast(k_proj, tf.int32)\n",
        "\n",
        "    def lin_attention(self, x, mask): # O(n)\n",
        "        shape = tf.shape(x) # (b, n, d)\n",
        "        b = tf.cast(shape[0], tf.int32)\n",
        "        n = tf.cast(shape[1], tf.int32)\n",
        "\n",
        "        assert x.shape[-1] is not None, \"The last dimension of x is undefined.\"\n",
        "\n",
        "        query, key, val = self.to_q(x), self.to_k(x), self.to_v(x)\n",
        "\n",
        "        # Project key and val into k-dimentional space\n",
        "        key = tf.einsum('bnd,nk->bkd', key, self.proj_k)\n",
        "        val = tf.einsum('bnd,nk->bkd', val, self.proj_v)\n",
        "\n",
        "        # Reshape splitting for heads\n",
        "        query = tf.reshape(query, (b, n, self.heads, self.dim_head))\n",
        "        key = tf.reshape(key, (b, self.k_proj, self.heads, self.dim_head))\n",
        "        val = tf.reshape(val, (b, self.k_proj, self.heads, self.dim_head))\n",
        "        query, key, val = [ tf.transpose(x, [0, 2, 1, 3]) for x in [query, key, val] ]\n",
        "\n",
        "        # Low-rank mask (n,k_proj)\n",
        "        mask = tf.expand_dims(mask, axis=-1)\n",
        "        mask = tf.image.resize(mask, [n, self.k_proj], method='nearest')\n",
        "        mask = tf.reshape(mask, (1, 1, n, self.k_proj))\n",
        "\n",
        "        # Main attn logic: sftmx( q@k / d**0.5 ) @ v\n",
        "        scores = tf.einsum('bhnd,bhkd->bhnk', query, key) / (tf.sqrt( tf.cast(self.dim_head, dtype=tf.float32) ))\n",
        "        scores += (mask * -1e9) if mask is not None else 0.\n",
        "        attn = tf.nn.softmax(scores, axis=-1) # (b,h,n,k_proj)\n",
        "        attn = self.dropout(attn)\n",
        "        out = tf.einsum('bhnk,bhkd->bhnd', attn, val)\n",
        "\n",
        "        # Reshape and pass through out layer\n",
        "        out = tf.transpose(out, [0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (b, n, -1))\n",
        "        return self.to_out(out)\n",
        "\n",
        "    def attention(self, x, mask): # O(n^2)\n",
        "        shape = tf.shape(x)\n",
        "        b = shape[0]\n",
        "        n = shape[1]\n",
        "        x = x[:, :, tf.newaxis] # (b,n,1)\n",
        "\n",
        "        query, key, val = self.to_q(x), self.to_k(x), self.to_v(x) # (b, n, d)\n",
        "        query, key, val = [ tf.reshape(x, (b, n, self.heads, self.dim_head)) for x in [query, key, val] ]\n",
        "        query, key, val = [ tf.cast( tf.transpose(x, [0, 2, 1, 3]), tf.float32 )\n",
        "                                                                            for x in [query, key, val] ]\n",
        "\n",
        "        scores = tf.einsum('bhqd,bhkd->bhqk', query, key) / (tf.sqrt( tf.cast(self.dim_head, tf.float32) ))\n",
        "        scores += (mask * -1e9) if mask is not None else 0. # apply mask non-edge connections\n",
        "        attn = tf.nn.softmax(scores, axis=-1) #-1\n",
        "        attn = self.dropout(attn)\n",
        "        out = tf.einsum('bhqk,bhkd->bhqd', attn, val)\n",
        "\n",
        "        out = tf.transpose(out, [0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (b, n, -1))\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "# class Decoder( TransformerDiffusion ):\n",
        "#     def __init__(self, args):\n",
        "#         super().__init__(args)\n",
        "#         self.transformer =\n",
        "\n",
        "#     # 'test' function\n",
        "#     def call(self, r_t):\n",
        "#         i = tf.constant(0)  # Initialize loop counter\n",
        "\n",
        "#         def condition(r_t, i):\n",
        "#             # Loop while i < self.m and syndrome sum is not zero\n",
        "#             return tf.logical_and(i < 5, tf.reduce_sum(self.get_syndrome(r_t)) != 0) # CHANGE 5 TO SELF.M\n",
        "\n",
        "#         def body(r_t, i):\n",
        "#             # Perform reverse or split diffusion\n",
        "#             r_t = tf.cond(\n",
        "#                 tf.logical_not(self.split_diff),\n",
        "#                 lambda: self.split_rdiff_call(r_t),\n",
        "#                 lambda: self.rev_diff_call(r_t),\n",
        "#             )\n",
        "#             return r_t, tf.add(i, 1)\n",
        "\n",
        "#         # Run tf.while_loop with the loop variables\n",
        "#         llr_hat, _ = tf.while_loop(\n",
        "#             condition,\n",
        "#             body,\n",
        "#             loop_vars=[r_t, i],\n",
        "#             maximum_iterations=self.m,\n",
        "#             shape_invariants=[tf.TensorShape([self.batch_size, self.n]), i.get_shape()]\n",
        "#         )\n",
        "\n",
        "#         # llr_hat, _ = self.tran_call(r_t)\n",
        "#         tf.print(\"llr_hat\", llr_hat)\n",
        "\n",
        "#         return llr_hat\n",
        "\n",
        "#     # Refines recieved codeword r at time t\n",
        "#     def rev_diff_call(self, r_t):\n",
        "#         tf.print(\"Rev def call with line-search...\")\n",
        "\n",
        "#         # Transformer error prediction\n",
        "#         z_hat_crude, t = self.tran_call(r_t) # (b,n)\n",
        "#         r_t1 = r_t - z_hat_crude*self.get_sigma(t)[:, tf.newaxis] # (b,n)\n",
        "#         # tf.print(r_t1)\n",
        "\n",
        "#         # # Refined estimate of the codeword for the ls diffusion step\n",
        "#         # r_t1, z_hat = self.line_search(r_t, sigma, err_hat) if self.ls_active else 1.\n",
        "#         # tf.print(\"After linesearch: \", r_t1)\n",
        "\n",
        "#         print(\"r_t1\", r_t1.shape, r_t1.dtype)\n",
        "#         return r_t1 # r at t-1, both (b,n)\n",
        "\n",
        "#     def split_rdiff_call(self, r_t):\n",
        "#         tf.print(\"Rev diff call with split diffusion...\")\n",
        "#         # First half-step condition subproblem\n",
        "#         z_hat_crude, t = self.tran_call(r_t)\n",
        "#         # tf.print(\"fc input: \", (z_hat_crude * self.get_sigma(t)[:, tf.newaxis]))\n",
        "#         r_t_half = r_t - 0.5 * self.fc( z_hat_crude * self.get_sigma(t)[:, tf.newaxis] )\n",
        "#         # tf.print(\"r_t_half\", r_t_half)\n",
        "\n",
        "#         # Full-step diffusion subproblem\n",
        "#         r_t1 = r_t_half + tf.random.normal(r_t_half.shape) * tf.sqrt(self.get_sigma(t)[:, tf.newaxis])\n",
        "\n",
        "#         # Second half-step condition subproblem\n",
        "#         z_hat_crude_half, t = self.tran_call(r_t1)  # Reuse the second `tran_call`\n",
        "#         r_t1 = r_t1 - 0.5 * self.fc(z_hat_crude_half * self.get_sigma(t)[:, tf.newaxis])\n",
        "#         print(\"r_t1\", r_t1.shape, r_t1.dtype)\n",
        "#         return r_t1  # r at t-1, both (b,n)\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Dense, LayerNormalization, Dropout\n",
        "\n",
        "class TransformerDecoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super(TransformerDecoderBlock, self).__init__()\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, mask, training):\n",
        "        # Multi-Head Attention\n",
        "        attn_output = self.mha(x, x, attention_mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # Add & Normalize\n",
        "\n",
        "        # Feedforward Network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Add & Normalize\n",
        "        return out2\n",
        "\n",
        "\n",
        "class Decoder( Layer ):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        code = args.code\n",
        "        self.pcm = tf.cast(code.H, dtype=tf.int32)\n",
        "\n",
        "        # shapes\n",
        "        self._m, self._n = self.pcm.shape\n",
        "        self._k = self._n - self._m\n",
        "        self.dims = args.d_model\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        # layers\n",
        "        self.encoder_blocks = [\n",
        "            TransformerDecoderBlock(\n",
        "                d_model=args.d_model,\n",
        "                num_heads=args.heads,\n",
        "                dff=args.d_model * 4,\n",
        "                dropout_rate=0.1,\n",
        "            )\n",
        "            for _ in range(args.t_layers)\n",
        "        ]\n",
        "        self.forward_channel = Dense(1)\n",
        "        self.to_n = Dense(self._n)\n",
        "\n",
        "        # mask\n",
        "        self.mask = self.create_mask(self.pcm)\n",
        "        # for matrix, title in zip([self.pcm, self.mask], [\"PCM Matrix\", \"Mask Matrix\"]):\n",
        "        #     plt.imshow(matrix, cmap='viridis'); plt.colorbar(); plt.title(title); plt.show()\n",
        "        # print(\"mask, pcm: \", self.mask, self.pcm)\n",
        "\n",
        "    def create_mask(self, H):\n",
        "        # Initialize diagonal identity mask\n",
        "        mask = tf.eye(2 * self._n - self._k, dtype=tf.float32)\n",
        "\n",
        "        # Get indices where H == 1\n",
        "        indices = tf.where(H == 1)  # Returns (row, col) pairs where H is 1\n",
        "        check_nodes, variable_nodes = indices[:, 0], indices[:, 1]\n",
        "\n",
        "        # Step 1: Update check node to variable node connections\n",
        "        mask = tf.tensor_scatter_nd_update(mask,\n",
        "                                          tf.stack([n + check_nodes, variable_nodes], axis=1),\n",
        "                                          tf.ones_like(check_nodes, dtype=tf.float32))\n",
        "        mask = tf.tensor_scatter_nd_update(mask,\n",
        "                                          tf.stack([variable_nodes, n + check_nodes], axis=1),\n",
        "                                          tf.ones_like(check_nodes, dtype=tf.float32))\n",
        "\n",
        "        # Step 2: Update variable node connections\n",
        "        for cn in tf.unique(check_nodes)[0]:  # Iterate over unique check nodes\n",
        "            related_vns = tf.boolean_mask(variable_nodes, check_nodes == cn)\n",
        "            indices = tf.stack(tf.meshgrid(related_vns, related_vns), axis=-1)\n",
        "            indices = tf.reshape(indices, [-1, 2])  # Flatten indices\n",
        "            mask = tf.tensor_scatter_nd_update(mask, indices, tf.ones_like(indices[:, 0], dtype=tf.float32))\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_syndrome(self, llr_vector):\n",
        "        \"\"\" Calculate syndrome (pcm @ r = 0) if r is correct in binary \"\"\"\n",
        "        llr_vector = tf.reshape(llr_vector, (self._n, -1)) # (n,b)\n",
        "        bin_vector = tf.cast(llr_to_bin(llr_vector), dtype=tf.int32)\n",
        "\n",
        "        return tf.cast( (self.pcm @ bin_vector) % 2, dtype=tf.float32) # (m,n)@(n,b)->(m,b)\n",
        "\n",
        "    def call(self, x_nodes, training=False):\n",
        "        tf.print(\"DECODER CALL\")\n",
        "        # Pass through each encoder block\n",
        "        for block in self.encoder_blocks:\n",
        "            x_nodes = block(x_nodes,\n",
        "                            mask=self.mask,\n",
        "                            training=training)\n",
        "            tf.print(\"x_nodes\", x_nodes)\n",
        "        x_nodes = tf.squeeze( self.forward_channel(x_nodes), axis=-1 ) # (b, n+m, hidden_dims)->(b, n+m)\n",
        "        tf.print(\"x_nodes\", x_nodes, x_nodes.shape)\n",
        "        llr_hat = self.to_n(x_nodes) # (b, n+m)->(b,n)\n",
        "        tf.print(\"Decoded output (llr_hat):\", llr_hat)\n",
        "        return llr_hat\n",
        "\n",
        "\n",
        "class E2EModel(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, k, n, return_infobits=False, es_no=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self._n = n\n",
        "        self._k = k\n",
        "        self._m = n - k\n",
        "\n",
        "        self._binary_source = BinarySource()\n",
        "        self._num_bits_per_symbol = 2\n",
        "        self._mapper = Mapper(\"qam\", self._num_bits_per_symbol)\n",
        "        self._demapper = Demapper(\"app\", \"qam\", self._num_bits_per_symbol)\n",
        "        self._channel = AWGN()\n",
        "        self._decoder = decoder\n",
        "        self._encoder = encoder\n",
        "        self._return_infobits = return_infobits\n",
        "        self._es_no = es_no\n",
        "\n",
        "    @tf.function(jit_compile=False)\n",
        "    def call(self, batch_size, ebno_db):\n",
        "\n",
        "        # no rate-adjustment for uncoded transmission or es_no scenario\n",
        "        if self._decoder is not None and self._es_no==False:\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, self._k/self._n)\n",
        "        else: #for uncoded transmissions the rate is 1\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, 1)\n",
        "\n",
        "        b = self._binary_source([batch_size, self._k])\n",
        "        if self._encoder is not None:\n",
        "            c = self._encoder(b)\n",
        "        else:\n",
        "            c = b\n",
        "\n",
        "        # check that rate calculations are correct\n",
        "        assert self._n==c.shape[-1], \"Invalid value of n.\"\n",
        "\n",
        "        # zero padding to support odd codeword lengths\n",
        "        if self._n%2==1:\n",
        "            c_pad = tf.concat([c, tf.zeros([batch_size, 1])], axis=1)\n",
        "        else: # no padding\n",
        "            c_pad = c\n",
        "        x = self._mapper(c_pad)\n",
        "\n",
        "        y = self._channel([x, no])\n",
        "        llr = self._demapper([y, no])\n",
        "\n",
        "        # remove zero padded bit at the end\n",
        "        if self._n%2==1:\n",
        "            llr = llr[:,:-1]\n",
        "        tf.print('PCM @ CW: ', self._decoder.pcm @\n",
        "                 tf.reshape(tf.cast(c, dtype=tf.int32), (self._n, -1)) % 2)\n",
        "\n",
        "        # decoder input nodes\n",
        "        syndrome = tf.reshape( self._decoder.get_syndrome(llr),\n",
        "                               (batch_size, self._m) ) # (m,n)@(n,b)->(m,b) check nodes\n",
        "        x_nodes = tf.concat([llr, syndrome], axis=1)[:, :, tf.newaxis] # (b, n+m, 1)\n",
        "\n",
        "        # and run the decoder\n",
        "        if self._decoder is not None:\n",
        "            tf.print('x_nodes input: ', x_nodes)\n",
        "            ############################\n",
        "            llr_hat = self._decoder(x_nodes)\n",
        "            ############################\n",
        "            # tf.print(\"llr_hat: \", llr_hat)\n",
        "\n",
        "        if self._return_infobits:\n",
        "            return b, llr_hat\n",
        "        else:\n",
        "            return c, llr_hat\n",
        "\n",
        "\n",
        "# args for decoder/discriminator\n",
        "args = Args(model_type='dis')\n",
        "args.code.H = pcm\n",
        "args.n, args.m = pcm.shape\n",
        "args.k = k\n",
        "args.n_steps = args.m + 5\n",
        "\n",
        "ltd_decoder = Decoder(args) # Linear Transformer Diffusion (LTD) Decoder\n",
        "\n",
        "e2e_ltd = E2EModel(encoder, ltd_decoder, k, n)\n",
        "\n",
        "\n",
        "def bin_to_llr(x):\n",
        "    \"\"\" Clip llrs to 20 for numerical stability \"\"\"\n",
        "    llr_vector = tf.where(x == 0, -20, 20)\n",
        "    return llr_vector\n",
        "\n",
        "\n",
        "def train_dec(model, args):\n",
        "    # loss\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    # optimizer\n",
        "    scheduler = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=args.lr, decay_steps=args.epochs) # 1000 is size of trainloader\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler)\n",
        "    # time start\n",
        "    time_start = time.time()\n",
        "\n",
        "    # SGD update iteration\n",
        "    @tf.function(jit_compile=False)\n",
        "    def train_step(batch_size):\n",
        "        # train for random SNRs within a pre-defined interval\n",
        "        ebno_db = tf.random.uniform([batch_size, 1],\n",
        "                                    minval=args.ebno_db_min,\n",
        "                                    maxval=args.ebno_db_max)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            c, llr_hat = model(batch_size, ebno_db)\n",
        "            # tf.print(c, llr_hat)\n",
        "\n",
        "            llr_y = bin_to_llr(c)\n",
        "            print(\"llr_hat\", llr_hat.shape, llr_hat.dtype)\n",
        "            loss_value = loss_fn(llr_y, llr_hat)\n",
        "\n",
        "        # and apply the SGD updates\n",
        "        weights = model.trainable_weights\n",
        "        grads = tape.gradient(loss_value, weights) # variables\n",
        "        optimizer.apply_gradients(zip(grads, weights))\n",
        "        return c, llr_hat\n",
        "\n",
        "    print(\"Training Linear Transformer Diffusion Model...\")\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_step(args.batch_size)\n",
        "\n",
        "        # eval train iter\n",
        "        if epoch % args.eval_train_iter == 0:\n",
        "            ebno_db = tf.random.uniform([args.batch_size, 1],\n",
        "                                          minval=args.ebno_db_eval,\n",
        "                                          maxval=args.ebno_db_eval)\n",
        "\n",
        "            c, llr_hat = model(args.batch_size, ebno_db)\n",
        "\n",
        "            # loss\n",
        "            llr_y = bin_to_llr(c)\n",
        "            loss_value = loss_fn(llr_y, llr_hat)\n",
        "\n",
        "            # ber\n",
        "            c_hat = llr_to_bin(llr_hat)\n",
        "            ber = compute_ber(c, c_hat).numpy()\n",
        "\n",
        "            # measure required time since last evaluation\n",
        "            duration = time.time() - time_start # in s\n",
        "            time_start = time.time() # reset counter\n",
        "\n",
        "            print(f'Training epoch {epoch}/{args.epochs}, LR={optimizer.learning_rate.numpy():.2e}, Loss={loss_value.numpy():.5e}, BER={ber}, duration: {duration:.2f}s')\n",
        "            break\n",
        "\n",
        "        # save weights iter\n",
        "        if epoch % args.save_weights_iter == 0:\n",
        "            pass\n",
        "\n",
        "        # heat-map visualization of the model's weights\n",
        "        # for var in self.trainable_variables:\n",
        "        #     var_name = var.name\n",
        "        #     var_value = var.numpy()\n",
        "\n",
        "        #     # Check if the variable is at least 2D (suitable for heatmap)\n",
        "        #     if len(var_value.shape) > 1:\n",
        "        #         plt.figure(figsize=(8, 6))\n",
        "        #         sns.heatmap(var_value, cmap='viridis')\n",
        "        #         plt.title(f'Heatmap of {var_name}')\n",
        "        #         plt.show()\n",
        "        #     else:\n",
        "        #         print(f\"{var_name} has shape {var_value.shape} which is not suitable for a heatmap.\")\n",
        "\n",
        "\n",
        "train_dec(e2e_ltd, args)"
      ],
      "metadata": {
        "id": "XOILyjSGXMdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69e4b0a-c833-4d47-b3fc-aa3fc00b2f72"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Linear Transformer Diffusion Model...\n",
            "llr_hat (160, 10) <dtype: 'float32'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llr_hat (160, 10) <dtype: 'float32'>\n",
            "PCM @ CW:  [[0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 1]\n",
            " [1 0 0 ... 0 0 1]\n",
            " [1 0 0 ... 0 0 1]\n",
            " [0 1 1 ... 1 0 0]]\n",
            "x_nodes input:  [[[-0.500589848]\n",
            "  [7.64186668]\n",
            "  [-4.37257338]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [0]]\n",
            "\n",
            " [[-2.27644467]\n",
            "  [2.71637321]\n",
            "  [-2.67925882]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [0]]\n",
            "\n",
            " [[3.25219131]\n",
            "  [10.389204]\n",
            "  [0.927835047]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[2.88789964]\n",
            "  [3.67632747]\n",
            "  [-3.88023782]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " [[-3.02720833]\n",
            "  [-2.14785814]\n",
            "  [2.68961978]\n",
            "  ...\n",
            "  [0]\n",
            "  [1]\n",
            "  [0]]\n",
            "\n",
            " [[-5.09194]\n",
            "  [-5.47058392]\n",
            "  [-0.00825977325]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [0]]]\n",
            "DECODER CALL\n",
            "x_nodes [[[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]]\n",
            "x_nodes [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]] TensorShape([160, 15])\n",
            "Decoded output (llr_hat): [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "PCM @ CW:  [[1 0 0 ... 0 1 1]\n",
            " [1 0 1 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 0 1 ... 0 1 1]\n",
            " [1 0 0 ... 1 0 1]]\n",
            "x_nodes input:  [[[3.525038]\n",
            "  [-2.64680409]\n",
            "  [8.23385811]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " [[7.13773727]\n",
            "  [7.77501202]\n",
            "  [-6.24976158]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " [[-3.22471166]\n",
            "  [6.6095767]\n",
            "  [-3.28072238]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.542564869]\n",
            "  [-5.61887]\n",
            "  [-1.03372562]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " [[-4.50942898]\n",
            "  [-6.62455654]\n",
            "  [6.00721836]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [0]]\n",
            "\n",
            " [[0.366203159]\n",
            "  [-3.11311483]\n",
            "  [0.5623492]\n",
            "  ...\n",
            "  [0]\n",
            "  [1]\n",
            "  [0]]]\n",
            "DECODER CALL\n",
            "x_nodes [[[-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.430673838 0.47042346 -0.412732 ... 0.464132279 -0.514555335 0.418732911]\n",
            "  ...\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]]\n",
            "\n",
            " [[-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.430673838 0.47042346 -0.412732 ... 0.464132279 -0.514555335 0.418732911]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  ...\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]]\n",
            "\n",
            " [[-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  ...\n",
            "  [-0.435748428 0.462834746 -0.426624775 ... 0.459143907 -0.478403896 0.436549246]\n",
            "  [-0.435768217 0.462804049 -0.426679432 ... 0.459123552 -0.478259653 0.43661955]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  ...\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]]\n",
            "\n",
            " [[-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  ...\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435768217 0.462804049 -0.426679432 ... 0.459123552 -0.478259653 0.43661955]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]]\n",
            "\n",
            " [[-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  ...\n",
            "  [-0.435748428 0.462834746 -0.426624775 ... 0.459143907 -0.478403896 0.436549246]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]\n",
            "  [-0.435827196 0.462712 -0.426843613 ... 0.459062487 -0.477827 0.436830401]]]\n",
            "x_nodes [[-6.06077862 -6.06077862 -6.06447 ... -6.06077862 -6.06077862 -6.06077862]\n",
            " [-6.06077862 -6.06447 -6.06077862 ... -6.06077862 -6.06077862 -6.06077862]\n",
            " [-6.06077862 -6.06077862 -6.06077862 ... -6.06086349 -6.06084251 -6.06077862]\n",
            " ...\n",
            " [-6.06077862 -6.06077862 -6.06077862 ... -6.06077862 -6.06077862 -6.06077862]\n",
            " [-6.06077862 -6.06077862 -6.06077862 ... -6.06077862 -6.06084251 -6.06077862]\n",
            " [-6.06077862 -6.06077862 -6.06077862 ... -6.06086349 -6.06077862 -6.06077862]] TensorShape([160, 15])\n",
            "Decoded output (llr_hat): [[-0.365651488 10.73668 -5.3945117 ... 8.91944599 -6.33598709 10.4527512]\n",
            " [-0.363683581 10.7376556 -5.39480495 ... 8.91904163 -6.33694649 10.4537401]\n",
            " [-0.364074528 10.7366085 -5.39371395 ... 8.91778374 -6.33530426 10.4509668]\n",
            " ...\n",
            " [-0.365136504 10.7380419 -5.39491081 ... 8.91751194 -6.33528423 10.4524908]\n",
            " [-0.364098847 10.7366076 -5.39371252 ... 8.91775608 -6.33526897 10.4509745]\n",
            " [-0.364086509 10.7365561 -5.39371634 ... 8.91777229 -6.33529472 10.4509897]]\n",
            "PCM @ CW:  [[1 1 1 ... 1 1 0]\n",
            " [1 0 0 ... 1 0 0]\n",
            " [1 0 1 ... 1 1 1]\n",
            " [0 0 0 ... 1 0 1]\n",
            " [1 0 0 ... 1 1 0]]\n",
            "x_nodes input:  [[[2.10869384]\n",
            "  [-7.63479757]\n",
            "  [-8.54474449]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " [[-1.88634133]\n",
            "  [3.04044604]\n",
            "  [4.29360485]\n",
            "  ...\n",
            "  [0]\n",
            "  [1]\n",
            "  [1]]\n",
            "\n",
            " [[-1.43476796]\n",
            "  [3.95150352]\n",
            "  [4.99698782]\n",
            "  ...\n",
            "  [0]\n",
            "  [1]\n",
            "  [0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[3.5249176]\n",
            "  [6.69104338]\n",
            "  [7.26308393]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " [[-5.21274853]\n",
            "  [0.791177094]\n",
            "  [2.75287771]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [0]]\n",
            "\n",
            " [[-1.88963091]\n",
            "  [-5.80171251]\n",
            "  [-4.59961414]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [0]]]\n",
            "DECODER CALL\n",
            "x_nodes [[[-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  ...\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]]\n",
            "\n",
            " [[-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  ...\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]]\n",
            "\n",
            " [[-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  ...\n",
            "  [-0.189467058 0.661096454 -0.186276361 ... 0.769230366 -0.216415107 -0.377348959]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189467058 0.661096454 -0.186276361 ... 0.769230366 -0.216415107 -0.377348959]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  ...\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]]\n",
            "\n",
            " [[-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  ...\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]]\n",
            "\n",
            " [[-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  [-0.189163372 0.661973715 -0.184637278 ... 0.771414936 -0.212144136 -0.364975959]\n",
            "  ...\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]\n",
            "  [-0.189469412 0.661089599 -0.186289206 ... 0.76921308 -0.216448694 -0.377446294]]]\n",
            "x_nodes [[0.528030396 0.528030396 0.528030396 ... 0.570684135 0.570684135 0.570684135]\n",
            " [0.528030396 0.528030396 0.528030396 ... 0.570684135 0.570684135 0.570684135]\n",
            " [0.570684135 0.528030396 0.528030396 ... 0.570350945 0.570684135 0.570350945]\n",
            " ...\n",
            " [0.528030396 0.528030396 0.528030396 ... 0.570684135 0.570684135 0.570684135]\n",
            " [0.528030396 0.570684135 0.528030396 ... 0.570684135 0.570684135 0.570684135]\n",
            " [0.570684135 0.528030396 0.528030396 ... 0.570684135 0.570684135 0.570684135]] TensorShape([160, 15])\n",
            "Decoded output (llr_hat): [[-0.001556159 -0.966239333 0.447735399 ... -0.77606523 0.574529946 -0.909724176]\n",
            " [0.030285893 -0.995417833 0.481173724 ... -0.773382604 0.574352682 -0.927918613]\n",
            " [0.00538156088 -1.00493467 0.486378342 ... -0.783959329 0.586036 -0.926385403]\n",
            " ...\n",
            " [0.0255622249 -1.00484192 0.457939595 ... -0.784274459 0.56453526 -0.929388702]\n",
            " [-0.0166252721 -0.977005 0.449682564 ... -0.801408589 0.547199726 -0.919167578]\n",
            " [0.0175850373 -1.0058099 0.480199665 ... -0.7840904 0.571117461 -0.915380299]]\n",
            "PCM @ CW:  [[0 0 1 ... 1 1 0]\n",
            " [1 0 0 ... 0 0 1]\n",
            " [1 1 1 ... 1 0 0]\n",
            " [0 1 0 ... 1 0 1]\n",
            " [1 1 1 ... 1 0 0]]\n",
            "x_nodes input:  [[[-2.30404806]\n",
            "  [-1.96533358]\n",
            "  [7.08330536]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " [[5.59378099]\n",
            "  [-0.353231728]\n",
            "  [-2.75111747]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [0]]\n",
            "\n",
            " [[3.94124722]\n",
            "  [-7.48442936]\n",
            "  [1.21191573]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.05112052]\n",
            "  [-2.00883389]\n",
            "  [2.83262968]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [0]]\n",
            "\n",
            " [[3.98311186]\n",
            "  [6.10944843]\n",
            "  [-4.73323202]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [0]]\n",
            "\n",
            " [[1.20162117]\n",
            "  [7.08301497]\n",
            "  [7.61822891]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [0]]]\n",
            "DECODER CALL\n",
            "x_nodes [[[-0.0509837791 0.934642792 -0.0684395805 ... 0.955298305 -0.00913354754 -0.759141207]\n",
            "  [-0.0509837791 0.934642792 -0.0684395805 ... 0.955298305 -0.00913354754 -0.759141207]\n",
            "  [-0.0597083569 0.965333283 -0.0618425608 ... 0.954022229 0.0100545585 -0.723701477]\n",
            "  ...\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]\n",
            "  [-0.0531601906 0.942314863 -0.0667961091 ... 0.955000162 -0.00434945524 -0.750321269]]\n",
            "\n",
            " [[-0.0597083569 0.965333283 -0.0618425608 ... 0.954022229 0.0100545585 -0.723701477]\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]\n",
            "  [-0.0509837791 0.934642792 -0.0684395805 ... 0.955298305 -0.00913354754 -0.759141207]\n",
            "  ...\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]]\n",
            "\n",
            " [[-0.0597083569 0.965333283 -0.0618425608 ... 0.954022229 0.0100545585 -0.723701477]\n",
            "  [-0.0597083569 0.965333283 -0.0618425608 ... 0.954022229 0.0100545585 -0.723701477]\n",
            "  [-0.0509837791 0.934642792 -0.0684395805 ... 0.955298305 -0.00913354754 -0.759141207]\n",
            "  ...\n",
            "  [-0.0509837791 0.934642792 -0.0684395805 ... 0.955298305 -0.00913354754 -0.759141207]\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]\n",
            "  [-0.0531601906 0.942314863 -0.0667961091 ... 0.955000162 -0.00434945524 -0.750321269]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.0509837791 0.934642792 -0.0684395805 ... 0.955298305 -0.00913354754 -0.759141207]\n",
            "  [-0.0509837791 0.934642792 -0.0684395805 ... 0.955298305 -0.00913354754 -0.759141207]\n",
            "  [-0.0509837791 0.934642792 -0.0684395805 ... 0.955298305 -0.00913354754 -0.759141207]\n",
            "  ...\n",
            "  [-0.0531601906 0.942314863 -0.0667961091 ... 0.955000162 -0.00434945524 -0.750321269]\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]]\n",
            "\n",
            " [[-0.0597083569 0.965333283 -0.0618425608 ... 0.954022229 0.0100545585 -0.723701477]\n",
            "  [-0.0597083569 0.965333283 -0.0618425608 ... 0.954022229 0.0100545585 -0.723701477]\n",
            "  [-0.0597083569 0.965333283 -0.0618425608 ... 0.954022229 0.0100545585 -0.723701477]\n",
            "  ...\n",
            "  [-0.0509837791 0.934642792 -0.0684395805 ... 0.955298305 -0.00913354754 -0.759141207]\n",
            "  [-0.0531601906 0.942314863 -0.0667961091 ... 0.955000162 -0.00434945524 -0.750321269]\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]]\n",
            "\n",
            " [[-0.0531601906 0.942314863 -0.0667961091 ... 0.955000162 -0.00434945524 -0.750321269]\n",
            "  [-0.0597083569 0.965333283 -0.0618425608 ... 0.954022229 0.0100545585 -0.723701477]\n",
            "  [-0.0597083569 0.965333283 -0.0618425608 ... 0.954022229 0.0100545585 -0.723701477]\n",
            "  ...\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]\n",
            "  [-0.0531601906 0.942314863 -0.0667961091 ... 0.955000162 -0.00434945524 -0.750321269]\n",
            "  [-0.0520714819 0.938478589 -0.0676182956 ... 0.955150962 -0.00674265623 -0.754734516]]]\n",
            "x_nodes [[4.2045579 4.2045579 4.13174391 ... 4.19553614 4.19553614 4.18649149]\n",
            " [4.13174391 4.19553614 4.2045579 ... 4.19553614 4.19553614 4.19553614]\n",
            " [4.13174391 4.13174391 4.2045579 ... 4.2045579 4.19553614 4.18649149]\n",
            " ...\n",
            " [4.2045579 4.2045579 4.2045579 ... 4.18649149 4.19553614 4.19553614]\n",
            " [4.13174391 4.13174391 4.13174391 ... 4.2045579 4.18649149 4.19553614]\n",
            " [4.18649149 4.13174391 4.13174391 ... 4.19553614 4.18649149 4.19553614]] TensorShape([160, 15])\n",
            "Decoded output (llr_hat): [[0.213735491 -7.37198877 3.69104719 ... -6.09191561 4.31116199 -7.17685699]\n",
            " [0.216229782 -7.35535288 3.65552139 ... -6.15199566 4.33652067 -7.16594744]\n",
            " [0.216611773 -7.3504014 3.66564703 ... -6.11319447 4.39591789 -7.15995693]\n",
            " ...\n",
            " [0.205062255 -7.3767314 3.7217598 ... -6.10328579 4.31985664 -7.16561556]\n",
            " [0.219154969 -7.43037367 3.67623663 ... -6.05546236 4.30916309 -7.12219763]\n",
            " [0.152196541 -7.35664558 3.64029598 ... -6.09255505 4.32995415 -7.12325907]]\n",
            "PCM @ CW:  [[0 1 1 ... 1 0 1]\n",
            " [1 1 0 ... 0 1 0]\n",
            " [1 1 1 ... 0 0 1]\n",
            " [0 1 0 ... 1 0 1]\n",
            " [0 0 1 ... 0 0 0]]\n",
            "x_nodes input:  [[[2.01793957]\n",
            "  [5.91894579]\n",
            "  [-2.91397023]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [0]]\n",
            "\n",
            " [[-4.11624098]\n",
            "  [2.62597966]\n",
            "  [-3.63401437]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " [[-5.26843691]\n",
            "  [-0.257785559]\n",
            "  [6.44663715]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.60286534]\n",
            "  [7.0311532]\n",
            "  [2.8473289]\n",
            "  ...\n",
            "  [0]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " [[-0.892707586]\n",
            "  [-9.76979065]\n",
            "  [3.24577022]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [0]]\n",
            "\n",
            " [[-5.14398527]\n",
            "  [3.1209147]\n",
            "  [1.7909615]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [0]]]\n",
            "DECODER CALL\n",
            "x_nodes [[[-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.261085153 1.32052159 -0.121291921 ... 1.34916043 -0.0948562175 -0.616393745]\n",
            "  [-0.269973367 1.33386731 -0.111846715 ... 1.35531044 -0.0828305781 -0.586782932]\n",
            "  ...\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.265809715 1.32764137 -0.116282612 ... 1.35246241 -0.0884766877 -0.600696087]]\n",
            "\n",
            " [[-0.261085153 1.32052159 -0.121291921 ... 1.34916043 -0.0948562175 -0.616393745]\n",
            "  [-0.269973367 1.33386731 -0.111846715 ... 1.35531044 -0.0828305781 -0.586782932]\n",
            "  [-0.269973367 1.33386731 -0.111846715 ... 1.35531044 -0.0828305781 -0.586782932]\n",
            "  ...\n",
            "  [-0.265809715 1.32764137 -0.116282612 ... 1.35246241 -0.0884766877 -0.600696087]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]]\n",
            "\n",
            " [[-0.261085153 1.32052159 -0.121291921 ... 1.34916043 -0.0948562175 -0.616393745]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.261085153 1.32052159 -0.121291921 ... 1.34916043 -0.0948562175 -0.616393745]\n",
            "  ...\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.261085153 1.32052159 -0.121291921 ... 1.34916043 -0.0948562175 -0.616393745]\n",
            "  [-0.269973367 1.33386731 -0.111846715 ... 1.35531044 -0.0828305781 -0.586782932]\n",
            "  ...\n",
            "  [-0.265809715 1.32764137 -0.116282612 ... 1.35246241 -0.0884766877 -0.600696087]\n",
            "  [-0.265809715 1.32764137 -0.116282612 ... 1.35246241 -0.0884766877 -0.600696087]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]]\n",
            "\n",
            " [[-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.278809309 1.34686863 -0.102265418 ... 1.36111712 -0.0707284212 -0.556924045]\n",
            "  [-0.269973367 1.33386731 -0.111846715 ... 1.35531044 -0.0828305781 -0.586782932]\n",
            "  ...\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]]\n",
            "\n",
            " [[-0.261085153 1.32052159 -0.121291921 ... 1.34916043 -0.0948562175 -0.616393745]\n",
            "  [-0.269973367 1.33386731 -0.111846715 ... 1.35531044 -0.0828305781 -0.586782932]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  ...\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.265532 1.32722437 -0.116577804 ... 1.35227036 -0.0888525844 -0.601621628]\n",
            "  [-0.265670836 1.32743311 -0.116430223 ... 1.35236657 -0.0886646211 -0.601158917]]]\n",
            "x_nodes [[1.1271131 1.20965731 1.04432476 ... 1.1271131 1.1271131 1.1219461]\n",
            " [1.20965731 1.04432476 1.04432476 ... 1.1219461 1.1271131 1.1271131]\n",
            " [1.20965731 1.1271131 1.20965731 ... 1.1271131 1.1271131 1.1271131]\n",
            " ...\n",
            " [1.1271131 1.20965731 1.04432476 ... 1.1219461 1.1219461 1.1271131]\n",
            " [1.1271131 0.878095806 1.04432476 ... 1.1271131 1.1271131 1.1271131]\n",
            " [1.20965731 1.04432476 1.1271131 ... 1.1271131 1.1271131 1.12452984]] TensorShape([160, 15])\n",
            "Decoded output (llr_hat): [[0.116684519 -2.08043981 1.14376616 ... -1.59305668 1.16244495 -1.98241258]\n",
            " [0.092406027 -2.06060743 0.992171884 ... -1.60842502 1.19483387 -1.87915254]\n",
            " [0.0273998957 -1.80728006 0.914975524 ... -1.59367824 1.15326929 -1.86812735]\n",
            " ...\n",
            " [0.0815802 -2.01073408 0.996789455 ... -1.58187807 1.14826715 -1.91866112]\n",
            " [0.0891424343 -2.00664949 0.965452552 ... -1.48173857 1.26166 -1.77890801]\n",
            " [0.0137388417 -1.96386135 0.981772721 ... -1.66668665 1.22961676 -1.8977046]]\n",
            "PCM @ CW:  [[1 0 1 ... 1 1 1]\n",
            " [0 1 0 ... 1 0 1]\n",
            " [1 0 1 ... 0 1 0]\n",
            " [0 1 1 ... 0 0 0]\n",
            " [0 0 0 ... 1 0 1]]\n",
            "x_nodes input:  [[[-5.11985302]\n",
            "  [-0.0387613773]\n",
            "  [-5.3188653]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [1]]\n",
            "\n",
            " [[7.99221802]\n",
            "  [-6.30502653]\n",
            "  [0.29445982]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [1]]\n",
            "\n",
            " [[-4.30535889]\n",
            "  [-5.79418802]\n",
            "  [8.0676136]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.761674583]\n",
            "  [2.43732834]\n",
            "  [-0.94548]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [0]]\n",
            "\n",
            " [[2.15584207]\n",
            "  [6.29684973]\n",
            "  [2.78620458]\n",
            "  ...\n",
            "  [1]\n",
            "  [1]\n",
            "  [0]]\n",
            "\n",
            " [[1.61757183]\n",
            "  [-1.48812604]\n",
            "  [2.14954972]\n",
            "  ...\n",
            "  [1]\n",
            "  [0]\n",
            "  [0]]]\n",
            "DECODER CALL\n",
            "x_nodes [[[-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457586646 1.5884 -0.16144672 ... 1.55634904 -0.165206432 -0.319999397]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  ...\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]]\n",
            "\n",
            " [[-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.458009779 1.58883739 -0.160866857 ... 1.55640781 -0.164524853 -0.318267226]\n",
            "  ...\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457586646 1.5884 -0.16144672 ... 1.55634904 -0.165206432 -0.319999397]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]]\n",
            "\n",
            " [[-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  ...\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457586646 1.5884 -0.16144672 ... 1.55634904 -0.165206432 -0.319999397]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  ...\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457586646 1.5884 -0.16144672 ... 1.55634904 -0.165206432 -0.319999397]]\n",
            "\n",
            " [[-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  ...\n",
            "  [-0.458009779 1.58883739 -0.160866857 ... 1.55640781 -0.164524853 -0.318267226]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457586646 1.5884 -0.16144672 ... 1.55634904 -0.165206432 -0.319999397]]\n",
            "\n",
            " [[-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  ...\n",
            "  [-0.457163304 1.58796096 -0.162026852 ... 1.55628848 -0.165887952 -0.321731418]\n",
            "  [-0.457586646 1.5884 -0.16144672 ... 1.55634904 -0.165206432 -0.319999397]\n",
            "  [-0.457586646 1.5884 -0.16144672 ... 1.55634904 -0.165206432 -0.319999397]]]\n",
            "x_nodes [[-2.24154878 -2.25078177 -2.24154878 ... -2.24154878 -2.24154878 -2.24154878]\n",
            " [-2.24154878 -2.24154878 -2.26001239 ... -2.24154878 -2.25078177 -2.24154878]\n",
            " [-2.24154878 -2.24154878 -2.24154878 ... -2.24154878 -2.24154878 -2.25078177]\n",
            " ...\n",
            " [-2.24154878 -2.24154878 -2.24154878 ... -2.24154878 -2.24154878 -2.25078177]\n",
            " [-2.24154878 -2.24154878 -2.24154878 ... -2.26001239 -2.24154878 -2.25078177]\n",
            " [-2.24154878 -2.24154878 -2.24154878 ... -2.24154878 -2.25078177 -2.25078177]] TensorShape([160, 15])\n",
            "Decoded output (llr_hat): [[-0.119148552 3.93067145 -1.95142853 ... 3.254035 -2.29966855 3.82892561]\n",
            " [-0.129592061 3.94181848 -1.96042383 ... 3.26205206 -2.3019743 3.83440638]\n",
            " [-0.119433522 3.9326663 -1.94743371 ... 3.24980426 -2.29415655 3.82217813]\n",
            " ...\n",
            " [-0.123674512 3.92716694 -1.94268966 ... 3.24984717 -2.29295707 3.82356286]\n",
            " [-0.116057873 3.93780613 -1.94568229 ... 3.25553727 -2.30356884 3.82271695]\n",
            " [-0.118628919 3.93520451 -1.95423436 ... 3.24930716 -2.29545689 3.81690764]]\n",
            "Training epoch 5/1000, LR=5.00e-04, Loss=4.09999e+02, BER=0.5, duration: 5.43s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCF233HXNUM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}