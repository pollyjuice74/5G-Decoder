{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD7QeglnnwdRO0ODPHgu+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/5G-Decoder/blob/main/LTD_model_reg_LDPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5q1VAmIeUKIn"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pollyjuice74/5G-Decoder\n",
        "!pip install sionna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy.sparse import issparse, csr_matrix\n",
        "\n",
        "from sionna.fec.utils import generate_reg_ldpc, load_parity_check_examples, LinearEncoder, gm2pcm\n",
        "from sionna.utils.plotting import PlotBER\n",
        "from sionna.fec.ldpc import LDPCBPDecoder\n",
        "\n",
        "import os\n",
        "# os.chdir('../..')\n",
        "if os.path.exists('5G-Decoder'):\n",
        "  os.rename('5G-Decoder', '5G_Decoder')\n",
        "os.chdir('5G_Decoder/adv_nn')\n",
        "\n",
        "from dataset import *\n",
        "from attention import *\n",
        "from channel import *\n",
        "from args import *\n",
        "from model_functs import *\n",
        "from models import *"
      ],
      "metadata": {
        "id": "U5U5qUUVUeRm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading LDPC code\")\n",
        "pcm, k, n, coderate = generate_reg_ldpc(v=3,\n",
        "                                        c=6,\n",
        "                                        n=100,\n",
        "                                        allow_flex_len=True,\n",
        "                                        verbose=True)\n",
        "\n",
        "encoder = LinearEncoder(pcm, is_pcm=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "6RF7dBDwWg0L",
        "outputId": "6d7395a2-a141-49b0-e013-119c6d3dc431"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LDPC code\n",
            "Setting n to:  100\n",
            "Number of edges (VN perspective):  300\n",
            "Number of edges (CN perspective):  300\n",
            "Generated regular (3,6) LDPC code of length n=100\n",
            "Code rate is r=0.500.\n",
            "Warning: The alias fec.utils.LinearEncoder will not be included in Sionna 1.0. Please use fec.linear.LinearEncoder instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for e2e model\n",
        "from sionna.utils import BinarySource, ebnodb2no\n",
        "from sionna.mapping import Mapper, Demapper\n",
        "from sionna.channel import AWGN\n",
        "# from sionna.fec.ldpc import LDPC5GDecoder, LDPC5GEncoder\n",
        "\n",
        "\n",
        "class Args():\n",
        "    def __init__(self, model_type, code_type='LDPC', n_look_up=121, k_look_up=80, n=400, k=200,\n",
        "                       n_rings=2, ls_active=True, split_diff=True, sigma=0.1,\n",
        "                       t_layers=1, d_model=128, heads=8, lr=5e-4,\n",
        "                       batch_size=160, batch_size_eval = 150,\n",
        "                       eval_train_iter=10, save_weights_iter=100,\n",
        "                       ebno_db_eval=2.5,\n",
        "                       ebno_db_min=0., ebno_db_max=4., ebno_db_stepsize=0.25,\n",
        "                       traindata_len=500, testdata_len=250, epochs=1000):\n",
        "        assert model_type in ['gen', 'dis'], \"Type must be: 'gen', Generator or 'dis', Discriminator.\"\n",
        "        assert code_type in ['POLAR', 'BCH', 'CCSDS', 'LDPC', 'MACKAY', 'LDPC5G', 'POLAR5G'], \"Invalid linear code type.\"\n",
        "\n",
        "\n",
        "        # model data\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.split_diff = split_diff\n",
        "        self.n_rings = n_rings # ring connectivity of mask\n",
        "        self.sigma = sigma\n",
        "        self.t_layers = t_layers\n",
        "        self.ls_active = ls_active\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "\n",
        "        # training data\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.traindata_len = traindata_len\n",
        "        self.testdata_len = testdata_len\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.ebno_db_min = ebno_db_min\n",
        "        self.ebno_db_max = ebno_db_max\n",
        "        self.ebno_db_stepsize = ebno_db_stepsize\n",
        "\n",
        "        self.ebno_db_eval = ebno_db_eval\n",
        "        self.eval_train_iter = eval_train_iter\n",
        "        self.save_weights_iter = save_weights_iter\n",
        "        self.batch_size_eval = batch_size_eval\n",
        "\n",
        "        # code data\n",
        "        self.code_type = code_type\n",
        "        self.code = self.get_code(n_look_up, k_look_up) # n,k look up values in Get_Generator_and_Parity\n",
        "\n",
        "        # if self.code_type not in ['LDPC5G', 'POLAR5G']:\n",
        "        #     self.n, self.m, self.k = self.code.n, self.code.m, self.code.k\n",
        "        # else:\n",
        "        #     self.n, self.m, self.k = n, n-k, k\n",
        "\n",
        "        # self.n_steps = self.m + 5  # Number of diffusion steps\n",
        "\n",
        "    def get_code(self, n_look_up, k_look_up):\n",
        "        code = type('Code', (), {})() # class Code, no base class, no attributes/methods, () instantiate object\n",
        "        # code.n_look_up, code.k_look_up = n_look_up, k_look_up\n",
        "        # code.code_type = self.code_type\n",
        "\n",
        "        # if self.code_type not in ['LDPC5G', 'POLAR5G']:\n",
        "        #     G, H = Get_Generator_and_Parity(code)\n",
        "        #     code.G, code.H = tf.convert_to_tensor(G), csr_matrix( tf.convert_to_tensor(H) )\n",
        "\n",
        "        #     code.m, code.n = code.H.shape\n",
        "        #     code.k = code.n - code.m\n",
        "\n",
        "        return code\n",
        "\n",
        "\n",
        "class MHAttention(Layer):\n",
        "    def __init__(self, dims, heads, mask_length, linear=True, dropout=0.01):\n",
        "        super().__init__()\n",
        "        assert (dims % heads) == 0, 'dimension must be divisible by the number of heads'\n",
        "        self.linear = linear\n",
        "        self.dims = dims\n",
        "        self.heads = heads\n",
        "        self.dim_head = dims // heads\n",
        "\n",
        "        if linear:\n",
        "            self.k_proj = self.get_k_proj(mask_length) # n+m\n",
        "            self.proj_k = None\n",
        "            self.proj_v = None\n",
        "\n",
        "        self.to_q, self.to_k, self.to_v = [ Dense(self.dims, use_bias=False) for _ in range(3) ]\n",
        "        self.to_out = Dense(dims)\n",
        "        self.dropout = Dropout(dropout) # to d-dimentional embeddings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Creates shape (n,k_proj) proj matrices for key and val in linear attention\n",
        "        n_value = input_shape[1]\n",
        "\n",
        "        if self.linear:\n",
        "            self.proj_k = self.add_weight(\"proj_k\", shape=[n_value, self.k_proj], initializer=GlorotUniform())\n",
        "            self.proj_v = self.add_weight(\"proj_v\", shape=[n_value, self.k_proj], initializer=GlorotUniform())\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        out_att = self.lin_attention(x, mask) if self.linear else self.attention(x, mask)\n",
        "        return out_att\n",
        "\n",
        "    def get_k_proj(self, mask_length):\n",
        "        # gets dimention for linear tranformer vector projection\n",
        "        for k_proj in range(mask_length // 2, 0, -1): # starts at half the mask length TO 0\n",
        "            if mask_length % k_proj == 0:\n",
        "                return tf.cast(k_proj, tf.int32)\n",
        "\n",
        "    def lin_attention(self, x, mask): # O(n)\n",
        "        shape = tf.shape(x) # (b, n, d)\n",
        "        b = tf.cast(shape[0], tf.int32)\n",
        "        n = tf.cast(shape[1], tf.int32)\n",
        "\n",
        "        assert x.shape[-1] is not None, \"The last dimension of x is undefined.\"\n",
        "\n",
        "        query, key, val = self.to_q(x), self.to_k(x), self.to_v(x)\n",
        "\n",
        "        # Project key and val into k-dimentional space\n",
        "        key = tf.einsum('bnd,nk->bkd', key, self.proj_k)\n",
        "        val = tf.einsum('bnd,nk->bkd', val, self.proj_v)\n",
        "\n",
        "        # Reshape splitting for heads\n",
        "        query = tf.reshape(query, (b, n, self.heads, self.dim_head))\n",
        "        key = tf.reshape(key, (b, self.k_proj, self.heads, self.dim_head))\n",
        "        val = tf.reshape(val, (b, self.k_proj, self.heads, self.dim_head))\n",
        "        query, key, val = [ tf.transpose(x, [0, 2, 1, 3]) for x in [query, key, val] ]\n",
        "\n",
        "        # Low-rank mask (n,k_proj)\n",
        "        mask = tf.expand_dims(mask, axis=-1)\n",
        "        mask = tf.image.resize(mask, [n, self.k_proj], method='nearest')\n",
        "        mask = tf.reshape(mask, (1, 1, n, self.k_proj))\n",
        "\n",
        "        # Main attn logic: sftmx( q@k / d**0.5 ) @ v\n",
        "        scores = tf.einsum('bhnd,bhkd->bhnk', query, key) / (tf.sqrt( tf.cast(self.dim_head, dtype=tf.float32) ))\n",
        "        scores += (mask * -1e9) if mask is not None else 0.\n",
        "        attn = tf.nn.softmax(scores, axis=-1) # (b,h,n,k_proj)\n",
        "        attn = self.dropout(attn)\n",
        "        out = tf.einsum('bhnk,bhkd->bhnd', attn, val)\n",
        "\n",
        "        # Reshape and pass through out layer\n",
        "        out = tf.transpose(out, [0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (b, n, -1))\n",
        "        return self.to_out(out)\n",
        "\n",
        "    def attention(self, mask): # O(n^2)\n",
        "        shape = tf.shape(x)\n",
        "        b = shape[0]\n",
        "        n = shape[1]\n",
        "\n",
        "        query, key, val = self.to_q(x), self.to_k(x), self.to_v(x)\n",
        "        query, key, val = [ tf.reshape(x, (b, n, self.heads, self.dim_head)) for x in [query, key, val] ]\n",
        "        query, key, val = [ tf.transpose(x, [0, 2, 1, 3]) for x in [query, key, val] ]\n",
        "\n",
        "        scores = tf.einsum('bhnd,bhnd->bhnn', query, key) / (tf.sqrt(self.dim_head))\n",
        "        scores += (mask * -1e9) if mask is not None else 0. # apply mask non-edge connections\n",
        "        attn = tf.nn.softmax(scores, axis=-1) #-1\n",
        "        attn = self.dropout(attn)\n",
        "        out = tf.einsum('bhnn,bhnm->bhnd', attn, v)\n",
        "\n",
        "        out = tf.transpose(out, [0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (b, n, -1))\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class TransformerDiffusion( Layer ):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.model_type = args.model_type\n",
        "        self.n_steps = args.n_steps\n",
        "\n",
        "        code = args.code\n",
        "        # assert isinstance(code.H, tf.sparse.SparseTensor), \"Code's pcm must be sparse.\"\n",
        "        self.pcm = tf.cast(code.H, dtype=tf.int32)\n",
        "        # shapes\n",
        "        self.m, self.n = self.pcm.shape\n",
        "        self.k = self.n - self.m\n",
        "        self.dims = args.d_model\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.mask = self.create_mask(self.pcm)\n",
        "        # trans_call layers\n",
        "        self.src_embed = tf.Variable( tf.random.uniform([self.dims, self.n + self.m, 1]), trainable=True )\n",
        "        self.decoder = Transformer(args.d_model, args.heads, self.mask, args.t_layers)\n",
        "        self.to_n = Dense(1)\n",
        "        self.to_m = Dense(1)\n",
        "        self.time_embed = Embedding(args.n_steps, args.d_model)\n",
        "        # diff layers\n",
        "        self.fc = Dense(1)\n",
        "\n",
        "        self.betas = tf.constant( tf.linspace(1e-3, 1e-2, args.n_steps)*0 + args.sigma )\n",
        "        self.betas_bar = tf.constant( tf.math.cumsum(self.betas, 0) )\n",
        "\n",
        "        self.split_diff = False#args.split_diff\n",
        "        self.ls_active = args.ls_active\n",
        "\n",
        "        scheduler = tf.keras.optimizers.schedules.CosineDecay( initial_learning_rate=args.lr, decay_steps=args.epochs ) # 1000 is size of trainloader\n",
        "        self.optimizer =  tf.keras.optimizers.Adam(learning_rate=scheduler)\n",
        "\n",
        "    def create_mask(self, H):\n",
        "        m,n = H.shape\n",
        "        mask = tf.eye(n+m, dtype=tf.float32) # (n+m, n+m)\n",
        "        indices = tf.where(H != 0)#H.indices\n",
        "        cn_con, vn_con = indices[:, 0], indices[:, 1]\n",
        "\n",
        "        for cn, vn_i in zip(cn_con, vn_con):\n",
        "            # cn to vn connections in the mask\n",
        "            mask = tf.tensor_scatter_nd_update(mask, [[n+cn, vn_i],[vn_i, n+cn]], [1.0,1.0])\n",
        "\n",
        "            # distance 2 vn neighbors of vn_i\n",
        "            related_vns = vn_con[cn_con==cn]\n",
        "            for vn_j in related_vns:\n",
        "                mask = tf.tensor_scatter_nd_update(mask, [[vn_i, vn_j],[vn_j, vn_i]], [1.0,1.0])\n",
        "\n",
        "        # -infinity where mask is not set\n",
        "        mask = tf.cast( tf.math.logical_not(mask > 0), dtype=tf.float32) # not(mask > 0) for setting non connections to -1e9\n",
        "        return mask\n",
        "\n",
        "    def get_sigma(self, t):\n",
        "        # make sure t is a positive int\n",
        "        t = tf.cast( tf.abs(t), tf.int32 )\n",
        "        # gather betas\n",
        "        betas_t = tf.gather(self.betas, t)\n",
        "        betas_bar_t = tf.gather(self.betas_bar, t)\n",
        "\n",
        "        return betas_bar_t * betas_t / (betas_bar_t + betas_t)\n",
        "\n",
        "    def get_syndrome(self, r_t):\n",
        "        # Calculate syndrome (pcm @ r = 0) if r is correct in binary\n",
        "        r_t = tf.reshape(r_t, (self.n, -1)) # (n,b)\n",
        "        r_t_bin = tf.cast(llr_to_bin(r_t), dtype=tf.int32)\n",
        "        return (self.pcm @ r_t_bin) % 2 # (m,n)@(n,b)->(m,b)\n",
        "\n",
        "    # Extracts noise estimate z_hat from r\n",
        "    def tran_call(self, r_t, t):\n",
        "        # Make sure r_t and t are compatible\n",
        "        r_t = tf.reshape(r_t, (self.n, self.batch_size)) # (n,b)\n",
        "        print(r_t.shape)\n",
        "        t = tf.cast(t, dtype=tf.int32)\n",
        "\n",
        "        # Compute synd and magn\n",
        "        syndrome = tf.reshape( self.get_syndrome(llr_to_bin(r_t)), (self.pcm.shape[0], self.batch_size) ) # (m,n)@(n,b)->(m,b) check nodes\n",
        "        magnitude = tf.reshape( tf.abs(r_t), (self.n, self.batch_size) ) #(n,b) variable nodes\n",
        "        # make sure their the same dtype\n",
        "        magnitude, syndrome = [ tf.cast(tensor, dtype=tf.float32) for tensor in [magnitude, syndrome] ]\n",
        "\n",
        "        # Concatenate synd and magn\n",
        "        nodes = tf.concat([magnitude, syndrome], axis=0) # data for vertices\n",
        "        nodes = tf.reshape(nodes, (1, self.n+self.m, self.batch_size)) # (1, n+m, b)\n",
        "        # print(nodes.shape)\n",
        "\n",
        "        print(self.src_embed.shape)\n",
        "        # Embedding nodes w/ attn and 'time' (sum syn errs) dims\n",
        "        nodes_emb = tf.reshape( self.src_embed * nodes, (self.src_embed.shape[0], self.pcm.shape[0]+self.n, self.batch_size) ) # (d,n+m,b)\n",
        "        time_emb = tf.reshape( self.time_embed(t), (self.src_embed.shape[0], 1, self.batch_size) ) # (d,1,b)\n",
        "        print(nodes_emb.shape, time_emb.shape)\n",
        "\n",
        "        # Applying embeds\n",
        "        emb_t = time_emb * nodes_emb # (d, n+m, b)\n",
        "        emb_t = tf.transpose(emb_t, (2, 1, 0)) # (d, n+m, b)-> (b, n+m, d)\n",
        "        print(emb_t.shape)\n",
        "        logits = self.decoder(emb_t) # (b, n+m, d) # TODO: missing batch dims b\n",
        "        logits = tf.transpose(logits, (2, 1, 0)) # (b, n+m, d)-> (d, n+m, b)\n",
        "        print(\"logits: \", logits.shape)\n",
        "\n",
        "        # Reduce (d,n+m,d)->(d,n+m)\n",
        "        # logits = tf.squeeze( self.fc(logits), axis=-1 )\n",
        "        vn_logits = tf.reshape( logits[:, :self.n, :], (self.n, self.batch_size, self.dims) ) # (n,b, d) take the first n logits from the concatenation\n",
        "        cn_logits = tf.reshape( logits[:, self.n:, :], (self.m, self.batch_size, self.dims) ) # (m,b, d) take the last m logits from the concatenation\n",
        "        # print(vn_logits, cn_logits)\n",
        "\n",
        "        z_hat = tf.squeeze( self.to_n(vn_logits), axis=-1 )# (n,b, d)->(n, b)\n",
        "        synd = tf.squeeze( self.to_m(cn_logits), axis=-1 )# (m,b, d)->(m, b)\n",
        "        print(z_hat.shape, synd.shape)\n",
        "\n",
        "        return z_hat, synd\n",
        "\n",
        "    # optimal lambda l for theoretical and for error prediction\n",
        "    def line_search(self, r_t, sigma, err_hat, lin_splits=20):\n",
        "        l_values =  tf.reshape( tf.linspace(1., 20., lin_splits), (1, 1, lin_splits) )\n",
        "        r_t, sigma, err_hat = [ tf.expand_dims(tensor, axis=-1) for tensor in [r_t, sigma, err_hat] ]# (n,b, 1)\n",
        "        # print(f\"sigma: {sigma}, err_hat: {err_hat}\")\n",
        "\n",
        "        # Compute theoretical step size w/ ls splits\n",
        "        z_hat_values = l_values*(sigma*err_hat) # (n,b, l), l is lin_splits\n",
        "        r_values = llr_to_bin(r_t - z_hat_values) # (n,b, l)\n",
        "        r_values = tf.reshape(r_values, [r_values.shape[0], -1]) # (n,b*l)\n",
        "\n",
        "        # sum of synds (m,n)@(n,b*l)->(m,b*l)->(b*l, 1)\n",
        "        sum_synds = tf.reduce_sum( tf.abs( (self.pcm @ r_values) % 2 ),\n",
        "                                   axis=0 )\n",
        "        sum_synds = tf.reshape(sum_synds, (-1, lin_splits)) # (b, l)\n",
        "        # print(sum_synds.shape)\n",
        "\n",
        "        # Pick optimal ls value\n",
        "        if self.model_type=='dis':\n",
        "             ixs = tf.math.argmin(sum_synds, axis=1, output_type=tf.int32)[:, tf.newaxis] # (b,1) w/ ixs of optimal line search for batch b\n",
        "        elif self.model_type=='gen':\n",
        "             ixs = tf.math.argmax(sum_synds, axis=1, output_type=tf.int32)[:, tf.newaxis] # (b,1)\n",
        "\n",
        "        # print(r_values.shape, z_hat_values.shape, ixs.shape)\n",
        "        # (b, l, n) for indexing on l\n",
        "        r_values, z_hat_values = [ tf.reshape(tensor, [-1, lin_splits, r_values.shape[0]])\n",
        "                                            for tensor in [r_values, z_hat_values] ]\n",
        "\n",
        "        # concat range of batch ixs [0,...,n-1] and optimal line search ixs for gather_nd\n",
        "        indices = tf.concat([ tf.range(ixs.shape[0])[:, tf.newaxis], ixs ],\n",
        "                                                            axis=-1) # (b,2)\n",
        "\n",
        "        # print(r_values, z_hat_values, indices)\n",
        "        # ix on lin_splits w/ gather_nd st. ix,(b, l, n)->(n,b)\n",
        "        r_t1, z_hat = [ tf.reshape( tf.gather_nd(tensor, indices), (self.n, -1) )\n",
        "                                             for tensor in [r_values, z_hat_values] ]\n",
        "        # print(r_t1, z_hat_values)\n",
        "        return r_t1, z_hat # r at t-1\n",
        "\n",
        "    def loss_fn(self, synd):\n",
        "        return tf.reduce_mean(tf.square(synd))\n",
        "\n",
        "    def train_step(self, llr_ch):\n",
        "        with tf.GradientTape() as tape:\n",
        "            _, synd = self.tran_call(llr_ch,\n",
        "                                     tf.reduce_sum( self.get_syndrome(llr_ch) ))\n",
        "            loss = self.loss_fn(synd)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    # def train(self, r_t, struct_noise=0, sim_ampl=True):\n",
        "    #     # t = tf.random.uniform( (c_0.shape[0] // 2 + 1,), minval=0,maxval=self.n_steps, dtype=tf.int32 )\n",
        "    #     # t = tf.concat([t, self.n_steps - t - 1], axis=0)[:c_0.shape[0]] # reshapes t to size x_0\n",
        "    #     # t = tf.cast(t, dtype=tf.int32)\n",
        "\n",
        "    #     # noise_factor = tf.math.sqrt( tf.gather(self.betas_bar, t) )\n",
        "    #     # noise_factor = tf.reshape(noise_factor, (-1, 1))\n",
        "    #     # z = tf.random.normal(c_0.shape)\n",
        "    #     # h = np.random.rayleigh(size=c_0.shape)if sim_ampl else 1.\n",
        "\n",
        "    #     # added noise to codeword\n",
        "    #     # c_t = tf.transpose(h * c_0 + struct_noise + (z*noise_factor))\n",
        "    #     # calculate sum of syndrome\n",
        "    #     t = tf.math.reduce_sum( self.get_syndrome( llr_to_bin(tf.sign(c_t)) ), axis=0 ) # (batch_size, 1)\n",
        "\n",
        "    #     z_hat = self.tran_call(c_t, t) # model prediction\n",
        "\n",
        "    #     if self.model_type=='dis':\n",
        "    #         z_mul = c_t * tf.transpose(c_0) # actual noise added through the channel\n",
        "\n",
        "    #     elif self.model_type=='gen':\n",
        "    #         c_t += z_hat # could contain positive or negative values\n",
        "    #         z_mul = c_t * tf.transpose(c_0) # moidfied channel noise st. it will fool the discriminator\n",
        "\n",
        "    #     z_mul = tf.reshape(z_mul, (z_hat.shape[0], -1))\n",
        "    #     return c_hat, synd #z_hat, llr_to_bin(z_mul), c_t\n",
        "\n",
        "# Construct discriminator (decoder using reverse diffusion)\n",
        "    # Will have to come up with ways to try to decode the noised codeword against specific noise\n",
        "    # that will be trying to fool it.\n",
        "\n",
        "    # For optimization:\n",
        "        # use Linformer having a O(n) on top of already improved complexity using the pcm mask\n",
        "        # use split diffusion to improve accuracy and efficiency by guiding model rather than EMA\n",
        "\n",
        "class Decoder( TransformerDiffusion ):\n",
        "    def __init__(self, args):\n",
        "        super().__init__(args)\n",
        "\n",
        "    # 'test' function\n",
        "    def call(self, r_t):\n",
        "        i = tf.constant(0)  # Initialize loop counter\n",
        "        z_hat = tf.zeros_like(r_t)  # Placeholder for z_hat\n",
        "\n",
        "        def condition(i, r_t, z_hat):\n",
        "            # Loop while i < self.m and syndrome sum is not zero\n",
        "            return tf.logical_and(i < self.m, tf.reduce_sum(self.get_syndrome(r_t)) != 0)\n",
        "\n",
        "        def body(i, r_t, z_hat):\n",
        "            # Perform reverse or split diffusion\n",
        "            r_t, z_hat = tf.cond(\n",
        "                tf.logical_not(self.split_diff),\n",
        "                lambda: self.rev_diff_call(r_t),\n",
        "                lambda: self.split_rdiff_call(r_t),\n",
        "            )\n",
        "            return tf.add(i, 1), r_t, z_hat\n",
        "\n",
        "        # Run tf.while_loop with the loop variables\n",
        "        i, final_r_t, final_z_hat = tf.while_loop(\n",
        "            condition,\n",
        "            body,\n",
        "            loop_vars=[i, r_t, z_hat],\n",
        "            maximum_iterations=self.n_steps,\n",
        "            # shape_invariants=[i.get_shape(), tf.TensorShape([None, None]), z_hat.get_shape()]\n",
        "        )\n",
        "\n",
        "        return final_r_t, final_z_hat, i\n",
        "\n",
        "\n",
        "    # Refines recieved codeword r at time t\n",
        "    def rev_diff_call(self, r_t):\n",
        "        print(\"Rev def call with line-search...\")\n",
        "        # Make sure r_t and t are compatible\n",
        "        r_t = tf.reshape(r_t, (self.n, -1)) # (n,b)\n",
        "        # 'time step' of diffusion is really ix of abs(sum synd errors)\n",
        "        t = tf.reduce_sum( self.get_syndrome(llr_to_bin(r_t)), axis=0 ) # (m,n)@(n,b)->(m,b)->(1,b)\n",
        "        t = tf.cast(tf.abs(t), dtype=tf.int32)\n",
        "\n",
        "        # Transformer error prediction\n",
        "        z_hat_crude, synd = self.tran_call(r_t, t) # (n,1), (m,1)\n",
        "        # print(\"z_hat_crude: \", z_hat_crude)\n",
        "\n",
        "        # Compute diffusion vars\n",
        "        sigma = self.get_sigma(t) # theoretical step size\n",
        "        # print(\"sigma: \", sigma)\n",
        "        err_hat = r_t - tf.sign(z_hat_crude * r_t) # (n,1)\n",
        "\n",
        "        # Refined estimate of the codeword for the ls diffusion step\n",
        "        r_t1, z_hat = self.line_search(r_t, sigma, err_hat) if self.ls_active else 1.\n",
        "\n",
        "        # Cast both outputs to float32 for consistency\n",
        "        r_t1, z_hat = [ tf.cast(tensor, tf.float32) for tensor in [r_t1, z_hat] ]\n",
        "        # # reshape to (n,b) for consistency\n",
        "        r_t1, z_hat = [ tf.reshape( tensor, (self.n, self.batch_size) )\n",
        "                                             for tensor in [r_t1, z_hat] ]\n",
        "\n",
        "        return r_t1, z_hat # r at t-1, both (n,1)\n",
        "\n",
        "    def split_rdiff_call(self, r_t):\n",
        "        print(\"Rev diff call with split diffusion...\")\n",
        "        # Ensure r_t is correctly shaped\n",
        "        r_t = tf.reshape(r_t, (self.n, -1))  # (n,b)\n",
        "        print(r_t.shape)\n",
        "        t = tf.reduce_sum(self.get_syndrome(llr_to_bin(r_t)), axis=0)  # (m,n)@(n,b)->(m,b)->(1,b)\n",
        "        t = tf.cast(tf.abs(t), dtype=tf.int32)\n",
        "\n",
        "        # First half-step condition subproblem\n",
        "        print(r_t.shape, t)\n",
        "        z_hat_crude, synd = self.tran_call(r_t, t)\n",
        "        print(\"fc input: \", (z_hat_crude * self.get_sigma(t)).shape)\n",
        "        r_t_half = r_t - 0.5 * self.fc(z_hat_crude * self.get_sigma(t))\n",
        "        print(r_t_half.shape)\n",
        "\n",
        "        # Full-step diffusion subproblem\n",
        "        r_t1 = r_t_half + tf.random.normal(r_t_half.shape) * tf.sqrt(self.get_sigma(t))\n",
        "\n",
        "        # Second half-step condition subproblem\n",
        "        z_hat_crude_half, synd = self.tran_call(r_t1, t)  # Reuse the second `tran_call`\n",
        "        r_t1 = r_t1 - 0.5 * self.fc(z_hat_crude_half * self.get_sigma(t))\n",
        "\n",
        "        # Cast both outputs to float32 for consistency\n",
        "        r_t1, z_hat_crude_half = [ tf.cast(tensor, tf.float32) for tensor in [r_t1, z_hat_crude_half] ]\n",
        "        # # reshape to (n,b) for consistency\n",
        "        r_t1, z_hat_crude_half = [ tf.reshape( tensor, (self.n, self.batch_size) )\n",
        "                                             for tensor in [r_t1, z_hat_crude_half] ]\n",
        "        print(r_t1.shape, z_hat_crude_half.shape)\n",
        "        return r_t1, z_hat_crude_half  # r at t-1, both (n,1)\n",
        "\n",
        "\n",
        "class E2EModel(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, k, n, return_infobits=False, es_no=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self._n = n\n",
        "        self._k = k\n",
        "\n",
        "        self._binary_source = BinarySource()\n",
        "        self._num_bits_per_symbol = 2\n",
        "        self._mapper = Mapper(\"qam\", self._num_bits_per_symbol)\n",
        "        self._demapper = Demapper(\"app\", \"qam\", self._num_bits_per_symbol)\n",
        "        self._channel = AWGN()\n",
        "        self._decoder = decoder\n",
        "        self._encoder = encoder\n",
        "        self._return_infobits = return_infobits\n",
        "        self._es_no = es_no\n",
        "\n",
        "    @tf.function(jit_compile=False)\n",
        "    def call(self, batch_size, ebno_db):\n",
        "\n",
        "        # no rate-adjustment for uncoded transmission or es_no scenario\n",
        "        if self._decoder is not None and self._es_no==False:\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, self._k/self._n)\n",
        "        else: #for uncoded transmissions the rate is 1\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, 1)\n",
        "\n",
        "        b = self._binary_source([batch_size, self._k])\n",
        "        if self._encoder is not None:\n",
        "            c = self._encoder(b)\n",
        "        else:\n",
        "            c = b\n",
        "\n",
        "        # check that rate calculations are correct\n",
        "        assert self._n==c.shape[-1], \"Invalid value of n.\"\n",
        "\n",
        "        # zero padding to support odd codeword lengths\n",
        "        if self._n%2==1:\n",
        "            c_pad = tf.concat([c, tf.zeros([batch_size, 1])], axis=1)\n",
        "        else: # no padding\n",
        "            c_pad = c\n",
        "        x = self._mapper(c_pad)\n",
        "\n",
        "        y = self._channel([x, no])\n",
        "        llr = self._demapper([y, no])\n",
        "\n",
        "        # remove zero padded bit at the end\n",
        "        if self._n%2==1:\n",
        "            llr = llr[:,:-1]\n",
        "\n",
        "        # and run the decoder\n",
        "        if self._decoder is not None:\n",
        "            llr = tf.transpose(llr, (1,0)) # (b,n)->(n,b)\n",
        "            print('llr: ', llr.shape)\n",
        "            llr_hat, z_hat, _ = self._decoder(llr)\n",
        "            llr_hat = tf.transpose(llr_hat, (1,0)) # (n,b)->(b,n)\n",
        "            print(\"llr_hat: \", llr_hat.shape)\n",
        "            print(\"z_hat: \", z_hat.shape)\n",
        "\n",
        "            # z_hat, z_mul, c_t = model.train(x)\n",
        "            # loss = loss_fn(z_hat, z_mul)\n",
        "\n",
        "        if self._return_infobits:\n",
        "            return b, llr_hat\n",
        "        else:\n",
        "            return c, llr_hat\n",
        "\n",
        "\n",
        "# args for decoder/discriminator\n",
        "args = Args(model_type='dis')\n",
        "args.code.H = pcm\n",
        "args.n, args.m = pcm.shape\n",
        "args.k = k\n",
        "args.n_steps = args.m + 5\n",
        "\n",
        "ltd_decoder = Decoder(args) # Linear Transformer Diffusion (LTD) Decoder\n",
        "\n",
        "e2e_ltd = E2EModel(encoder, ltd_decoder, k, n)"
      ],
      "metadata": {
        "id": "XOILyjSGXMdb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dec(model, args):\n",
        "    # loss\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "    # optimizer\n",
        "    scheduler = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=args.lr, decay_steps=args.epochs) # 1000 is size of trainloader\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler)\n",
        "    # time start\n",
        "    time_start = time.time()\n",
        "\n",
        "    # SGD update iteration\n",
        "    @tf.function(jit_compile=False)\n",
        "    def train_step(batch_size):\n",
        "        # train for random SNRs within a pre-defined interval\n",
        "        ebno_db = tf.random.uniform([batch_size, 1],\n",
        "                                    minval=args.ebno_db_min,\n",
        "                                    maxval=args.ebno_db_max)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            c, llr_hat = model(batch_size, ebno_db)\n",
        "            print(c, llr_hat)\n",
        "\n",
        "            loss_value = loss_fn(c, llr_hat)\n",
        "\n",
        "        # and apply the SGD updates\n",
        "        weights = model.trainable_weights\n",
        "        grads = tape.gradient(loss_value, weights) # variables\n",
        "        optimizer.apply_gradients(zip(grads, weights))\n",
        "        return c, llr_hat\n",
        "\n",
        "    print(\"Training Linear Transformer Diffusion Model...\")\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_step(args.batch_size)\n",
        "\n",
        "        # eval train iter\n",
        "        if epoch % args.eval_train_iter == 0:\n",
        "            ebno_db = tf.random.uniform([args.batch_size, 1],\n",
        "                                          minval=args.ebno_db_eval,\n",
        "                                          maxval=args.ebno_db_eval)\n",
        "\n",
        "            c, llr_hat = model(args.batch_size, ebno_db)\n",
        "\n",
        "            print(c, llr_hat)\n",
        "            loss_value = loss_fn(c, llr_hat)\n",
        "            # for _, l in llr_hat:\n",
        "            #     loss_value += loss(c, l)\n",
        "\n",
        "            c_hat = tf.cast(tf.greater(llr_hat[-1], 0), tf.float32)\n",
        "            ber = compute_ber(c, c_hat).numpy()\n",
        "\n",
        "            # measure required time since last evaluation\n",
        "            duration = time.time() - time_start # in s\n",
        "            time_start = time.time() # reset counter\n",
        "\n",
        "            print(f'Training epoch {epoch}/{args.epochs}, LR={optimizer.learning_rate.numpy():.2e}, Loss={loss_value.numpy():.5e}, BER={ber}, duration: {duration:.2f}s')\n",
        "\n",
        "        # save weights iter\n",
        "        if epoch % args.save_weights_iter == 0:\n",
        "            pass\n",
        "\n",
        "        # heat-map visualization of the model's weights\n",
        "        # for var in self.trainable_variables:\n",
        "        #     var_name = var.name\n",
        "        #     var_value = var.numpy()\n",
        "\n",
        "        #     # Check if the variable is at least 2D (suitable for heatmap)\n",
        "        #     if len(var_value.shape) > 1:\n",
        "        #         plt.figure(figsize=(8, 6))\n",
        "        #         sns.heatmap(var_value, cmap='viridis')\n",
        "        #         plt.title(f'Heatmap of {var_name}')\n",
        "        #         plt.show()\n",
        "        #     else:\n",
        "        #         print(f\"{var_name} has shape {var_value.shape} which is not suitable for a heatmap.\")\n",
        "\n",
        "\n",
        "train_dec(e2e_ltd, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NhZncoEEpgNv",
        "outputId": "54e8b9b0-e07e-4b49-ac83-8ba28ca044c7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Linear Transformer Diffusion Model...\n",
            "llr:  (100, 160)\n",
            "Rev def call with line-search...\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "Rev diff call with split diffusion...\n",
            "(100, 160)\n",
            "(100, 160) Tensor(\"decoder_14/while/cond/Abs:0\", shape=(160,), dtype=int32)\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "fc input:  (100, 160)\n",
            "(100, 160)\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "(100, 160) (100, 160)\n",
            "llr_hat:  (160, 100)\n",
            "z_hat:  (100, 160)\n",
            "llr:  (100, 160)\n",
            "Rev def call with line-search...\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "Rev diff call with split diffusion...\n",
            "(100, 160)\n",
            "(100, 160) Tensor(\"decoder_14/while/cond/Abs:0\", shape=(160,), dtype=int32)\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "fc input:  (100, 160)\n",
            "(100, 160)\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "(100, 160) (100, 160)\n",
            "llr_hat:  (160, 100)\n",
            "z_hat:  (100, 160)\n",
            "Tensor(\"e2e_model_14/StatefulPartitionedCall:0\", shape=(160, 100), dtype=float32) Tensor(\"e2e_model_14/StatefulPartitionedCall:1\", shape=(160, 100), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"e2e_model_14/StatefulPartitionedCall:0\", shape=(160, 100), dtype=float32) Tensor(\"e2e_model_14/StatefulPartitionedCall:1\", shape=(160, 100), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0. 1. 1. ... 1. 1. 0.]\n",
            " [1. 1. 0. ... 1. 1. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 1. ... 0. 0. 1.]\n",
            " [1. 1. 0. ... 0. 0. 1.]\n",
            " [0. 0. 1. ... 0. 0. 0.]], shape=(160, 100), dtype=float32) tf.Tensor(\n",
            "[[1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " ...\n",
            " [0. 1. 1. ... 1. 0. 0.]\n",
            " [0. 1. 1. ... 1. 0. 0.]\n",
            " [0. 1. 1. ... 1. 0. 0.]], shape=(160, 100), dtype=float32)\n",
            "Training epoch 10/1000, LR=5.00e-04, Loss=7.78417e+00, BER=0.5005, duration: 30.25s\n",
            "tf.Tensor(\n",
            "[[0. 0. 1. ... 0. 0. 1.]\n",
            " [1. 1. 1. ... 1. 0. 0.]\n",
            " [0. 1. 1. ... 1. 1. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 1.]\n",
            " [1. 0. 0. ... 1. 1. 0.]\n",
            " [1. 1. 0. ... 0. 1. 1.]], shape=(160, 100), dtype=float32) tf.Tensor(\n",
            "[[1. 1. 0. ... 0. 0. 0.]\n",
            " [1. 1. 0. ... 0. 0. 0.]\n",
            " [1. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]], shape=(160, 100), dtype=float32)\n",
            "Training epoch 20/1000, LR=5.00e-04, Loss=7.64884e+00, BER=0.501375, duration: 18.09s\n",
            "tf.Tensor(\n",
            "[[0. 0. 1. ... 0. 1. 0.]\n",
            " [1. 1. 0. ... 1. 0. 0.]\n",
            " [1. 1. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 1. 0. 1.]\n",
            " [1. 1. 1. ... 1. 0. 1.]\n",
            " [1. 1. 0. ... 1. 1. 0.]], shape=(160, 100), dtype=float32) tf.Tensor(\n",
            "[[1. 1. 0. ... 1. 0. 1.]\n",
            " [1. 1. 0. ... 1. 0. 1.]\n",
            " [1. 1. 0. ... 1. 0. 1.]\n",
            " ...\n",
            " [1. 1. 1. ... 1. 0. 1.]\n",
            " [1. 1. 1. ... 1. 0. 1.]\n",
            " [1. 1. 1. ... 1. 0. 1.]], shape=(160, 100), dtype=float32)\n",
            "Training epoch 30/1000, LR=4.99e-04, Loss=7.56309e+00, BER=0.505625, duration: 16.83s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3edb022b8503>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mtrain_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2e_ltd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-3edb022b8503>\u001b[0m in \u001b[0;36mtrain_dec\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Linear Transformer Diffusion Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# eval train iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuPmknHQuxFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}