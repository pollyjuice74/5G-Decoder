{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIZj7XwsD6RLyAVumNUaMx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/5G-Decoder/blob/main/LTD_model_reg_LDPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "5q1VAmIeUKIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8922a61-910c-4664-91c6-ad55463d3e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '5G-Decoder'...\n",
            "remote: Enumerating objects: 1358, done.\u001b[K\n",
            "remote: Counting objects: 100% (361/361), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 1358 (delta 242), reused 239 (delta 168), pack-reused 997 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1358/1358), 1.55 MiB | 6.23 MiB/s, done.\n",
            "Resolving deltas: 100% (842/842), done.\n",
            "Collecting sionna\n",
            "  Downloading sionna-0.19.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting tensorflow<2.16.0,>=2.13.0 (from sionna)\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sionna) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from sionna) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sionna) (1.13.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from sionna) (6.4.5)\n",
            "Collecting mitsuba>=3.2.0 (from sionna)\n",
            "  Downloading mitsuba-3.5.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pythreejs>=2.4.2 (from sionna)\n",
            "  Downloading pythreejs-2.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from sionna)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting ipydatawidgets==4.3.2 (from sionna)\n",
            "  Downloading ipydatawidgets-4.3.2-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting jupyterlab-widgets==3.0.5 (from sionna)\n",
            "  Downloading jupyterlab_widgets-3.0.5-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: traittypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipydatawidgets==4.3.2->sionna) (0.2.1)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->sionna)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.0.4->sionna)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: pip is looking at multiple versions of ipywidgets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting ipywidgets>=8.0.4 (from sionna)\n",
            "  Downloading ipywidgets-8.1.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (5.5.6)\n",
            "  Downloading ipywidgets-8.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "INFO: pip is still looking at multiple versions of ipywidgets to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading ipywidgets-8.0.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting drjit==0.4.6 (from mitsuba>=3.2.0->sionna)\n",
            "  Downloading drjit-0.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16.0,>=2.13.0->sionna) (0.44.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.2.2)\n",
            "Downloading sionna-0.19.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipydatawidgets-4.3.2-py2.py3-none-any.whl (271 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_widgets-3.0.5-py3-none-any.whl (384 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.3/384.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.0.5-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mitsuba-3.5.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (40.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading drjit-0.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pythreejs-2.4.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: wrapt, widgetsnbextension, tensorflow-estimator, ml-dtypes, keras, jupyterlab-widgets, jedi, drjit, mitsuba, ipywidgets, tensorboard, ipydatawidgets, tensorflow, pythreejs, sionna\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.9\n",
            "    Uninstalling widgetsnbextension-3.6.9:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.9\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: jupyterlab-widgets\n",
            "    Found existing installation: jupyterlab_widgets 3.0.13\n",
            "    Uninstalling jupyterlab_widgets-3.0.13:\n",
            "      Successfully uninstalled jupyterlab_widgets-3.0.13\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed drjit-0.4.6 ipydatawidgets-4.3.2 ipywidgets-8.0.5 jedi-0.19.1 jupyterlab-widgets-3.0.5 keras-2.15.0 mitsuba-3.5.2 ml-dtypes-0.3.2 pythreejs-2.4.2 sionna-0.19.0 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 widgetsnbextension-4.0.13 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pollyjuice74/5G-Decoder\n",
        "!pip install sionna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy.sparse import issparse, csr_matrix\n",
        "\n",
        "from sionna.fec.utils import generate_reg_ldpc, load_parity_check_examples, LinearEncoder, gm2pcm\n",
        "from sionna.utils.plotting import PlotBER\n",
        "from sionna.fec.ldpc import LDPCBPDecoder\n",
        "\n",
        "import os\n",
        "# os.chdir('../..')\n",
        "if os.path.exists('5G-Decoder'):\n",
        "  os.rename('5G-Decoder', '5G_Decoder')\n",
        "os.chdir('5G_Decoder/adv_nn')\n",
        "\n",
        "from dataset import *\n",
        "from attention import *\n",
        "from channel import *\n",
        "from args import *\n",
        "from model_functs import *\n",
        "from models import *"
      ],
      "metadata": {
        "id": "U5U5qUUVUeRm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading LDPC code\")\n",
        "pcm, k, n, coderate = generate_reg_ldpc(v=3,\n",
        "                                        c=6,\n",
        "                                        n=100,\n",
        "                                        allow_flex_len=True,\n",
        "                                        verbose=True)\n",
        "\n",
        "encoder = LinearEncoder(pcm, is_pcm=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "6RF7dBDwWg0L",
        "outputId": "36111a94-9432-48a9-e4d7-5989ea0df725"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LDPC code\n",
            "Setting n to:  100\n",
            "Number of edges (VN perspective):  300\n",
            "Number of edges (CN perspective):  300\n",
            "Generated regular (3,6) LDPC code of length n=100\n",
            "Code rate is r=0.500.\n",
            "Warning: The alias fec.utils.LinearEncoder will not be included in Sionna 1.0. Please use fec.linear.LinearEncoder instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for e2e model\n",
        "from sionna.utils import BinarySource, ebnodb2no\n",
        "from sionna.mapping import Mapper, Demapper\n",
        "from sionna.channel import AWGN\n",
        "# from sionna.fec.ldpc import LDPC5GDecoder, LDPC5GEncoder\n",
        "\n",
        "\n",
        "class Args():\n",
        "    def __init__(self, model_type, code_type='LDPC', n_look_up=121, k_look_up=80, n=400, k=200,\n",
        "                       n_rings=2, ls_active=True, split_diff=True, sigma=0.1,\n",
        "                       t_layers=1, d_model=128, heads=8, lr=5e-4,\n",
        "                       batch_size=160, batch_size_eval = 150,\n",
        "                       eval_train_iter=10, save_weights_iter=100,\n",
        "                       ebno_db_eval=2.5,\n",
        "                       ebno_db_min=0., ebno_db_max=4., ebno_db_stepsize=0.25,\n",
        "                       traindata_len=500, testdata_len=250, epochs=1000):\n",
        "        assert model_type in ['gen', 'dis'], \"Type must be: 'gen', Generator or 'dis', Discriminator.\"\n",
        "        assert code_type in ['POLAR', 'BCH', 'CCSDS', 'LDPC', 'MACKAY', 'LDPC5G', 'POLAR5G'], \"Invalid linear code type.\"\n",
        "\n",
        "\n",
        "        # model data\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.split_diff = split_diff\n",
        "        self.n_rings = n_rings # ring connectivity of mask\n",
        "        self.sigma = sigma\n",
        "        self.t_layers = t_layers\n",
        "        self.ls_active = ls_active\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "\n",
        "        # training data\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.traindata_len = traindata_len\n",
        "        self.testdata_len = testdata_len\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.ebno_db_min = ebno_db_min\n",
        "        self.ebno_db_max = ebno_db_max\n",
        "        self.ebno_db_stepsize = ebno_db_stepsize\n",
        "\n",
        "        self.ebno_db_eval = ebno_db_eval\n",
        "        self.eval_train_iter = eval_train_iter\n",
        "        self.save_weights_iter = save_weights_iter\n",
        "        self.batch_size_eval = batch_size_eval\n",
        "\n",
        "        # code data\n",
        "        self.code_type = code_type\n",
        "        self.code = self.get_code(n_look_up, k_look_up) # n,k look up values in Get_Generator_and_Parity\n",
        "\n",
        "        # if self.code_type not in ['LDPC5G', 'POLAR5G']:\n",
        "        #     self.n, self.m, self.k = self.code.n, self.code.m, self.code.k\n",
        "        # else:\n",
        "        #     self.n, self.m, self.k = n, n-k, k\n",
        "\n",
        "        # self.n_steps = self.m + 5  # Number of diffusion steps\n",
        "\n",
        "    def get_code(self, n_look_up, k_look_up):\n",
        "        code = type('Code', (), {})() # class Code, no base class, no attributes/methods, () instantiate object\n",
        "        # code.n_look_up, code.k_look_up = n_look_up, k_look_up\n",
        "        # code.code_type = self.code_type\n",
        "\n",
        "        # if self.code_type not in ['LDPC5G', 'POLAR5G']:\n",
        "        #     G, H = Get_Generator_and_Parity(code)\n",
        "        #     code.G, code.H = tf.convert_to_tensor(G), csr_matrix( tf.convert_to_tensor(H) )\n",
        "\n",
        "        #     code.m, code.n = code.H.shape\n",
        "        #     code.k = code.n - code.m\n",
        "\n",
        "        return code\n",
        "\n",
        "\n",
        "class MHAttention(Layer):\n",
        "    def __init__(self, dims, heads, mask_length, linear=True, dropout=0.01):\n",
        "        super().__init__()\n",
        "        assert (dims % heads) == 0, 'dimension must be divisible by the number of heads'\n",
        "        self.linear = linear\n",
        "        self.dims = dims\n",
        "        self.heads = heads\n",
        "        self.dim_head = dims // heads\n",
        "\n",
        "        if linear:\n",
        "            self.k_proj = self.get_k_proj(mask_length) # n+m\n",
        "            self.proj_k = None\n",
        "            self.proj_v = None\n",
        "\n",
        "        self.to_q, self.to_k, self.to_v = [ Dense(self.dims, use_bias=False) for _ in range(3) ]\n",
        "        self.to_out = Dense(dims)\n",
        "        self.dropout = Dropout(dropout) # to d-dimentional embeddings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Creates shape (n,k_proj) proj matrices for key and val in linear attention\n",
        "        n_value = input_shape[1]\n",
        "\n",
        "        if self.linear:\n",
        "            self.proj_k = self.add_weight(\"proj_k\", shape=[n_value, self.k_proj], initializer=GlorotUniform())\n",
        "            self.proj_v = self.add_weight(\"proj_v\", shape=[n_value, self.k_proj], initializer=GlorotUniform())\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        out_att = self.lin_attention(x, mask) if self.linear else self.attention(x, mask)\n",
        "        return out_att\n",
        "\n",
        "    def get_k_proj(self, mask_length):\n",
        "        # gets dimention for linear tranformer vector projection\n",
        "        for k_proj in range(mask_length // 2, 0, -1): # starts at half the mask length TO 0\n",
        "            if mask_length % k_proj == 0:\n",
        "                return tf.cast(k_proj, tf.int32)\n",
        "\n",
        "    def lin_attention(self, x, mask): # O(n)\n",
        "        shape = tf.shape(x) # (b, n, d)\n",
        "        b = tf.cast(shape[0], tf.int32)\n",
        "        n = tf.cast(shape[1], tf.int32)\n",
        "\n",
        "        assert x.shape[-1] is not None, \"The last dimension of x is undefined.\"\n",
        "\n",
        "        query, key, val = self.to_q(x), self.to_k(x), self.to_v(x)\n",
        "\n",
        "        # Project key and val into k-dimentional space\n",
        "        key = tf.einsum('bnd,nk->bkd', key, self.proj_k)\n",
        "        val = tf.einsum('bnd,nk->bkd', val, self.proj_v)\n",
        "\n",
        "        # Reshape splitting for heads\n",
        "        query = tf.reshape(query, (b, n, self.heads, self.dim_head))\n",
        "        key = tf.reshape(key, (b, self.k_proj, self.heads, self.dim_head))\n",
        "        val = tf.reshape(val, (b, self.k_proj, self.heads, self.dim_head))\n",
        "        query, key, val = [ tf.transpose(x, [0, 2, 1, 3]) for x in [query, key, val] ]\n",
        "\n",
        "        # Low-rank mask (n,k_proj)\n",
        "        mask = tf.expand_dims(mask, axis=-1)\n",
        "        mask = tf.image.resize(mask, [n, self.k_proj], method='nearest')\n",
        "        mask = tf.reshape(mask, (1, 1, n, self.k_proj))\n",
        "\n",
        "        # Main attn logic: sftmx( q@k / d**0.5 ) @ v\n",
        "        scores = tf.einsum('bhnd,bhkd->bhnk', query, key) / (tf.sqrt( tf.cast(self.dim_head, dtype=tf.float32) ))\n",
        "        scores += (mask * -1e9) if mask is not None else 0.\n",
        "        attn = tf.nn.softmax(scores, axis=-1) # (b,h,n,k_proj)\n",
        "        attn = self.dropout(attn)\n",
        "        out = tf.einsum('bhnk,bhkd->bhnd', attn, val)\n",
        "\n",
        "        # Reshape and pass through out layer\n",
        "        out = tf.transpose(out, [0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (b, n, -1))\n",
        "        return self.to_out(out)\n",
        "\n",
        "    def attention(self, mask): # O(n^2)\n",
        "        shape = tf.shape(x)\n",
        "        b = shape[0]\n",
        "        n = shape[1]\n",
        "\n",
        "        query, key, val = self.to_q(x), self.to_k(x), self.to_v(x)\n",
        "        query, key, val = [ tf.reshape(x, (b, n, self.heads, self.dim_head)) for x in [query, key, val] ]\n",
        "        query, key, val = [ tf.transpose(x, [0, 2, 1, 3]) for x in [query, key, val] ]\n",
        "\n",
        "        scores = tf.einsum('bhnd,bhnd->bhnn', query, key) / (tf.sqrt(self.dim_head))\n",
        "        scores += (mask * -1e9) if mask is not None else 0. # apply mask non-edge connections\n",
        "        attn = tf.nn.softmax(scores, axis=-1) #-1\n",
        "        attn = self.dropout(attn)\n",
        "        out = tf.einsum('bhnn,bhnm->bhnd', attn, v)\n",
        "\n",
        "        out = tf.transpose(out, [0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (b, n, -1))\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class TransformerDiffusion( Layer ):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.model_type = args.model_type\n",
        "        self.n_steps = args.n_steps\n",
        "\n",
        "        code = args.code\n",
        "        # assert isinstance(code.H, tf.sparse.SparseTensor), \"Code's pcm must be sparse.\"\n",
        "        self.pcm = tf.cast(code.H, dtype=tf.int32)\n",
        "        # shapes\n",
        "        self.m, self.n = self.pcm.shape\n",
        "        self.k = self.n - self.m\n",
        "        self.dims = args.d_model\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.mask = self.create_mask(self.pcm)\n",
        "        # trans_call layers\n",
        "        self.src_embed = tf.Variable( tf.random.uniform([self.dims, self.n + self.m, 1]), trainable=True )\n",
        "        self.decoder = Transformer(args.d_model, args.heads, self.mask, args.t_layers)\n",
        "        self.to_n = Dense(1)\n",
        "        self.to_m = Dense(1)\n",
        "        self.time_embed = Embedding(args.n_steps, args.d_model)\n",
        "        # diff layers\n",
        "        self.fc = Dense(1)\n",
        "\n",
        "        self.betas = tf.constant( tf.linspace(1e-3, 1e-2, args.n_steps)*0 + args.sigma )\n",
        "        self.betas_bar = tf.constant( tf.math.cumsum(self.betas, 0) )\n",
        "\n",
        "        self.split_diff = False#args.split_diff\n",
        "        self.ls_active = args.ls_active\n",
        "\n",
        "        scheduler = tf.keras.optimizers.schedules.CosineDecay( initial_learning_rate=args.lr, decay_steps=args.epochs ) # 1000 is size of trainloader\n",
        "        self.optimizer =  tf.keras.optimizers.Adam(learning_rate=scheduler)\n",
        "\n",
        "    def create_mask(self, H):\n",
        "        m,n = H.shape\n",
        "        mask = tf.eye(n+m, dtype=tf.float32) # (n+m, n+m)\n",
        "        indices = tf.where(H != 0)#H.indices\n",
        "        cn_con, vn_con = indices[:, 0], indices[:, 1]\n",
        "\n",
        "        for cn, vn_i in zip(cn_con, vn_con):\n",
        "            # cn to vn connections in the mask\n",
        "            mask = tf.tensor_scatter_nd_update(mask, [[n+cn, vn_i],[vn_i, n+cn]], [1.0,1.0])\n",
        "\n",
        "            # distance 2 vn neighbors of vn_i\n",
        "            related_vns = vn_con[cn_con==cn]\n",
        "            for vn_j in related_vns:\n",
        "                mask = tf.tensor_scatter_nd_update(mask, [[vn_i, vn_j],[vn_j, vn_i]], [1.0,1.0])\n",
        "\n",
        "        # -infinity where mask is not set\n",
        "        mask = tf.cast( tf.math.logical_not(mask > 0), dtype=tf.float32) # not(mask > 0) for setting non connections to -1e9\n",
        "        return mask\n",
        "\n",
        "    def get_sigma(self, t):\n",
        "        # make sure t is a positive int\n",
        "        t = tf.cast( tf.abs(t), tf.int32 )\n",
        "        # gather betas\n",
        "        betas_t = tf.gather(self.betas, t)\n",
        "        betas_bar_t = tf.gather(self.betas_bar, t)\n",
        "\n",
        "        return betas_bar_t * betas_t / (betas_bar_t + betas_t)\n",
        "\n",
        "    def get_syndrome(self, r_t):\n",
        "        # Calculate syndrome (pcm @ r = 0) if r is correct in binary\n",
        "        r_t = tf.reshape(r_t, (self.n, -1)) # (n,b)\n",
        "        r_t_bin = tf.cast(llr_to_bin(r_t), dtype=tf.int32)\n",
        "        return (self.pcm @ r_t_bin) % 2 # (m,n)@(n,b)->(m,b)\n",
        "\n",
        "    # Extracts noise estimate z_hat from r\n",
        "    def tran_call(self, r_t, t):\n",
        "        # Make sure r_t and t are compatible\n",
        "        r_t = tf.reshape(r_t, (self.n, self.batch_size)) # (n,b)\n",
        "        print(r_t.shape)\n",
        "        t = tf.cast(t, dtype=tf.int32)\n",
        "\n",
        "        # Compute synd and magn\n",
        "        syndrome = tf.reshape( self.get_syndrome(llr_to_bin(r_t)), (self.pcm.shape[0], self.batch_size) ) # (m,n)@(n,b)->(m,b) check nodes\n",
        "        magnitude = tf.reshape( tf.abs(r_t), (self.n, self.batch_size) ) #(n,b) variable nodes\n",
        "        # make sure their the same dtype\n",
        "        magnitude, syndrome = [ tf.cast(tensor, dtype=tf.float32) for tensor in [magnitude, syndrome] ]\n",
        "\n",
        "        # Concatenate synd and magn\n",
        "        nodes = tf.concat([magnitude, syndrome], axis=0) # data for vertices\n",
        "        nodes = tf.reshape(nodes, (1, self.n+self.m, self.batch_size)) # (1, n+m, b)\n",
        "        # print(nodes.shape)\n",
        "\n",
        "        print(self.src_embed.shape)\n",
        "        # Embedding nodes w/ attn and 'time' (sum syn errs) dims\n",
        "        nodes_emb = tf.reshape( self.src_embed * nodes, (self.src_embed.shape[0], self.pcm.shape[0]+self.n, self.batch_size) ) # (d,n+m,b)\n",
        "        time_emb = tf.reshape( self.time_embed(t), (self.src_embed.shape[0], 1, self.batch_size) ) # (d,1,b)\n",
        "        print(nodes_emb.shape, time_emb.shape)\n",
        "\n",
        "        # Applying embeds\n",
        "        emb_t = time_emb * nodes_emb # (d, n+m, b)\n",
        "        emb_t = tf.transpose(emb_t, (2, 1, 0)) # (d, n+m, b)-> (b, n+m, d)\n",
        "        print(emb_t.shape)\n",
        "        logits = self.decoder(emb_t) # (b, n+m, d) # TODO: missing batch dims b\n",
        "        logits = tf.transpose(logits, (2, 1, 0)) # (b, n+m, d)-> (d, n+m, b)\n",
        "        print(\"logits: \", logits.shape)\n",
        "\n",
        "        # Reduce (d,n+m,d)->(d,n+m)\n",
        "        # logits = tf.squeeze( self.fc(logits), axis=-1 )\n",
        "        vn_logits = tf.reshape( logits[:, :self.n, :], (self.n, self.batch_size, self.dims) ) # (n,b, d) take the first n logits from the concatenation\n",
        "        cn_logits = tf.reshape( logits[:, self.n:, :], (self.m, self.batch_size, self.dims) ) # (m,b, d) take the last m logits from the concatenation\n",
        "        # print(vn_logits, cn_logits)\n",
        "\n",
        "        z_hat = tf.squeeze( self.to_n(vn_logits), axis=-1 )# (n,b, d)->(n, b)\n",
        "        synd = tf.squeeze( self.to_m(cn_logits), axis=-1 )# (m,b, d)->(m, b)\n",
        "        print(z_hat.shape, synd.shape)\n",
        "\n",
        "        return z_hat, synd\n",
        "\n",
        "    # optimal lambda l for theoretical and for error prediction\n",
        "    def line_search(self, r_t, sigma, err_hat, lin_splits=20):\n",
        "        l_values =  tf.reshape( tf.linspace(1., 20., lin_splits), (1, 1, lin_splits) )\n",
        "        r_t, sigma, err_hat = [ tf.expand_dims(tensor, axis=-1) for tensor in [r_t, sigma, err_hat] ]# (n,b, 1)\n",
        "        # print(f\"sigma: {sigma}, err_hat: {err_hat}\")\n",
        "\n",
        "        # Compute theoretical step size w/ ls splits\n",
        "        z_hat_values = l_values*(sigma*err_hat) # (n,b, l), l is lin_splits\n",
        "        r_values = llr_to_bin(r_t - z_hat_values) # (n,b, l)\n",
        "        r_values = tf.reshape(r_values, [r_values.shape[0], -1]) # (n,b*l)\n",
        "\n",
        "        # sum of synds (m,n)@(n,b*l)->(m,b*l)->(b*l, 1)\n",
        "        sum_synds = tf.reduce_sum( tf.abs( (self.pcm @ r_values) % 2 ),\n",
        "                                   axis=0 )\n",
        "        sum_synds = tf.reshape(sum_synds, (-1, lin_splits)) # (b, l)\n",
        "        # print(sum_synds.shape)\n",
        "\n",
        "        # Pick optimal ls value\n",
        "        if self.model_type=='dis':\n",
        "             ixs = tf.math.argmin(sum_synds, axis=1, output_type=tf.int32)[:, tf.newaxis] # (b,1) w/ ixs of optimal line search for batch b\n",
        "        elif self.model_type=='gen':\n",
        "             ixs = tf.math.argmax(sum_synds, axis=1, output_type=tf.int32)[:, tf.newaxis] # (b,1)\n",
        "\n",
        "        # print(r_values.shape, z_hat_values.shape, ixs.shape)\n",
        "        # (b, l, n) for indexing on l\n",
        "        r_values, z_hat_values = [ tf.reshape(tensor, [-1, lin_splits, r_values.shape[0]])\n",
        "                                            for tensor in [r_values, z_hat_values] ]\n",
        "\n",
        "        # concat range of batch ixs [0,...,n-1] and optimal line search ixs for gather_nd\n",
        "        indices = tf.concat([ tf.range(ixs.shape[0])[:, tf.newaxis], ixs ],\n",
        "                                                            axis=-1) # (b,2)\n",
        "\n",
        "        # print(r_values, z_hat_values, indices)\n",
        "        # ix on lin_splits w/ gather_nd st. ix,(b, l, n)->(n,b)\n",
        "        r_t1, z_hat = [ tf.reshape( tf.gather_nd(tensor, indices), (self.n, -1) )\n",
        "                                             for tensor in [r_values, z_hat_values] ]\n",
        "        # print(r_t1, z_hat_values)\n",
        "        return r_t1, z_hat # r at t-1\n",
        "\n",
        "    def loss_fn(self, synd):\n",
        "        return tf.reduce_mean(tf.square(synd))\n",
        "\n",
        "    def train_step(self, llr_ch):\n",
        "        with tf.GradientTape() as tape:\n",
        "            _, synd = self.tran_call(llr_ch,\n",
        "                                     tf.reduce_sum( self.get_syndrome(llr_ch) ))\n",
        "            loss = self.loss_fn(synd)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    # def train(self, r_t, struct_noise=0, sim_ampl=True):\n",
        "    #     # t = tf.random.uniform( (c_0.shape[0] // 2 + 1,), minval=0,maxval=self.n_steps, dtype=tf.int32 )\n",
        "    #     # t = tf.concat([t, self.n_steps - t - 1], axis=0)[:c_0.shape[0]] # reshapes t to size x_0\n",
        "    #     # t = tf.cast(t, dtype=tf.int32)\n",
        "\n",
        "    #     # noise_factor = tf.math.sqrt( tf.gather(self.betas_bar, t) )\n",
        "    #     # noise_factor = tf.reshape(noise_factor, (-1, 1))\n",
        "    #     # z = tf.random.normal(c_0.shape)\n",
        "    #     # h = np.random.rayleigh(size=c_0.shape)if sim_ampl else 1.\n",
        "\n",
        "    #     # added noise to codeword\n",
        "    #     # c_t = tf.transpose(h * c_0 + struct_noise + (z*noise_factor))\n",
        "    #     # calculate sum of syndrome\n",
        "    #     t = tf.math.reduce_sum( self.get_syndrome( llr_to_bin(tf.sign(c_t)) ), axis=0 ) # (batch_size, 1)\n",
        "\n",
        "    #     z_hat = self.tran_call(c_t, t) # model prediction\n",
        "\n",
        "    #     if self.model_type=='dis':\n",
        "    #         z_mul = c_t * tf.transpose(c_0) # actual noise added through the channel\n",
        "\n",
        "    #     elif self.model_type=='gen':\n",
        "    #         c_t += z_hat # could contain positive or negative values\n",
        "    #         z_mul = c_t * tf.transpose(c_0) # moidfied channel noise st. it will fool the discriminator\n",
        "\n",
        "    #     z_mul = tf.reshape(z_mul, (z_hat.shape[0], -1))\n",
        "    #     return c_hat, synd #z_hat, llr_to_bin(z_mul), c_t\n",
        "\n",
        "# Construct discriminator (decoder using reverse diffusion)\n",
        "    # Will have to come up with ways to try to decode the noised codeword against specific noise\n",
        "    # that will be trying to fool it.\n",
        "\n",
        "    # For optimization:\n",
        "        # use Linformer having a O(n) on top of already improved complexity using the pcm mask\n",
        "        # use split diffusion to improve accuracy and efficiency by guiding model rather than EMA\n",
        "\n",
        "class Decoder( TransformerDiffusion ):\n",
        "    def __init__(self, args):\n",
        "        super().__init__(args)\n",
        "\n",
        "    # 'test' function\n",
        "    def call(self, r_t):\n",
        "        i = tf.constant(0)  # Initialize loop counter\n",
        "        z_hat = tf.zeros_like(r_t)  # Placeholder for z_hat\n",
        "\n",
        "        def condition(i, r_t, z_hat):\n",
        "            # Loop while i < self.m and syndrome sum is not zero\n",
        "            return tf.logical_and(i < self.m, tf.reduce_sum(self.get_syndrome(r_t)) != 0)\n",
        "\n",
        "        def body(i, r_t, z_hat):\n",
        "            # Perform reverse or split diffusion\n",
        "            r_t, z_hat = tf.cond(\n",
        "                tf.logical_not(self.split_diff),\n",
        "                lambda: self.rev_diff_call(r_t),\n",
        "                lambda: self.split_rdiff_call(r_t),\n",
        "            )\n",
        "            return tf.add(i, 1), r_t, z_hat\n",
        "\n",
        "        # Run tf.while_loop with the loop variables\n",
        "        i, final_r_t, final_z_hat = tf.while_loop(\n",
        "            condition,\n",
        "            body,\n",
        "            loop_vars=[i, r_t, z_hat],\n",
        "            maximum_iterations=self.n_steps,\n",
        "            # shape_invariants=[i.get_shape(), tf.TensorShape([None, None]), z_hat.get_shape()]\n",
        "        )\n",
        "\n",
        "        return final_r_t, final_z_hat, i\n",
        "\n",
        "\n",
        "    # Refines recieved codeword r at time t\n",
        "    def rev_diff_call(self, r_t):\n",
        "        print(\"Rev def call with line-search...\")\n",
        "        # Make sure r_t and t are compatible\n",
        "        r_t = tf.reshape(r_t, (self.n, -1)) # (n,b)\n",
        "        # 'time step' of diffusion is really ix of abs(sum synd errors)\n",
        "        t = tf.reduce_sum( self.get_syndrome(llr_to_bin(r_t)), axis=0 ) # (m,n)@(n,b)->(m,b)->(1,b)\n",
        "        t = tf.cast(tf.abs(t), dtype=tf.int32)\n",
        "\n",
        "        # Transformer error prediction\n",
        "        z_hat_crude, synd = self.tran_call(r_t, t) # (n,1), (m,1)\n",
        "        # print(\"z_hat_crude: \", z_hat_crude)\n",
        "\n",
        "        # Compute diffusion vars\n",
        "        sigma = self.get_sigma(t) # theoretical step size\n",
        "        # print(\"sigma: \", sigma)\n",
        "        err_hat = r_t - tf.sign(z_hat_crude * r_t) # (n,1)\n",
        "\n",
        "        # Refined estimate of the codeword for the ls diffusion step\n",
        "        r_t1, z_hat = self.line_search(r_t, sigma, err_hat) if self.ls_active else 1.\n",
        "\n",
        "        # Cast both outputs to float32 for consistency\n",
        "        r_t1, z_hat = [ tf.cast(tensor, tf.float32) for tensor in [r_t1, z_hat] ]\n",
        "        # # reshape to (n,b) for consistency\n",
        "        r_t1, z_hat = [ tf.reshape( tensor, (self.n, self.batch_size) )\n",
        "                                             for tensor in [r_t1, z_hat] ]\n",
        "\n",
        "        return r_t1, z_hat # r at t-1, both (n,1)\n",
        "\n",
        "    def split_rdiff_call(self, r_t):\n",
        "        print(\"Rev diff call with split diffusion...\")\n",
        "        # Ensure r_t is correctly shaped\n",
        "        r_t = tf.reshape(r_t, (self.n, -1))  # (n,b)\n",
        "        print(r_t.shape)\n",
        "        t = tf.reduce_sum(self.get_syndrome(llr_to_bin(r_t)), axis=0)  # (m,n)@(n,b)->(m,b)->(1,b)\n",
        "        t = tf.cast(tf.abs(t), dtype=tf.int32)\n",
        "\n",
        "        # First half-step condition subproblem\n",
        "        print(r_t.shape, t)\n",
        "        z_hat_crude, synd = self.tran_call(r_t, t)\n",
        "        print(\"fc input: \", (z_hat_crude * self.get_sigma(t)).shape)\n",
        "        r_t_half = r_t - 0.5 * self.fc(z_hat_crude * self.get_sigma(t))\n",
        "        print(r_t_half.shape)\n",
        "\n",
        "        # Full-step diffusion subproblem\n",
        "        r_t1 = r_t_half + tf.random.normal(r_t_half.shape) * tf.sqrt(self.get_sigma(t))\n",
        "\n",
        "        # Second half-step condition subproblem\n",
        "        z_hat_crude_half, synd = self.tran_call(r_t1, t)  # Reuse the second `tran_call`\n",
        "        r_t1 = r_t1 - 0.5 * self.fc(z_hat_crude_half * self.get_sigma(t))\n",
        "\n",
        "        # Cast both outputs to float32 for consistency\n",
        "        r_t1, z_hat_crude_half = [ tf.cast(tensor, tf.float32) for tensor in [r_t1, z_hat_crude_half] ]\n",
        "        # # reshape to (n,b) for consistency\n",
        "        r_t1, z_hat_crude_half = [ tf.reshape( tensor, (self.n, self.batch_size) )\n",
        "                                             for tensor in [r_t1, z_hat_crude_half] ]\n",
        "        print(r_t1.shape, z_hat_crude_half.shape)\n",
        "        return r_t1, z_hat_crude_half  # r at t-1, both (n,1)\n",
        "\n",
        "\n",
        "class E2EModel(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, k, n, return_infobits=False, es_no=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self._n = n\n",
        "        self._k = k\n",
        "\n",
        "        self._binary_source = BinarySource()\n",
        "        self._num_bits_per_symbol = 2\n",
        "        self._mapper = Mapper(\"qam\", self._num_bits_per_symbol)\n",
        "        self._demapper = Demapper(\"app\", \"qam\", self._num_bits_per_symbol)\n",
        "        self._channel = AWGN()\n",
        "        self._decoder = decoder\n",
        "        self._encoder = encoder\n",
        "        self._return_infobits = return_infobits\n",
        "        self._es_no = es_no\n",
        "\n",
        "    @tf.function(jit_compile=False)\n",
        "    def call(self, batch_size, ebno_db):\n",
        "\n",
        "        # no rate-adjustment for uncoded transmission or es_no scenario\n",
        "        if self._decoder is not None and self._es_no==False:\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, self._k/self._n)\n",
        "        else: #for uncoded transmissions the rate is 1\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, 1)\n",
        "\n",
        "        b = self._binary_source([batch_size, self._k])\n",
        "        if self._encoder is not None:\n",
        "            c = self._encoder(b)\n",
        "        else:\n",
        "            c = b\n",
        "\n",
        "        # check that rate calculations are correct\n",
        "        assert self._n==c.shape[-1], \"Invalid value of n.\"\n",
        "\n",
        "        # zero padding to support odd codeword lengths\n",
        "        if self._n%2==1:\n",
        "            c_pad = tf.concat([c, tf.zeros([batch_size, 1])], axis=1)\n",
        "        else: # no padding\n",
        "            c_pad = c\n",
        "        x = self._mapper(c_pad)\n",
        "\n",
        "        y = self._channel([x, no])\n",
        "        llr = self._demapper([y, no])\n",
        "\n",
        "        # remove zero padded bit at the end\n",
        "        if self._n%2==1:\n",
        "            llr = llr[:,:-1]\n",
        "\n",
        "        # and run the decoder\n",
        "        if self._decoder is not None:\n",
        "            llr = tf.transpose(llr, (1,0)) # (b,n)->(n,b)\n",
        "            tf.print('llr: ', llr)\n",
        "            llr_hat, z_hat, _ = self._decoder(llr)\n",
        "            llr_hat = tf.transpose(llr_hat, (1,0)) # (n,b)->(b,n)\n",
        "            tf.print(\"llr_hat: \", llr_hat)\n",
        "            tf.print(\"z_hat: \", z_hat)\n",
        "\n",
        "            # z_hat, z_mul, c_t = model.train(x)\n",
        "            # loss = loss_fn(z_hat, z_mul)\n",
        "\n",
        "        if self._return_infobits:\n",
        "            return b, llr_hat\n",
        "        else:\n",
        "            return c, llr_hat\n",
        "\n",
        "\n",
        "# args for decoder/discriminator\n",
        "args = Args(model_type='dis')\n",
        "args.code.H = pcm\n",
        "args.n, args.m = pcm.shape\n",
        "args.k = k\n",
        "args.n_steps = args.m + 5\n",
        "\n",
        "ltd_decoder = Decoder(args) # Linear Transformer Diffusion (LTD) Decoder\n",
        "\n",
        "e2e_ltd = E2EModel(encoder, ltd_decoder, k, n)"
      ],
      "metadata": {
        "id": "XOILyjSGXMdb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dec(model, args):\n",
        "    # loss\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "    # optimizer\n",
        "    scheduler = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=args.lr, decay_steps=args.epochs) # 1000 is size of trainloader\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler)\n",
        "    # time start\n",
        "    time_start = time.time()\n",
        "\n",
        "    # SGD update iteration\n",
        "    @tf.function(jit_compile=False)\n",
        "    def train_step(batch_size):\n",
        "        # train for random SNRs within a pre-defined interval\n",
        "        ebno_db = tf.random.uniform([batch_size, 1],\n",
        "                                    minval=args.ebno_db_min,\n",
        "                                    maxval=args.ebno_db_max)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            c, llr_hat = model(batch_size, ebno_db)\n",
        "            print(c, llr_hat)\n",
        "\n",
        "            loss_value = loss_fn(c, llr_hat)\n",
        "\n",
        "        # and apply the SGD updates\n",
        "        weights = model.trainable_weights\n",
        "        grads = tape.gradient(loss_value, weights) # variables\n",
        "        optimizer.apply_gradients(zip(grads, weights))\n",
        "        return c, llr_hat\n",
        "\n",
        "    print(\"Training Linear Transformer Diffusion Model...\")\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_step(args.batch_size)\n",
        "\n",
        "        # eval train iter\n",
        "        if epoch % args.eval_train_iter == 0:\n",
        "            ebno_db = tf.random.uniform([args.batch_size, 1],\n",
        "                                          minval=args.ebno_db_eval,\n",
        "                                          maxval=args.ebno_db_eval)\n",
        "\n",
        "            c, llr_hat = model(args.batch_size, ebno_db)\n",
        "\n",
        "            print(c, llr_hat)\n",
        "            loss_value = loss_fn(c, llr_hat)\n",
        "            # for _, l in llr_hat:\n",
        "            #     loss_value += loss(c, l)\n",
        "\n",
        "            c_hat = tf.cast(tf.greater(llr_hat[-1], 0), tf.float32)\n",
        "            ber = compute_ber(c, c_hat).numpy()\n",
        "\n",
        "            # measure required time since last evaluation\n",
        "            duration = time.time() - time_start # in s\n",
        "            time_start = time.time() # reset counter\n",
        "\n",
        "            print(f'Training epoch {epoch}/{args.epochs}, LR={optimizer.learning_rate.numpy():.2e}, Loss={loss_value.numpy():.5e}, BER={ber}, duration: {duration:.2f}s')\n",
        "\n",
        "        # save weights iter\n",
        "        if epoch % args.save_weights_iter == 0:\n",
        "            pass\n",
        "\n",
        "        # heat-map visualization of the model's weights\n",
        "        # for var in self.trainable_variables:\n",
        "        #     var_name = var.name\n",
        "        #     var_value = var.numpy()\n",
        "\n",
        "        #     # Check if the variable is at least 2D (suitable for heatmap)\n",
        "        #     if len(var_value.shape) > 1:\n",
        "        #         plt.figure(figsize=(8, 6))\n",
        "        #         sns.heatmap(var_value, cmap='viridis')\n",
        "        #         plt.title(f'Heatmap of {var_name}')\n",
        "        #         plt.show()\n",
        "        #     else:\n",
        "        #         print(f\"{var_name} has shape {var_value.shape} which is not suitable for a heatmap.\")\n",
        "\n",
        "\n",
        "train_dec(e2e_ltd, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NhZncoEEpgNv",
        "outputId": "a7ce32ba-de92-4f71-f16f-8484ba7691ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Linear Transformer Diffusion Model...\n",
            "Rev def call with line-search...\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "Rev diff call with split diffusion...\n",
            "(100, 160)\n",
            "(100, 160) Tensor(\"decoder_1/while/cond/Abs:0\", shape=(160,), dtype=int32)\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "fc input:  (100, 160)\n",
            "(100, 160)\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "(100, 160) (100, 160)\n",
            "Rev def call with line-search...\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "Rev diff call with split diffusion...\n",
            "(100, 160)\n",
            "(100, 160) Tensor(\"decoder_1/while/cond/Abs:0\", shape=(160,), dtype=int32)\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "fc input:  (100, 160)\n",
            "(100, 160)\n",
            "(100, 160)\n",
            "(128, 150, 1)\n",
            "(128, 150, 160) (128, 1, 160)\n",
            "(160, 150, 128)\n",
            "logits:  (128, 150, 160)\n",
            "(100, 160) (50, 160)\n",
            "(100, 160) (100, 160)\n",
            "Tensor(\"e2e_model_1/StatefulPartitionedCall:0\", shape=(160, 100), dtype=float32) Tensor(\"e2e_model_1/StatefulPartitionedCall:1\", shape=(160, 100), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"e2e_model_1/StatefulPartitionedCall:0\", shape=(160, 100), dtype=float32) Tensor(\"e2e_model_1/StatefulPartitionedCall:1\", shape=(160, 100), dtype=float32)\n",
            "llr:  [[5.97313643 0.221720278 -5.08555317 ... 0.840385735 -3.82113647 0.0535812378]\n",
            " [-2.78018785 2.84408808 -3.79367638 ... 4.307796 -1.76783216 -4.18988562]\n",
            " [-4.35836554 1.27790809 -6.0343051 ... 6.57518053 -2.20654726 -2.95650196]\n",
            " ...\n",
            " [1.46107793 0.00794136524 0.68728441 ... 3.62587428 -3.68916416 -8.22439671]\n",
            " [-2.97065878 4.38622713 -4.21316051 ... 1.50171232 1.03032255 -4.4432416]\n",
            " [-4.07833481 -2.70766401 6.73509264 ... -5.87926102 -0.524069071 2.12103415]]\n",
            "llr_hat:  [[0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [0 1 0 ... 0 1 0]\n",
            " [0 1 0 ... 0 1 0]\n",
            " [0 1 0 ... 0 1 0]]\n",
            "z_hat:  [[0.348656863 0.697313726 1.04597056 ... -2.80002737 -2.95558453 -3.11114168]\n",
            " [-0.0389129184 -0.0778258368 -0.116738752 ... 6.32065582 6.671803 7.02295065]\n",
            " [0.0501374751 0.10027495 0.150412425 ... 3.89518213 4.11158133 4.32798]\n",
            " ...\n",
            " [0.201348364 0.402696729 0.604045093 ... 0.130880445 0.138151571 0.145422712]\n",
            " [0.15055415 0.301108301 0.451662451 ... 2.79444313 2.94968987 3.10493684]\n",
            " [-0.129089773 -0.258179545 -0.387269318 ... 1.24737048 1.31666875 1.38596714]]\n",
            "llr:  [[-3.18770456 3.97989845 -5.96670914 ... -1.26985157 -3.81608772 -5.31811428]\n",
            " [5.70886564 -0.367428422 -7.73781252 ... 4.03998566 4.7811656 -0.919735551]\n",
            " [-1.1024369 6.30227041 8.68015289 ... -1.8243432 3.47429729 -3.25584793]\n",
            " ...\n",
            " [-3.98325276 -1.10946822 -4.30134058 ... 0.8643381 -2.08992839 3.33947206]\n",
            " [2.9221921 2.99009705 8.49062347 ... 0.285424531 -3.61418295 -0.887531519]\n",
            " [2.04920244 5.81309462 -6.51255 ... -3.35617328 3.66155863 -1.25799894]]\n",
            "llr_hat:  [[0 1 0 ... 0 0 1]\n",
            " [0 1 0 ... 0 0 1]\n",
            " [0 1 0 ... 0 0 1]\n",
            " ...\n",
            " [1 0 0 ... 1 1 1]\n",
            " [1 0 0 ... 1 1 1]\n",
            " [1 0 0 ... 1 1 1]]\n",
            "z_hat:  [[0.282753617 0.565507233 0.84826088 ... 4.57028818 4.82419348 5.0780983]\n",
            " [-0.20258078 -0.40516156 -0.60774231 ... -5.00851822 -5.28676939 -5.56502056]\n",
            " [0.556212246 1.11242449 1.6686368 ... -8.58544 -9.06240845 -9.53937721]\n",
            " ...\n",
            " [0.335731775 0.671463549 1.00719535 ... 2.29742 2.42505455 2.55268908]\n",
            " [0.197673216 0.395346433 0.593019664 ... -7.0497694 -7.44142342 -7.83307743]\n",
            " [-0.0930372626 -0.186074525 -0.279111803 ... -4.84866333 -5.11803389 -5.38740396]]\n",
            "llr:  [[2.89370847 3.1463275 2.56275821 ... 2.56689405 -4.43718624 -2.81394482]\n",
            " [-4.75655651 2.49270201 -5.50148058 ... 6.13738966 6.25606251 4.64967775]\n",
            " [-5.2028842 8.75766182 10.6234894 ... 5.93090534 2.09264231 5.42651224]\n",
            " ...\n",
            " [-3.95679092 -3.52402902 -4.86038828 ... 3.29566383 -4.20095873 5.83222771]\n",
            " [-4.31107903 -5.35700846 -3.99440026 ... -1.97907829 7.62009048 -3.19425702]\n",
            " [7.63771343 5.91093445 0.294473 ... -1.09825015 -0.958426714 6.24849892]]\n",
            "llr_hat:  [[0 1 0 ... 0 1 0]\n",
            " [0 1 0 ... 0 1 0]\n",
            " [0 1 0 ... 0 1 0]\n",
            " ...\n",
            " [1 0 0 ... 0 1 0]\n",
            " [1 0 0 ... 0 1 0]\n",
            " [1 0 0 ... 0 1 0]]\n",
            "z_hat:  [[0.194685444 0.389370888 0.584056318 ... 3.21020484 3.38854957 3.56689429]\n",
            " [-0.271859348 -0.543718696 -0.815578043 ... -5.58459187 -5.89484692 -6.20510197]\n",
            " [0.161708042 0.323416084 0.485124111 ... 0.914868951 0.965695 1.0165211]\n",
            " ...\n",
            " [-0.030509254 -0.0610185079 -0.09152776 ... -7.36669922 -7.77596045 -8.18522167]\n",
            " [-0.0432080477 -0.0864160955 -0.129624143 ... -2.40615344 -2.53982854 -2.67350388]\n",
            " [0.176637515 0.353275031 0.529912531 ... 0.562652767 0.593911231 0.625169754]]\n",
            "llr:  [[4.26625967 2.34896708 -1.00825787 ... 3.09803414 -0.527187228 -0.551138401]\n",
            " [3.3399539 5.34638786 -5.74457455 ... -4.16865635 -1.21452892 0.490250528]\n",
            " [-1.47392249 1.01562357 2.02184534 ... 4.86412716 4.82669163 -1.53605175]\n",
            " ...\n",
            " [-5.63064289 -1.68744886 -3.88302135 ... 4.89197254 -1.98038125 3.52442813]\n",
            " [0.458781719 1.2974937 3.47835898 ... 4.57686663 1.6943574 -3.93181109]\n",
            " [0.0152587295 4.81116 -2.56049895 ... 4.1967206 -0.369212151 -0.283141196]]\n",
            "llr_hat:  [[1 1 0 ... 0 1 1]\n",
            " [1 1 0 ... 0 1 1]\n",
            " [1 1 0 ... 0 1 1]\n",
            " ...\n",
            " [1 1 0 ... 1 0 1]\n",
            " [1 1 0 ... 1 0 1]\n",
            " [1 1 0 ... 1 0 1]]\n",
            "z_hat:  [[-0.37909025 -0.758180499 -1.13727069 ... -1.91283059 -2.019099 -2.1253674]\n",
            " [-0.145309076 -0.290618151 -0.435927212 ... 0.428406477 0.45220685 0.476007193]\n",
            " [0.313355416 0.626710832 0.940066218 ... -2.60521722 -2.7499516 -2.89468575]\n",
            " ...\n",
            " [0.244932264 0.489864528 0.734796762 ... -5.14768791 -5.43367052 -5.71965313]\n",
            " [-0.24336955 -0.486739099 -0.730108619 ... -1.00719213 -1.06314719 -1.11910236]\n",
            " [-0.335235447 -0.670470893 -1.00570631 ... -5.5674305 -5.87673235 -6.1860342]]\n",
            "llr:  [[-2.80368662 -1.8422 3.70435953 ... 4.18691969 1.79195213 3.14584064]\n",
            " [-5.3676033 2.5639472 1.39743459 ... -0.581648529 -0.0869571 -6.40729284]\n",
            " [-2.0284524 6.50559282 -6.5556078 ... 2.38618851 4.96564722 1.89466405]\n",
            " ...\n",
            " [-6.03164482 -2.49609613 -6.65524387 ... -2.87550783 -4.0175705 2.35860729]\n",
            " [5.71260929 -2.17475748 -8.85502 ... 4.33586645 5.6769805 -3.39015055]\n",
            " [4.34506941 4.81229544 -2.84802961 ... 4.17101049 1.39695382 -1.53418589]]\n",
            "llr_hat:  [[0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [1 0 0 ... 1 0 0]\n",
            " [1 0 0 ... 1 0 0]\n",
            " [1 0 0 ... 1 0 0]]\n",
            "z_hat:  [[-0.0126432255 -0.025286451 -0.0379296765 ... 4.66822815 4.92757416 5.18692]\n",
            " [0.139597625 0.279195249 0.418792874 ... -7.61635256 -8.03948307 -8.46261406]\n",
            " [0.27114594 0.54229188 0.813437819 ... -3.6555233 -3.85860801 -4.06169271]\n",
            " ...\n",
            " [-0.0346312299 -0.0692624599 -0.10389369 ... -1.20151854 -1.26826966 -1.33502066]\n",
            " [0.0560574345 0.112114869 0.1681723 ... 3.43106627 3.62168097 3.81229591]\n",
            " [-0.192401499 -0.384803 -0.577204466 ... -3.25963235 -3.44072294 -3.62181377]]\n",
            "llr:  [[1.03626561 4.91505909 3.32107687 ... -3.26981258 7.33654785 4.15365744]\n",
            " [3.49629855 -5.47929859 -4.28329849 ... 1.64023304 -2.47631764 0.728078902]\n",
            " [-1.60273433 -7.28349304 -0.904736638 ... 5.22303915 -3.31022835 -3.3963244]\n",
            " ...\n",
            " [-4.96643353 -3.62522531 5.32635689 ... 1.24598598 1.93563175 -3.07757378]\n",
            " [0.861654103 5.89444637 4.76766491 ... -3.29398 -5.33412027 -6.32846117]\n",
            " [2.68241119 5.79811049 -4.78550529 ... 1.57325804 5.83412743 7.97209167]]\n",
            "llr_hat:  [[1 0 1 ... 0 0 1]\n",
            " [1 0 1 ... 0 0 1]\n",
            " [1 0 1 ... 0 0 1]\n",
            " ...\n",
            " [1 1 0 ... 0 0 1]\n",
            " [1 1 0 ... 0 0 1]\n",
            " [1 1 0 ... 0 0 1]]\n",
            "z_hat:  [[-0.0141875995 -0.0283751991 -0.0425627977 ... -2.04283142 -2.15632224 -2.26981282]\n",
            " [0.41682744 0.833654881 1.25048232 ... 5.59341288 5.90415764 6.21490288]\n",
            " [-0.0588297285 -0.117659457 -0.176489189 ... -1.09706926 -1.15801764 -1.21896589]\n",
            " ...\n",
            " [0.0414827503 0.0829655 0.124448255 ... -5.08724117 -5.36986589 -5.65249]\n",
            " [0.115201 0.230402 0.345603 ... 4.3183 4.5582056 4.79811096]\n",
            " [-0.289275289 -0.578550577 -0.867825866 ... -1.63753557 -1.72850978 -1.819484]]\n",
            "llr:  [[-3.58922386 4.07859612 7.10082531 ... -0.275857449 5.60710526 6.40411043]\n",
            " [-7.89099789 6.64738 5.00542355 ... -4.22572 -2.61194229 6.34071064]\n",
            " [1.75369573 3.71608281 7.34125233 ... 5.16810322 0.447787344 -8.25768948]\n",
            " ...\n",
            " [3.39029527 4.47787666 3.55023623 ... 5.67629719 -0.0115278959 3.65011859]\n",
            " [-1.66144419 4.50513124 -6.08873081 ... -3.1005 -5.2578826 -4.77631187]\n",
            " [3.81500554 4.41982031 0.724769831 ... 9.15967083 -0.384637594 6.71846771]]\n",
            "llr_hat:  [[1 0 0 ... 0 1 0]\n",
            " [1 0 0 ... 0 1 0]\n",
            " [1 0 0 ... 0 1 0]\n",
            " ...\n",
            " [0 0 1 ... 1 0 0]\n",
            " [0 0 1 ... 1 0 0]\n",
            " [0 0 1 ... 1 0 0]]\n",
            "z_hat:  [[-0.194706559 -0.389413118 -0.584119678 ... -6.38355589 -6.73819828 -7.09284]\n",
            " [0.0816884562 0.163376912 0.245065361 ... -6.00807762 -6.34185934 -6.67564154]\n",
            " [0.547159195 1.09431839 1.64147758 ... 7.19320583 7.59282875 7.99245119]\n",
            " ...\n",
            " [-0.0497127399 -0.0994254798 -0.149138212 ... -0.104568288 -0.11037764 -0.116186991]\n",
            " [-0.0690851659 -0.138170332 -0.207255498 ... -1.42982733 -1.50926208 -1.58869696]\n",
            " [0.0637849867 0.127569973 0.19135496 ... 0.194954544 0.205785364 0.216616169]]\n",
            "llr:  [[2.51108479 1.98881662 6.68058586 ... 2.11862946 -13.578187 7.05245066]\n",
            " [-3.54304099 0.478748322 -7.26355839 ... 4.98339653 -4.88076925 -1.65947437]\n",
            " [-6.89149904 -0.7101987 1.7559545 ... 2.0716188 0.830826283 3.76395774]\n",
            " ...\n",
            " [-4.17797136 -3.8013289 -2.49824047 ... 2.05976677 -1.10816276 -1.04153609]\n",
            " [-2.46526599 -1.45146191 -8.66302204 ... 2.65849233 4.03208 -2.92167664]\n",
            " [2.47911501 -2.4358952 7.94599056 ... 0.200610399 -1.80469441 5.49000597]]\n",
            "llr_hat:  [[0 1 1 ... 0 1 0]\n",
            " [0 1 1 ... 0 1 0]\n",
            " [0 1 1 ... 0 1 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 1]\n",
            " [0 1 0 ... 0 0 1]\n",
            " [0 1 0 ... 0 0 1]]\n",
            "z_hat:  [[0.0755542442 0.151108488 0.226662725 ... -5.17048502 -5.45773458 -5.74498367]\n",
            " [-0.0912721455 -0.182544291 -0.273816437 ... 3.56350493 3.76147723 3.95944977]\n",
            " [-0.162339985 -0.324679971 -0.487019956 ... -2.74917388 -2.90190578 -3.05463767]\n",
            " ...\n",
            " [-0.00761943543 -0.0152388709 -0.0228583068 ... 0.214647129 0.226571977 0.23849681]\n",
            " [-0.116207205 -0.23241441 -0.348621607 ... -1.11830056 -1.18042839 -1.24255621]\n",
            " [0.360672086 0.721344173 1.08201623 ... 5.39893866 5.69887972 5.99882078]]\n",
            "llr:  [[3.16480684 -2.93917918 3.54765487 ... 2.69850445 -4.02903223 -5.97953653]\n",
            " [7.03737593 4.52154493 0.629599452 ... -5.06227 1.39754057 5.87039137]\n",
            " [5.54590797 1.35476875 -2.05451727 ... 0.0475231409 1.17281365 3.8441658]\n",
            " ...\n",
            " [6.11416912 4.1480608 3.4840405 ... 5.45091152 2.16711402 -1.9958005]\n",
            " [-1.16927814 -1.67361975 -4.54918194 ... -4.64152336 -2.3152163 -1.30808461]\n",
            " [-4.83662605 5.68786287 -4.52177048 ... 0.0485028625 1.33752227 7.52778292]]\n",
            "llr_hat:  [[1 0 0 ... 1 0 0]\n",
            " [1 0 0 ... 1 0 0]\n",
            " [1 0 0 ... 1 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 1 0]\n",
            " [0 1 0 ... 0 1 0]\n",
            " [0 1 0 ... 0 1 0]]\n",
            "z_hat:  [[-0.0680355132 -0.136071026 -0.204106539 ... -1.99183559 -2.10249329 -2.21315074]\n",
            " [0.325880796 0.651761591 0.977642417 ... 6.35529518 6.70836735 7.06143904]\n",
            " [0.250668347 0.501336694 0.752005041 ... -0.00467326678 -0.00493289251 -0.0051925187]\n",
            " ...\n",
            " [-0.0719454 -0.143890798 -0.215836197 ... 6.04491472 6.3807435 6.71657181]\n",
            " [0.251518041 0.503036082 0.754554152 ... 1.78387475 1.88297892 1.98208308]\n",
            " [0.0962036923 0.192407385 0.288611084 ... 1.42760992 1.50692153 1.58623326]]\n",
            "llr:  [[1.666731 -4.9050622 -1.78185654 ... -1.82344413 -2.42497921 -2.86583686]\n",
            " [-10.1637096 -2.76416731 -1.30369854 ... 2.56281042 9.69957 2.18104649]\n",
            " [2.44009209 1.87442958 -5.7337923 ... 0.950543642 3.47954822 -0.787541747]\n",
            " ...\n",
            " [-5.42227507 -0.705563247 -4.07847261 ... -3.3900435 1.21116829 6.13704777]\n",
            " [-8.7150631 -4.10332966 -5.30066538 ... -4.63042 2.0587523 0.218072653]\n",
            " [3.73418903 3.26680303 -9.44890785 ... -5.02603817 -6.40335035 7.60155]]\n",
            "llr_hat:  [[0 1 1 ... 0 0 0]\n",
            " [0 1 1 ... 0 0 0]\n",
            " [0 1 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 1 ... 1 1 0]\n",
            " [0 0 1 ... 1 1 0]\n",
            " [0 0 1 ... 1 1 0]]\n",
            "z_hat:  [[0.133336559 0.266673118 0.400009692 ... -2.54109979 -2.6822722 -2.82344437]\n",
            " [-0.171248972 -0.342497945 -0.513746917 ... -6.14310646 -6.48439 -6.82567358]\n",
            " [-0.178778082 -0.357556164 -0.536334276 ... 3.5292635 3.72533369 3.92140388]\n",
            " ...\n",
            " [0.358475924 0.716951847 1.07542777 ... -1.88182962 -1.98637581 -2.09092188]\n",
            " [0.142467752 0.284935504 0.427403271 ... 4.24451351 4.4803195 4.71612597]\n",
            " [0.248135403 0.496270806 0.744406223 ... 0.612573862 0.64660579 0.680637658]]\n",
            "llr:  [[-5.85225391 -7.65872669 4.18721628 ... -8.36684608 -6.9370079 1.06542873]\n",
            " [-6.37426138 7.74411058 5.81073093 ... 1.24497581 7.97179651 4.36925173]\n",
            " [-3.8002069 7.34008026 -0.294107199 ... -1.41429114 3.84468126 1.80447042]\n",
            " ...\n",
            " [-1.04406929 4.63087082 5.91478968 ... 10.0090446 0.974849522 3.80575871]\n",
            " [6.9165349 -1.68091512 -0.803144276 ... 4.36025906 -4.54860687 -4.3909936]\n",
            " [-1.62864399 -7.79828167 3.48227835 ... 5.93810797 1.52590728 -6.88550615]]\n",
            "llr_hat:  [[1 1 1 ... 0 0 0]\n",
            " [1 1 1 ... 0 0 0]\n",
            " [1 1 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 1 ... 1 0 0]\n",
            " [0 0 1 ... 1 0 0]\n",
            " [0 0 1 ... 1 0 0]]\n",
            "z_hat:  [[-0.342612714 -0.685225427 -1.02783811 ... -8.43016243 -8.89850521 -9.36684704]\n",
            " [-0.396850437 -0.793700874 -1.19055128 ... -2.90177751 -3.06298733 -3.22419739]\n",
            " [0.0326167792 0.0652335584 0.0978503302 ... -2.83814478 -2.99581957 -3.15349436]\n",
            " ...\n",
            " [0.159922287 0.319844574 0.479766816 ... 0.208048031 0.219606251 0.231164485]\n",
            " [0.0711995065 0.142399013 0.213598505 ... 3.83260751 4.04553032 4.25845289]\n",
            " [0.240798041 0.481596082 0.722394109 ... 2.3540411 2.48482108 2.61560106]]\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 1. 1. ... 1. 0. 0.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " ...\n",
            " [0. 1. 0. ... 1. 1. 1.]\n",
            " [0. 1. 1. ... 1. 0. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]], shape=(160, 100), dtype=float32) tf.Tensor(\n",
            "[[1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 1. 0. 0.]\n",
            " [0. 0. 1. ... 1. 0. 0.]\n",
            " [0. 0. 1. ... 1. 0. 0.]], shape=(160, 100), dtype=float32)\n",
            "Training epoch 10/1000, LR=5.00e-04, Loss=7.69630e+00, BER=0.496125, duration: 32.31s\n",
            "llr:  [[-4.31398535 4.14928055 3.32925534 ... -6.2253418 7.85921907 -5.87011051]\n",
            " [6.15481186 0.90745306 -5.26467133 ... -9.24476051 -3.2754972 -3.00654507]\n",
            " [-2.2840662 2.42444849 3.7873795 ... 2.37906981 3.19762921 -5.97873926]\n",
            " ...\n",
            " [-0.864078403 -4.85963154 1.66816068 ... 2.303931 0.737638235 6.18463707]\n",
            " [6.47486448 -0.256495953 -6.81190681 ... -1.03281808 -1.4939599 -8.35856056]\n",
            " [1.19816375 -4.77039957 -0.376578927 ... -9.13157082 -6.20724678 0.377453566]]\n",
            "llr_hat:  [[1 0 0 ... 0 1 0]\n",
            " [1 0 0 ... 0 1 0]\n",
            " [1 0 0 ... 0 1 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]]\n",
            "z_hat:  [[-0.165699288 -0.331398576 -0.49709785 ... 2.7995 2.95502758 3.11055541]\n",
            " [0.0406122059 0.0812244117 0.121836618 ... -2.63478279 -2.78115964 -2.92753649]\n",
            " [0.34449476 0.68898952 1.03348422 ... -7.17256165 -7.57103729 -7.96951294]\n",
            " ...\n",
            " [0.107031472 0.214062944 0.321094424 ... 2.27468443 2.40105581 2.5274272]\n",
            " [-0.292378664 -0.584757328 -0.877136 ... -5.83482742 -6.15898466 -6.4831419]\n",
            " [0.392959446 0.785918891 1.17887831 ... 0.168967813 0.178354919 0.187742025]]\n",
            "llr:  [[-9.54385567 2.18554354 8.07263374 ... 5.19751 4.6604147 -0.385534883]\n",
            " [-3.0461421 -7.92336369 0.00419855118 ... -6.13067675 4.30844879 -6.06818867]\n",
            " [1.32853 4.52929735 -4.59779 ... 4.38644409 -4.40055799 -3.8393712]\n",
            " ...\n",
            " [-0.237749338 2.43577623 -4.18972778 ... -7.40891647 -3.59256 -6.18767357]\n",
            " [-5.29925442 -1.85845184 5.44158173 ... -6.79858875 -0.288428187 0.303640515]\n",
            " [-6.68657541 5.1803627 2.98464227 ... 1.70859575 4.90276384 -1.50545073]]\n",
            "llr_hat:  [[1 0 1 ... 0 0 0]\n",
            " [1 0 1 ... 0 0 0]\n",
            " [1 0 1 ... 0 0 0]\n",
            " ...\n",
            " [1 0 0 ... 1 0 0]\n",
            " [1 0 0 ... 1 0 0]\n",
            " [1 0 0 ... 1 0 0]]\n",
            "z_hat:  [[-0.527192831 -1.05438566 -1.58157849 ... -1.61096859 -1.70046675 -1.78996503]\n",
            " [0.104125202 0.208250403 0.312375605 ... -3.12005901 -3.29339552 -3.46673226]\n",
            " [0.00711958483 0.0142391697 0.0213587545 ... -7.96790743 -8.41056919 -8.85323]\n",
            " ...\n",
            " [0.339171439 0.678342879 1.01751435 ... 0.734724045 0.775542 0.816360056]\n",
            " [0.0155516518 0.0311033037 0.0466549546 ... -6.44187164 -6.79975319 -7.15763521]\n",
            " [0.395748705 0.791497409 1.18724608 ... 3.74884748 3.9571166 4.1653862]]\n",
            "llr:  [[6.71588421 5.74522495 -6.1752243 ... -5.08941 0.27544874 5.07084]\n",
            " [-4.28171206 -7.50209379 -4.57423639 ... -8.66076183 0.715153277 -3.76566267]\n",
            " [-4.23766279 4.51703787 -0.551091135 ... 7.50359106 -4.62983513 5.3283844]\n",
            " ...\n",
            " [7.09376907 -3.2704618 5.49531507 ... -2.62738633 -5.92185259 1.26139581]\n",
            " [5.75521135 -1.17109585 8.74532413 ... 7.57382727 -4.92620754 -5.19167614]\n",
            " [-4.83161974 2.79144573 -1.36776519 ... 4.73526669 5.04285526 7.44307804]]\n",
            "llr_hat:  [[0 0 1 ... 0 0 1]\n",
            " [0 0 1 ... 0 0 1]\n",
            " [0 0 1 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 0 1 1]\n",
            " [0 0 0 ... 0 1 1]\n",
            " [0 0 0 ... 0 1 1]]\n",
            "z_hat:  [[0.164513201 0.329026401 0.493539602 ... -3.92351174 -4.14148474 -4.35945749]\n",
            " [0.079026334 0.158052668 0.237079 ... -4.07062149 -4.29676676 -4.5229125]\n",
            " [-0.324162364 -0.648324728 -0.972487092 ... -5.99493408 -6.32798576 -6.66103792]\n",
            " ...\n",
            " [0.0646643937 0.129328787 0.193993181 ... -3.85174322 -4.06572914 -4.27971458]\n",
            " [0.127737716 0.255475432 0.383213162 ... 3.96692634 4.18731117 4.40769577]\n",
            " [-0.187705651 -0.375411302 -0.563116968 ... -1.88312972 -1.98774803 -2.09236646]]\n",
            "llr:  [[-1.11228549 2.85177112 4.73895168 ... 2.98552394 -4.7645812 -1.16557848]\n",
            " [-4.95777655 -4.96457434 4.52709389 ... -4.36663628 4.41336679 3.82893491]\n",
            " [8.86965656 3.06616378 -1.7684617 ... -5.47844887 9.66765213 -4.59039307]\n",
            " ...\n",
            " [2.65908813 -3.96338701 0.278066099 ... 4.43407059 -5.57781267 -7.44839573]\n",
            " [-10.6111298 4.73757744 -1.5646193 ... -6.84291887 -9.83581448 -4.95234]\n",
            " [-3.16893291 0.831822038 1.42383587 ... 0.421972752 -8.94553185 -4.46748543]]\n",
            "llr_hat:  [[1 1 0 ... 0 1 1]\n",
            " [1 1 0 ... 0 1 1]\n",
            " [1 1 0 ... 0 1 1]\n",
            " ...\n",
            " [1 0 1 ... 0 1 1]\n",
            " [1 0 1 ... 0 1 1]\n",
            " [1 0 1 ... 0 1 1]]\n",
            "z_hat:  [[-0.10561429 -0.211228579 -0.316842854 ... 4.87163925 5.14228582 5.4129324]\n",
            " [-0.00206996221 -0.00413992442 -0.00620988663 ... -1.5916791 -1.68010581 -1.7685324]\n",
            " [0.0722616166 0.144523233 0.21678485 ... 2.00520253 2.11660266 2.22800279]\n",
            " ...\n",
            " [0.108234853 0.216469705 0.324704558 ... 1.65886879 1.75102818 1.84318757]\n",
            " [-0.384672761 -0.769345522 -1.15401828 ... 3.86325 4.07787514 4.2925]\n",
            " [-0.37897262 -0.75794524 -1.13691783 ... -3.99080968 -4.21252155 -4.43423319]]\n",
            "llr:  [[7.81727886 -2.15705633 -4.54675102 ... -7.63166046 -5.87548876 -8.79248142]\n",
            " [-6.51690865 -3.58386135 -6.08908796 ... -1.88571513 -1.36228836 4.36695]\n",
            " [-2.25563598 -2.7597177 0.229458094 ... 3.79417896 -3.58361578 -5.28826523]\n",
            " ...\n",
            " [-4.93674469 3.28438663 1.58155501 ... -5.56363392 -2.28855324 8.33203]\n",
            " [-4.07364225 0.0608278513 5.95282888 ... 3.90864038 -5.01212406 1.03443646]\n",
            " [-3.95023322 2.10042977 -0.435827613 ... -3.11628294 -1.12805092 -0.144467354]]\n",
            "llr_hat:  [[0 1 1 ... 0 0 1]\n",
            " [0 1 1 ... 0 0 1]\n",
            " [0 1 1 ... 0 0 1]\n",
            " ...\n",
            " [1 1 1 ... 1 0 1]\n",
            " [1 1 1 ... 1 0 1]\n",
            " [1 1 1 ... 1 0 1]]\n",
            "z_hat:  [[0.440864 0.881728 1.32259202 ... 2.60113 2.74563742 2.89014459]\n",
            " [-0.172511876 -0.345023751 -0.517535627 ... -8.45494747 -8.92466736 -9.39438629]\n",
            " [-0.385809302 -0.771618605 -1.15742791 ... -3.53205299 -3.72827816 -3.92450333]\n",
            " ...\n",
            " [0.136624306 0.273248613 0.409872919 ... 0.104934789 0.110764503 0.11659421]\n",
            " [0.168462634 0.336925268 0.505387902 ... 0.990386903 1.04540837 1.10042989]\n",
            " [-0.0717913881 -0.143582776 -0.215374172 ... 1.79097629 1.89047492 1.98997366]]\n",
            "llr:  [[-1.71669316 -0.00353765488 -3.24071264 ... 6.55275106 6.70823908 2.67629]\n",
            " [-4.1149888 3.69493365 -3.13852382 ... -5.85089684 2.50337029 -5.72810125]\n",
            " [5.38192606 3.26648283 -2.72393417 ... 1.00449908 1.40415 2.46244502]\n",
            " ...\n",
            " [3.93684912 -4.11305 6.19565105 ... -7.02371693 -3.47733545 1.10269165]\n",
            " [6.66026831 1.93359542 2.79864359 ... 4.45130873 -1.30932856 0.149774492]\n",
            " [-2.27221036 3.11792922 5.47642279 ... 0.8401227 -4.37938118 -3.36827874]]\n",
            "llr_hat:  [[0 1 0 ... 1 1 1]\n",
            " [0 1 0 ... 1 1 1]\n",
            " [0 1 0 ... 1 1 1]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 1 ... 0 0 1]\n",
            " [0 0 1 ... 0 0 1]]\n",
            "z_hat:  [[0.0968167856 0.193633571 0.290450364 ... -7.68350315 -8.11036491 -8.53722572]\n",
            " [-0.123395212 -0.246790424 -0.370185643 ... 3.10370183 3.27612972 3.44855762]\n",
            " [0.226643488 0.453286976 0.679930449 ... 10.1018286 10.6630421 11.2242546]\n",
            " ...\n",
            " [0.0395754762 0.0791509524 0.118726432 ... -6.58754444 -6.95351934 -7.31949377]\n",
            " [0.00648856536 0.0129771307 0.0194656961 ... -6.57762289 -6.94304657 -7.30847]\n",
            " [-0.081250459 -0.162500918 -0.243751377 ... 2.8020134 2.9576807 3.11334825]]\n",
            "llr:  [[-2.70323777 -1.44693 -7.39520693 ... -1.3521347 3.52125049 -6.87845135]\n",
            " [-3.0263443 3.26278472 1.95486748 ... 6.01197863 -1.4824028 5.09328747]\n",
            " [2.26288319 3.06850171 9.94506836 ... 1.38710666 -5.29889917 -4.60468149]\n",
            " ...\n",
            " [3.20733976 -9.2560358 -3.22245169 ... -4.0855937 5.09566212 2.03021717]\n",
            " [1.32750607 1.79018462 7.04001379 ... 4.19853687 -7.06443119 7.44843578]\n",
            " [3.5759387 1.37279546 -3.1555655 ... -4.65020561 4.8926034 -3.87447166]]\n",
            "llr_hat:  [[1 0 1 ... 0 1 1]\n",
            " [1 0 1 ... 0 1 1]\n",
            " [1 0 1 ... 0 1 1]\n",
            " ...\n",
            " [0 0 1 ... 0 1 0]\n",
            " [0 0 1 ... 0 1 0]\n",
            " [0 0 1 ... 0 1 0]]\n",
            "z_hat:  [[-0.0851618946 -0.170323789 -0.255485684 ... -3.09491634 -3.26685596 -3.4387958]\n",
            " [0.323653251 0.647306502 0.970959783 ... -6.77722597 -7.1537385 -7.53025103]\n",
            " [-0.310501426 -0.621002853 -0.93150425 ... 3.65138078 3.85423517 4.05709]\n",
            " ...\n",
            " [0.115012787 0.230025575 0.345038354 ... 5.28833628 5.58213234 5.87592888]\n",
            " [-0.396570325 -0.79314065 -1.18971097 ... -2.30434179 -2.43236089 -2.56037974]\n",
            " [-0.0734049901 -0.14680998 -0.220214963 ... -2.34830093 -2.47876191 -2.60922313]]\n",
            "llr:  [[-6.27855396 1.40410316 2.04295826 ... 4.38547802 -3.9707768 -0.708168328]\n",
            " [2.1478982 -0.911401391 8.26352406 ... -7.44882679 -1.23440766 -2.51259041]\n",
            " [-4.1851263 -0.0360223055 -4.25432253 ... -4.07843876 7.1456356 4.50226498]\n",
            " ...\n",
            " [-3.88029432 -2.69573379 -5.30913305 ... 2.86685181 2.09685683 1.29120088]\n",
            " [-4.59553719 0.579791546 7.07496595 ... 1.27665734 -4.05002499 -8.74705124]\n",
            " [-1.95380199 5.29412127 -2.5087986 ... 8.27956486 1.55722392 -2.57377958]]\n",
            "llr_hat:  [[1 0 0 ... 0 0 1]\n",
            " [1 0 0 ... 0 0 1]\n",
            " [1 0 1 ... 0 0 1]\n",
            " ...\n",
            " [1 0 0 ... 1 0 1]\n",
            " [1 0 0 ... 1 0 1]\n",
            " [1 0 0 ... 1 0 1]]\n",
            "z_hat:  [[-0.263927728 -0.527855456 -0.791783214 ... -3.01682019 -3.1844213 -3.35202241]\n",
            " [0.277023405 0.55404681 0.831070185 ... 6.96232605 7.34912157 7.73591757]\n",
            " [0.0567966402 0.11359328 0.17038992 ... 2.36005664 2.49117088 2.62228513]\n",
            " ...\n",
            " [0.278395265 0.556790531 0.835185766 ... -3.82601118 -4.03856754 -4.25112343]\n",
            " [0.0638185292 0.127637058 0.191455588 ... -5.11279583 -5.39684 -5.68088436]\n",
            " [-0.103358895 -0.206717789 -0.310076684 ... 0.337159067 0.355890125 0.374621183]]\n",
            "llr:  [[2.10540867 -6.88773966 -4.12491226 ... -3.77245378 -4.02160025 -4.76833153]\n",
            " [2.91367888 -4.45673704 -1.56763947 ... 4.39170837 3.48974729 1.1479212]\n",
            " [-5.70710659 0.188757062 -5.86714029 ... -2.31224728 -3.79593182 -0.429742336]\n",
            " ...\n",
            " [-10.1364307 -6.25625467 4.80191422 ... -2.38873219 0.0222789049 2.26882339]\n",
            " [5.71281624 -5.06396103 -0.807174206 ... 6.46174288 2.24011683 1.59625113]\n",
            " [3.05415249 -1.75498915 -4.7544961 ... 1.65465403 -3.00834966 -1.43386066]]\n",
            "llr_hat:  [[0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " ...\n",
            " [1 0 1 ... 0 0 0]\n",
            " [1 0 1 ... 0 0 0]\n",
            " [1 0 1 ... 0 0 0]]\n",
            "z_hat:  [[-0.0487435572 -0.0974871144 -0.146230668 ... -3.62447143 -3.82583094 -4.02719069]\n",
            " [0.213250279 0.426500559 0.639750838 ... -1.08338666 -1.14357471 -1.20376289]\n",
            " [0.0670663193 0.134132639 0.201198965 ... -2.83814096 -2.99581528 -3.15348983]\n",
            " ...\n",
            " [-0.0497236513 -0.0994473 -0.14917095 ... 3.27895093 3.46111488 3.64327884]\n",
            " [-0.150703117 -0.301406235 -0.452109337 ... 1.55365729 1.63997149 1.72628582]\n",
            " [0.127245128 0.254490256 0.381735384 ... 0.960049391 1.01338542 1.06672156]]\n",
            "llr:  [[2.76222205 2.21696162 -2.38015842 ... 0.539496183 5.68586 2.58458352]\n",
            " [-1.46754646 -4.33110809 1.54061925 ... 1.53440678 1.09752631 -4.69560051]\n",
            " [3.7887 -3.72764063 2.89784193 ... 0.329399884 -4.13785315 -4.43279457]\n",
            " ...\n",
            " [-2.22898817 -6.09356117 -1.62663758 ... 1.19015491 -6.29859877 -5.79120111]\n",
            " [-1.77622736 1.50506425 -0.540311456 ... -3.3020196 6.46086121 4.25161123]\n",
            " [-2.67044091 -0.469180167 -0.775228143 ... 0.728907049 -3.94511938 -0.665619]]\n",
            "llr_hat:  [[0 1 0 ... 1 0 0]\n",
            " [0 1 0 ... 1 0 0]\n",
            " [0 1 0 ... 1 0 0]\n",
            " ...\n",
            " [1 0 1 ... 0 1 0]\n",
            " [1 0 1 ... 0 1 0]\n",
            " [1 0 1 ... 0 1 0]]\n",
            "z_hat:  [[0.188111126 0.376222253 0.564333379 ... 4.63408375 4.8915329 5.14898205]\n",
            " [-0.156174555 -0.312349111 -0.468523681 ... 0.787930429 0.831704319 0.875478268]\n",
            " [0.0684520528 0.136904106 0.205356151 ... -5.81387615 -6.13686943 -6.45986271]\n",
            " ...\n",
            " [0.0019196393 0.00383927859 0.00575891789 ... -1.43524444 -1.5149802 -1.59471607]\n",
            " [0.112426959 0.224853918 0.337280869 ... 6.21586 6.56118584 6.90651131]\n",
            " [0.100935958 0.201871917 0.302807868 ... 1.42462993 1.50377607 1.58292222]]\n",
            "llr:  [[-9.78464127 5.73788214 -3.41968489 ... -2.67004871 6.06130314 0.0572344661]\n",
            " [-4.69207478 -3.20820308 2.74387717 ... -1.03101015 6.36279917 -3.32665443]\n",
            " [-4.5425539 -6.95338917 2.92823935 ... -5.86980867 9.62887478 -5.68267393]\n",
            " ...\n",
            " [3.29682255 -2.61246395 -2.33195853 ... -8.40387917 -3.0807488 -6.43206501]\n",
            " [3.23865509 0.7941522 5.03510523 ... -5.29042673 0.260286033 -2.89069343]\n",
            " [3.11490774 -1.70623946 4.96087 ... -1.05438626 -0.0985248089 7.05680275]]\n",
            "llr_hat:  [[1 0 1 ... 1 0 0]\n",
            " [1 0 1 ... 1 0 0]\n",
            " [1 0 1 ... 1 0 0]\n",
            " ...\n",
            " [0 1 1 ... 0 0 0]\n",
            " [0 1 1 ... 0 0 0]\n",
            " [0 1 1 ... 0 0 0]]\n",
            "z_hat:  [[-0.334726602 -0.669453204 -1.00417972 ... 0.71819222 0.758091807 0.797991395]\n",
            " [0.0773167908 0.154633582 0.231950358 ... 0.992468297 1.0476054 1.10274255]\n",
            " [-0.433712512 -0.867425 -1.30113745 ... -4.75948811 -5.02390432 -5.28832]\n",
            " ...\n",
            " [-0.165646955 -0.331293911 -0.496940821 ... -2.80406022 -2.95984125 -3.11562252]\n",
            " [0.0810091197 0.162018239 0.243027344 ... 3.98428249 4.20563126 4.4269805]\n",
            " [0.086130254 0.172260508 0.258390754 ... -4.2194252 -4.45383739 -4.68825]]\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 1. 1. 0.]\n",
            " [1. 0. 0. ... 0. 1. 0.]\n",
            " [0. 1. 1. ... 0. 1. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]], shape=(160, 100), dtype=float32) tf.Tensor(\n",
            "[[1. 0. 1. ... 1. 0. 0.]\n",
            " [1. 0. 1. ... 1. 0. 0.]\n",
            " [1. 0. 1. ... 1. 0. 0.]\n",
            " ...\n",
            " [0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 1. 1. ... 0. 0. 0.]], shape=(160, 100), dtype=float32)\n",
            "Training epoch 20/1000, LR=5.00e-04, Loss=7.67018e+00, BER=0.5035, duration: 20.75s\n",
            "llr:  [[-4.68792915 -4.45226669 1.99833548 ... 4.47152042 5.18161392 -2.87403393]\n",
            " [-5.32982063 -5.08742142 -1.0125885 ... -6.9275713 -3.14913774 -2.93041658]\n",
            " [5.63790226 -2.62157297 5.27740574 ... -3.46302509 3.6132338 0.719780922]\n",
            " ...\n",
            " [-3.79296422 2.64700437 -3.85213804 ... -3.26803851 -1.2847414 2.06890082]\n",
            " [5.66028118 -5.67903423 4.67699671 ... 0.548435688 -4.11234617 4.7935133]\n",
            " [0.00705134869 3.78132439 -3.51227903 ... -4.96807671 6.46459389 1.68889737]]\n",
            "llr_hat:  [[1 1 0 ... 1 0 1]\n",
            " [1 1 0 ... 1 0 1]\n",
            " [1 1 0 ... 1 0 1]\n",
            " ...\n",
            " [0 1 0 ... 1 1 1]\n",
            " [0 1 0 ... 1 1 1]\n",
            " [0 1 0 ... 1 1 1]]\n",
            "z_hat:  [[-0.184396476 -0.368792951 -0.553189397 ... -5.67844486 -5.99391365 -6.30938292]\n",
            " [-0.0795656517 -0.159131303 -0.238696963 ... 6.43192291 6.7892518 7.1465807]\n",
            " [0.358235955 0.71647191 1.07470787 ... 1.93223774 2.0395844 2.14693093]\n",
            " ...\n",
            " [-0.122595236 -0.245190471 -0.367785692 ... 4.36232901 4.60468054 4.84703207]\n",
            " [0.121482193 0.242964387 0.36444658 ... 6.81070089 7.18907356 7.56744576]\n",
            " [-0.148631781 -0.297263563 -0.445895344 ... -1.57546282 -1.66298854 -1.75051427]]\n",
            "llr:  [[3.13286495 -2.29017878 4.97745705 ... -5.93960094 3.18600678 9.33607578]\n",
            " [-1.26239884 2.64201808 -1.40951705 ... -2.11359453 8.95599079 2.53002834]\n",
            " [-1.67134666 2.03158164 1.41128743 ... 4.77292919 0.635126293 -0.893251061]\n",
            " ...\n",
            " [2.46920013 4.28675127 -4.66321087 ... -3.10229111 4.42322731 -4.43054247]\n",
            " [-4.10348892 1.11262703 -1.85896921 ... -1.41987908 5.10240555 2.45227599]\n",
            " [4.73555565 7.05677605 -0.334281862 ... 2.05759072 2.46727633 -5.32055378]]\n",
            "llr_hat:  [[0 0 1 ... 0 1 1]\n",
            " [0 0 1 ... 0 1 1]\n",
            " [0 0 1 ... 0 1 1]\n",
            " ...\n",
            " [1 1 1 ... 0 0 0]\n",
            " [1 1 1 ... 0 0 0]\n",
            " [1 1 1 ... 0 0 0]]\n",
            "z_hat:  [[0.10664326 0.213286519 0.319929779 ... 2.422472 2.5570538 2.69163561]\n",
            " [0.0822831616 0.164566323 0.246849477 ... -2.34122705 -2.47129512 -2.60136342]\n",
            " [-0.352793783 -0.705587566 -1.05838132 ... 4.79721737 5.06372929 5.3302412]\n",
            " ...\n",
            " [0.101295933 0.202591866 0.303887784 ... -5.77622938 -6.09713078 -6.41803265]\n",
            " [-0.226957932 -0.453915864 -0.680873811 ... -3.34790111 -3.53389549 -3.71989]\n",
            " [-0.211569846 -0.423139691 -0.634709537 ... 0.498041511 0.525710523 0.553379476]]\n",
            "llr:  [[2.44856977 -0.322909296 -7.95568657 ... -3.01319885 -5.31433773 -2.29431891]\n",
            " [-1.7364552 3.59148932 4.97280359 ... -0.025472343 -0.307246268 5.24079609]\n",
            " [3.8323946 0.47630024 -6.30902481 ... 2.78246713 -5.03946924 3.05015731]\n",
            " ...\n",
            " [-3.0115428 -2.31697416 8.71601105 ... 0.42971009 4.25566 3.68770409]\n",
            " [0.462535799 0.459535062 3.66370559 ... 5.85967684 -3.31309366 2.27674675]\n",
            " [-4.16822767 4.79790735 3.19863653 ... 1.57829368 -2.68371773 4.87137651]]\n",
            "llr_hat:  [[0 0 1 ... 0 1 0]\n",
            " [0 0 1 ... 0 1 0]\n",
            " [0 0 1 ... 0 1 0]\n",
            " ...\n",
            " [0 0 1 ... 1 1 0]\n",
            " [0 0 1 ... 1 1 0]\n",
            " [0 0 1 ... 1 1 0]]\n",
            "z_hat:  [[0.0724284947 0.144856989 0.217285484 ... 3.51322126 3.70840025 3.90357924]\n",
            " [0.308231562 0.616463125 0.924694657 ... -2.17171144 -2.29236197 -2.41301274]\n",
            " [-0.170376942 -0.340753883 -0.51113081 ... -2.33163929 -2.46117473 -2.59071016]\n",
            " ...\n",
            " [0.254560053 0.509120107 0.76368016 ... -3.46597958 -3.65853381 -3.85108829]\n",
            " [-0.0166747235 -0.033349447 -0.0500241704 ... -2.86924124 -3.02864361 -3.18804598]\n",
            " [0.220470205 0.44094041 0.66141063 ... -0.733443141 -0.77419 -0.814936817]]\n",
            "llr:  [[1.4475224 -1.6640625 0.421414196 ... -1.40639305 4.98714113 0.254222035]\n",
            " [-2.43444967 5.32280684 -1.48735642 ... -3.90904832 2.13046694 -6.65872908]\n",
            " [3.31882954 8.16218376 -0.98294127 ... -1.97775483 -2.19708276 -2.1229043]\n",
            " ...\n",
            " [0.649992585 2.46095848 1.8076936 ... -0.961222768 7.07102537 -1.89423168]\n",
            " [3.21167946 1.07463968 -0.47557646 ... 5.36391354 6.79223251 -1.22945392]\n",
            " [-5.36191273 7.8861661 -3.48675346 ... -3.57035565 -0.0992159843 -6.97955942]]\n",
            "llr_hat:  [[0 0 1 ... 1 0 0]\n",
            " [0 0 1 ... 1 0 0]\n",
            " [0 0 1 ... 1 0 0]\n",
            " ...\n",
            " [1 1 0 ... 0 1 1]\n",
            " [1 1 0 ... 0 1 1]\n",
            " [1 1 0 ... 0 1 1]]\n",
            "z_hat:  [[0.0988523811 0.197704762 0.296557128 ... -2.86172247 -3.02070713 -3.17969179]\n",
            " [0.409359217 0.818718433 1.22807765 ... 0.326618 0.344763428 0.36290887]\n",
            " [0.0451903865 0.0903807729 0.135571152 ... -4.20736313 -4.44110584 -4.67484808]\n",
            " ...\n",
            " [-0.0886877403 -0.177375481 -0.266063213 ... 1.26748419 1.33789992 1.40831578]\n",
            " [0.218922898 0.437845796 0.65676868 ... 2.31175971 2.44019079 2.56862187]\n",
            " [0.170731187 0.341462374 0.512193561 ... 4.70769405 4.96923256 5.23077106]]\n",
            "llr:  [[0.704600513 -7.87111235 -2.32100558 ... 1.72934985 -1.94623017 -0.460045576]\n",
            " [-3.62721 -5.33224773 -4.2014246 ... 3.61623693 1.1425879 -6.14020872]\n",
            " [-4.01810646 -4.6906 2.3780055 ... -2.4595232 2.70938182 -3.48896694]\n",
            " ...\n",
            " [-0.656958103 -5.41136408 -1.54160523 ... -0.218966126 2.13830304 -1.94782662]\n",
            " [-6.26634598 4.16111755 -1.75412297 ... -2.90505767 -4.08410072 -5.90199]\n",
            " [-0.820206165 -0.325256348 1.38005412 ... -2.66632032 -1.21236682 -4.1709609]]\n",
            "llr_hat:  [[0 1 0 ... 0 0 1]\n",
            " [0 1 0 ... 0 0 1]\n",
            " [0 1 0 ... 0 0 1]\n",
            " ...\n",
            " [0 1 0 ... 1 0 0]\n",
            " [0 1 0 ... 1 0 0]\n",
            " [0 1 0 ... 1 0 0]]\n",
            "z_hat:  [[0.0852300376 0.170460075 0.255690098 ... -3.09784937 -3.2699523 -3.44205499]\n",
            " [-0.287515461 -0.575030923 -0.862546384 ... 3.78859258 3.99907 4.20954752]\n",
            " [0.085750252 0.171500504 0.257250756 ... -3.74138141 -3.94923592 -4.15709066]\n",
            " ...\n",
            " [0.234243363 0.468486726 0.70273006 ... 1.60810733 1.69744658 1.78678584]\n",
            " [0.30347532 0.606950641 0.910425961 ... -0.994591415 -1.04984653 -1.10510159]\n",
            " [0.051674 0.103348 0.155022 ... 1.26452661 1.33477807 1.40502954]]\n",
            "llr:  [[-3.85712886 -7.37068129 1.70822918 ... -3.22411704 2.20205021 -5.06647301]\n",
            " [1.56390619 6.8052125 -3.19149852 ... -2.57244349 -1.62785935 -0.0256909132]\n",
            " [-2.43100071 -3.12400699 3.41695261 ... -2.20188332 -1.75035834 -0.782345057]\n",
            " ...\n",
            " [6.72999239 -0.246790648 -1.48764873 ... 3.94465256 -1.2629149 2.438622]\n",
            " [3.5873189 5.72481537 1.3490454 ... -8.5931 -2.68274021 3.09112191]\n",
            " [-3.5391469 -7.10143042 -1.28385854 ... 3.48969555 2.67277 0.911630511]]\n",
            "llr_hat:  [[1 0 0 ... 0 1 1]\n",
            " [1 0 0 ... 0 1 1]\n",
            " [1 0 0 ... 0 1 1]\n",
            " ...\n",
            " [1 1 0 ... 1 0 0]\n",
            " [1 1 0 ... 1 0 0]\n",
            " [1 1 0 ... 1 0 0]]\n",
            "z_hat:  [[-0.279310763 -0.558621526 -0.837932289 ... 2.83664918 2.994241 3.15183258]\n",
            " [-0.0498747379 -0.0997494757 -0.149624214 ... -2.45331526 -2.58961058 -2.7259059]\n",
            " [0.313527614 0.627055228 0.940582871 ... -4.55894518 -4.81222 -5.06549454]\n",
            " ...\n",
            " [0.351429254 0.702858508 1.05428779 ... -2.31032443 -2.43867588 -2.56702709]\n",
            " [-0.191238686 -0.382477373 -0.573716044 ... -1.65628636 -1.74830222 -1.8403182]\n",
            " [-0.479655027 -0.959310055 -1.43896508 ... 1.86237371 1.96583891 2.06930423]]\n",
            "llr:  [[0.630932331 0.483735323 -0.53169322 ... -3.13897705 3.47486305 0.789606333]\n",
            " [-1.88546968 5.09285164 4.64584637 ... 3.90662384 -4.55936956 -7.29925728]\n",
            " [11.5986834 -2.52694607 6.11693907 ... -4.10805178 4.67103481 -3.77309179]\n",
            " ...\n",
            " [-4.90875578 -9.34204865 -4.93711281 ... -4.40029049 0.62256825 -3.46746469]\n",
            " [3.66585922 -3.27468324 -0.894646406 ... -4.28673697 8.4683 0.84882617]\n",
            " [-2.54995298 -3.5876646 -1.00710559 ... -4.46927786 1.3533709 9.18349838]]\n",
            "llr_hat:  [[0 1 1 ... 1 0 1]\n",
            " [0 1 1 ... 1 0 1]\n",
            " [0 1 1 ... 1 0 1]\n",
            " ...\n",
            " [1 1 0 ... 0 0 0]\n",
            " [1 1 0 ... 0 0 0]\n",
            " [1 1 0 ... 0 0 0]]\n",
            "z_hat:  [[0.0390520133 0.0781040266 0.117156044 ... -2.26595116 -2.39183736 -2.51772356]\n",
            " [-0.109984145 -0.219968289 -0.329952419 ... 5.23684025 5.52777576 5.81871128]\n",
            " [-0.156140134 -0.312280267 -0.468420386 ... 1.98584926 2.09617424 2.2064991]\n",
            " ...\n",
            " [-0.417102456 -0.834204912 -1.25130737 ... 3.16814446 3.34415269 3.52016068]\n",
            " [0.217511058 0.435022116 0.652533174 ... -4.36650372 -4.60908699 -4.85167074]\n",
            " [-0.264336884 -0.528673768 -0.793010652 ... 4.26645041 4.50347519 4.74050045]]\n",
            "llr:  [[-3.02042723 2.53108478 -4.58457 ... 6.81705189 7.29785585 1.14926803]\n",
            " [-5.95035219 -4.97044277 4.08775187 ... 4.74791479 2.07032824 -5.76721334]\n",
            " [-3.65089035 3.57055664 -0.500177324 ... 6.93860626 -8.89014 3.67109466]\n",
            " ...\n",
            " [-0.382395864 -2.80390453 -8.57312202 ... -4.52574253 5.14697409 -5.06005335]\n",
            " [0.372400939 -3.04860306 -5.82563925 ... 6.36790848 5.95423126 -8.12316895]\n",
            " [-1.84594536 1.453969 -2.91937423 ... 3.887115 2.66507077 1.47543526]]\n",
            "llr_hat:  [[1 1 1 ... 1 0 1]\n",
            " [1 1 1 ... 1 0 1]\n",
            " [1 1 1 ... 1 0 1]\n",
            " ...\n",
            " [0 1 1 ... 1 1 0]\n",
            " [0 1 1 ... 1 1 0]\n",
            " [0 1 1 ... 1 1 0]]\n",
            "z_hat:  [[0.0366566 0.0733132 0.109969795 ... 2.97200799 3.13711953 3.30223107]\n",
            " [0.00496590184 0.00993180368 0.014897706 ... 0.782018363 0.825463831 0.868909299]\n",
            " [-0.0899172276 -0.179834455 -0.269751668 ... -2.57573962 -2.71883607 -2.86193275]\n",
            " ...\n",
            " [-0.264208972 -0.528417945 -0.792626917 ... -0.351512104 -0.371040553 -0.390569]\n",
            " [0.316842884 0.633685768 0.950528622 ... -4.33240128 -4.57309055 -4.81377935]\n",
            " [-0.134389013 -0.268778026 -0.403167039 ... 3.52682614 3.72276092 3.91869569]]\n",
            "llr:  [[4.4084568 -2.36332464 1.09094262 ... -1.36187899 -1.18940353 3.11308932]\n",
            " [7.94898272 2.35922956 0.963243723 ... -6.76816368 5.61283731 -2.28678441]\n",
            " [0.829291 -3.30543375 -0.277355194 ... 2.53486085 3.05147195 -4.97716856]\n",
            " ...\n",
            " [6.85920906 0.399503 0.623782575 ... -0.188445687 1.66123271 -6.59224939]\n",
            " [-2.44872212 0.610133171 -2.69017029 ... -0.295077801 -2.08241367 -11.3132372]\n",
            " [0.412615478 2.41453242 2.13137197 ... -0.9156726 -3.67358565 -0.24113965]]\n",
            "llr_hat:  [[0 1 0 ... 0 1 1]\n",
            " [0 1 0 ... 0 1 1]\n",
            " [0 1 0 ... 0 1 1]\n",
            " ...\n",
            " [1 1 0 ... 0 0 0]\n",
            " [1 1 0 ... 0 0 0]\n",
            " [1 1 0 ... 0 0 0]]\n",
            "z_hat:  [[0.262372524 0.524745047 0.7871176 ... 4.38256359 4.62603903 4.86951494]\n",
            " [0.0469706506 0.0939413 0.140911952 ... -5.44227695 -5.74462605 -6.04697466]\n",
            " [0.239085987 0.478171974 0.717258 ... -3.7460618 -3.95417643 -4.16229105]\n",
            " ...\n",
            " [0.212704271 0.425408542 0.638112783 ... -5.67591429 -5.99124289 -6.30657148]\n",
            " [-0.195637479 -0.391274959 -0.586912453 ... -1.26684463 -1.33722484 -1.40760517]\n",
            " [-0.125543967 -0.251087934 -0.376631916 ... 3.33758378 3.52300501 3.70842648]]\n",
            "llr:  [[2.55455303 -8.49963188 -1.61326885 ... 3.9418602 4.06667852 1.40560389]\n",
            " [0.429202259 -2.5693891 -4.82664108 ... 1.15320635 -2.93477869 1.76084435]\n",
            " [3.36895 -10.0840759 3.83132172 ... -1.00048971 -2.22612429 4.1334939]\n",
            " ...\n",
            " [10.8132935 4.05554914 2.05945563 ... -5.4014883 -5.87920761 -0.320161045]\n",
            " [-2.33114052 3.96688914 1.07421827 ... 3.56113029 -0.189693093 0.217088819]\n",
            " [-7.54131269 -3.83961296 3.00451136 ... 0.304026783 -4.04644251 2.60548353]]\n",
            "llr_hat:  [[0 0 0 ... 1 1 1]\n",
            " [0 0 0 ... 1 1 1]\n",
            " [0 0 0 ... 1 1 1]\n",
            " ...\n",
            " [0 1 0 ... 1 1 1]\n",
            " [0 1 0 ... 1 1 1]\n",
            " [0 1 0 ... 1 1 1]]\n",
            "z_hat:  [[0.177727669 0.355455339 0.533183 ... 5.17420912 5.46166515 5.74912119]\n",
            " [0.155282646 0.310565293 0.465847939 ... 4.154181 4.38496876 4.61575651]\n",
            " [0.0822159052 0.16443181 0.246647716 ... 1.72474337 1.82056248 1.9163816]\n",
            " ...\n",
            " [-0.340233892 -0.680467784 -1.02070165 ... 0.357569158 0.377434134 0.397299081]\n",
            " [-0.113681339 -0.227362677 -0.341044 ... 6.35738182 6.71057 7.0637579]\n",
            " [-0.382543147 -0.765086293 -1.1476295 ... 0.729420543 0.769943953 0.810467303]]\n",
            "llr:  [[0.0905424356 -8.05059528 -2.95898581 ... 1.52533221 -2.7759366 6.96399593]\n",
            " [-1.51097226 4.20857143 6.59819174 ... -1.69584823 -0.331879079 6.32811308]\n",
            " [0.915674686 0.526386499 -4.50440359 ... -0.415385127 3.93070745 4.37433863]\n",
            " ...\n",
            " [-2.10045934 1.79003835 0.942039251 ... -1.15305758 -3.14730239 -5.51843739]\n",
            " [-4.22792625 -1.33620322 2.59881902 ... -4.17963362 6.41491222 -6.97273636]\n",
            " [4.69883347 0.662865102 3.60758066 ... 5.55480528 4.76290417 -6.39947748]]\n",
            "llr_hat:  [[1 0 0 ... 1 0 0]\n",
            " [1 0 0 ... 1 0 0]\n",
            " [1 0 0 ... 1 0 0]\n",
            " ...\n",
            " [1 1 0 ... 0 1 1]\n",
            " [1 1 0 ... 0 1 1]\n",
            " [1 1 0 ... 0 1 1]]\n",
            "z_hat:  [[-0.00393102225 -0.0078620445 -0.0117930658 ... 3.20989227 3.38821983 3.56654716]\n",
            " [0.102695294 0.205390587 0.308085859 ... 4.51743317 4.76840162 5.01937]\n",
            " [0.409569919 0.819139838 1.2287097 ... -2.92036843 -3.08261108 -3.24485397]\n",
            " ...\n",
            " [-0.162999555 -0.325999111 -0.488998622 ... 5.50151157 5.80715132 6.11279106]\n",
            " [0.0888419896 0.177683979 0.266525954 ... 7.52304268 7.94098949 8.35893631]\n",
            " [0.355792403 0.711584806 1.06737721 ... -0.945839703 -0.998386323 -1.050933]]\n",
            "tf.Tensor(\n",
            "[[1. 1. 0. ... 0. 0. 1.]\n",
            " [0. 1. 1. ... 0. 1. 0.]\n",
            " [0. 1. 0. ... 1. 1. 1.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 1.]\n",
            " [0. 1. 1. ... 0. 1. 1.]\n",
            " [1. 1. 1. ... 0. 0. 0.]], shape=(160, 100), dtype=float32) tf.Tensor(\n",
            "[[1. 0. 0. ... 1. 0. 0.]\n",
            " [1. 0. 0. ... 1. 0. 0.]\n",
            " [1. 0. 0. ... 1. 0. 0.]\n",
            " ...\n",
            " [1. 1. 0. ... 0. 1. 1.]\n",
            " [1. 1. 0. ... 0. 1. 1.]\n",
            " [1. 1. 0. ... 0. 1. 1.]], shape=(160, 100), dtype=float32)\n",
            "Training epoch 30/1000, LR=4.99e-04, Loss=7.81043e+00, BER=0.4965, duration: 20.24s\n",
            "llr:  [[-7.39463949 -1.57061839 -5.73782492 ... 8.45595 1.1700896 0.299788415]\n",
            " [-3.57823491 2.6088829 2.70308113 ... -1.23381972 2.3109684 -1.63629961]\n",
            " [4.57123613 0.532465816 2.66947269 ... -0.956444085 -1.83806598 3.0456]\n",
            " ...\n",
            " [-2.03999472 -1.75822759 -0.515733957 ... -5.20879459 0.70743078 -1.15575886]\n",
            " [-3.2716639 -2.43207574 -1.28563309 ... -2.45308304 -4.96752691 3.08802509]\n",
            " [-3.86611772 -0.215307057 4.88115692 ... -1.05253255 -0.276638448 4.32883453]]\n",
            "llr_hat:  [[1 0 0 ... 1 0 1]\n",
            " [1 0 0 ... 1 0 1]\n",
            " [1 0 0 ... 1 0 1]\n",
            " ...\n",
            " [0 1 1 ... 1 1 1]\n",
            " [0 1 1 ... 1 1 1]\n",
            " [0 1 1 ... 1 1 1]]\n",
            "z_hat:  [[-0.0444578491 -0.0889157 -0.133373544 ... 3.73529816 3.94281483 4.1503315]\n",
            " [-0.00053425139 -0.00106850278 -0.00160275423 ... 4.57733679 4.83163309 5.08593]\n",
            " [0.191666737 0.383333474 0.575000226 ... -0.625418246 -0.660163701 -0.694909155]\n",
            " ...\n",
            " [-0.14812547 -0.296250939 -0.444376409 ... 0.357951224 0.37783739 0.397723585]\n",
            " [0.296837419 0.593674839 0.890512228 ... 1.54523134 1.63107753 1.71692371]\n",
            " [-0.234403655 -0.46880731 -0.70321095 ... 3.93115735 4.14955521 4.36795282]]\n",
            "llr:  [[6.91235065 2.71363926 -3.83351684 ... -2.57368827 -4.32288074 -0.735388041]\n",
            " [5.84373 -5.22508192 3.70438 ... -6.39954567 5.56655169 -1.64264178]\n",
            " [4.46986341 0.881546497 1.89282787 ... -1.27376008 1.06120539 -4.16243601]\n",
            " ...\n",
            " [-3.20495462 -16.4714031 2.20638609 ... 3.91492534 2.49614096 2.76771545]\n",
            " [-4.44486284 8.11882114 4.28156853 ... -2.42199326 -7.23088455 -1.93228889]\n",
            " [10.400547 -7.00747108 -0.652858078 ... -1.68147171 -3.41497397 4.24710131]]\n",
            "llr_hat:  [[0 1 1 ... 1 1 1]\n",
            " [0 1 1 ... 1 1 1]\n",
            " [0 1 1 ... 1 1 1]\n",
            " ...\n",
            " [0 0 1 ... 1 0 0]\n",
            " [0 0 1 ... 1 0 0]\n",
            " [0 0 1 ... 1 0 0]]\n",
            "z_hat:  [[0.395617574 0.791235149 1.18685269 ... -2.70841646 -2.85888386 -3.00935149]\n",
            " [-0.341979831 -0.683959663 -1.02593946 ... -4.75363874 -5.01772976 -5.28182077]\n",
            " [-0.323792249 -0.647584498 -0.971376777 ... 2.50997782 2.64942122 2.78886437]\n",
            " ...\n",
            " [-0.873570263 -1.74714053 -2.62071085 ... -7.83965206 -8.27518845 -8.71072483]\n",
            " [-0.265341252 -0.530682504 -0.796023726 ... 0.677855849 0.715514481 0.753173172]\n",
            " [-0.227606133 -0.455212265 -0.682818413 ... 1.89906204 2.00456548 2.11006904]]\n",
            "llr:  [[0.0983535647 2.3283608 -1.13143075 ... 5.16484213 -2.69575191 -0.781069398]\n",
            " [4.64209557 -2.29102945 -3.39493775 ... -6.84727669 1.43665063 -2.0685997]\n",
            " [0.778972805 -5.83676338 -1.26525164 ... -2.03473258 -1.75525761 -1.2661438]\n",
            " ...\n",
            " [-0.789123774 6.75041389 -4.61540604 ... -6.81110859 3.51199 -2.89195442]\n",
            " [-2.01559448 5.10583687 0.467926085 ... -6.56181622 1.76028395 0.951094]\n",
            " [5.1888361 -2.34361887 -1.30890691 ... 6.09008217 -4.39601278 3.02941823]]\n",
            "llr_hat:  [[0 0 1 ... 1 1 0]\n",
            " [1 0 1 ... 1 1 0]\n",
            " [1 0 1 ... 1 1 0]\n",
            " ...\n",
            " [1 0 1 ... 1 1 1]\n",
            " [1 0 1 ... 1 1 1]\n",
            " [1 0 1 ... 1 1 1]]\n",
            "z_hat:  [[0.0549176857 0.109835371 0.16475305 ... 4.61384392 4.87016821 5.12649298]\n",
            " [0.354308456 0.708616912 1.06292534 ... -2.79514432 -2.95043 -3.10571575]\n",
            " [-0.378628939 -0.757257879 -1.13588679 ... 4.88713 5.15863705 5.43014431]\n",
            " ...\n",
            " [-0.228851363 -0.457702726 -0.686554074 ... -1.35406315 -1.42928898 -1.50451469]\n",
            " [-0.00969563797 -0.0193912759 -0.0290869139 ... 2.40450644 2.53809023 2.67167377]\n",
            " [0.285044611 0.570089221 0.855133832 ... -2.14468288 -2.26383209 -2.38298106]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3edb022b8503>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mtrain_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2e_ltd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-3edb022b8503>\u001b[0m in \u001b[0;36mtrain_dec\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Linear Transformer Diffusion Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# eval train iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuPmknHQuxFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}