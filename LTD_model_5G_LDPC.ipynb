{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2Z3xfeRpOgV9D3qdkjytx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/5G-Decoder/blob/main/LTD_model_5G_LDPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rSR0Kwz5vmA",
        "outputId": "ad7e19d8-9c1a-4da3-ee4f-cdb773b1d92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '5G-Decoder'...\n",
            "remote: Enumerating objects: 1491, done.\u001b[K\n",
            "remote: Counting objects: 100% (1491/1491), done.\u001b[K\n",
            "remote: Compressing objects: 100% (551/551), done.\u001b[K\n",
            "remote: Total 1491 (delta 943), reused 1462 (delta 926), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1491/1491), 2.03 MiB | 11.09 MiB/s, done.\n",
            "Resolving deltas: 100% (943/943), done.\n",
            "Collecting sionna\n",
            "  Downloading sionna-0.19.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting tensorflow<2.16.0,>=2.13.0 (from sionna)\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sionna) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.5.3 in /usr/local/lib/python3.10/dist-packages (from sionna) (3.8.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sionna) (1.13.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from sionna) (6.4.5)\n",
            "Collecting mitsuba<3.6.0,>=3.2.0 (from sionna)\n",
            "  Downloading mitsuba-3.5.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pythreejs>=2.4.2 (from sionna)\n",
            "  Downloading pythreejs-2.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from sionna)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting ipydatawidgets==4.3.2 (from sionna)\n",
            "  Downloading ipydatawidgets-4.3.2-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting jupyterlab-widgets==3.0.5 (from sionna)\n",
            "  Downloading jupyterlab_widgets-3.0.5-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: traittypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipydatawidgets==4.3.2->sionna) (0.2.1)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->sionna)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.0.4->sionna)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: pip is looking at multiple versions of ipywidgets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting ipywidgets>=8.0.4 (from sionna)\n",
            "  Downloading ipywidgets-8.1.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (5.5.6)\n",
            "  Downloading ipywidgets-8.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "INFO: pip is still looking at multiple versions of ipywidgets to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading ipywidgets-8.0.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->sionna) (2.8.2)\n",
            "Collecting drjit==0.4.6 (from mitsuba<3.6.0,>=3.2.0->sionna)\n",
            "  Downloading drjit-0.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.68.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow<2.16.0,>=2.13.0->sionna)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16.0,>=2.13.0->sionna) (0.45.1)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.2.2)\n",
            "Downloading sionna-0.19.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipydatawidgets-4.3.2-py2.py3-none-any.whl (271 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_widgets-3.0.5-py3-none-any.whl (384 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.3/384.3 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.0.5-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mitsuba-3.5.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (40.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading drjit-0.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pythreejs-2.4.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, widgetsnbextension, tensorflow-estimator, ml-dtypes, keras, jupyterlab-widgets, jedi, drjit, mitsuba, ipywidgets, tensorboard, ipydatawidgets, tensorflow, pythreejs, sionna\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.0\n",
            "    Uninstalling wrapt-1.17.0:\n",
            "      Successfully uninstalled wrapt-1.17.0\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: jupyterlab-widgets\n",
            "    Found existing installation: jupyterlab_widgets 3.0.13\n",
            "    Uninstalling jupyterlab_widgets-3.0.13:\n",
            "      Successfully uninstalled jupyterlab_widgets-3.0.13\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed drjit-0.4.6 ipydatawidgets-4.3.2 ipywidgets-8.0.5 jedi-0.19.2 jupyterlab-widgets-3.0.5 keras-2.15.0 mitsuba-3.5.2 ml-dtypes-0.3.2 pythreejs-2.4.2 sionna-0.19.1 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 widgetsnbextension-4.0.13 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pollyjuice74/5G-Decoder\n",
        "# !git clone https://github.com/NVlabs/gnn-decoder.git\n",
        "!pip install sionna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy.sparse import issparse, csr_matrix\n",
        "\n",
        "from sionna.fec.utils import generate_reg_ldpc, load_parity_check_examples, LinearEncoder, gm2pcm\n",
        "from sionna.utils.plotting import PlotBER\n",
        "from sionna.fec.ldpc import LDPCBPDecoder, LDPC5GEncoder, LDPC5GDecoder\n",
        "\n",
        "# from gnn import * # load GNN functions\n",
        "# from wbp import * # load weighted BP functions\n",
        "\n",
        "import os\n",
        "# os.chdir('../..')\n",
        "if os.path.exists('5G-Decoder'):\n",
        "  os.rename('5G-Decoder', '5G_Decoder')\n",
        "os.chdir('5G_Decoder/adv_nn')\n",
        "\n",
        "from dataset import *\n",
        "from attention import *\n",
        "from channel import *\n",
        "from args import *\n",
        "from model_functs import *\n",
        "from models import *"
      ],
      "metadata": {
        "id": "dPJ99LvS6imV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@staticmethod\n",
        "def bin_to_llr(x):\n",
        "    \"\"\"\n",
        "    Converts binary values (0 or 1) to log-likelihood ratios (LLRs), clipping values to ±20 for numerical stability.\n",
        "    Args:\n",
        "        x (Tensor): Binary input tensor with values 0 or 1.\n",
        "    Returns:\n",
        "        Tensor: Tensor of LLR values with clipped range.\n",
        "    \"\"\"\n",
        "    llr_vector = tf.where(x == 0, -20, 20)\n",
        "    return llr_vector\n",
        "\n",
        "@staticmethod\n",
        "def llr_to_bin(c):\n",
        "    \"\"\"\n",
        "    Converts log-likelihood ratios (LLRs) to binary values based on their sign.\n",
        "    Args:\n",
        "        c (Tensor): Tensor of LLR values.\n",
        "    Returns:\n",
        "        Tensor: Binary tensor with values 0 or 1.\n",
        "    \"\"\"\n",
        "    return tf.cast(tf.greater(c, 0), tf.int32)\n",
        "\n",
        "def generate_pruned_pcm_5g(decoder, n, verbose=True):\n",
        "    \"\"\"Utility function to get the pruned parity-check matrix of the 5G code.\n",
        "\n",
        "    Identifies the pruned and shortened positions.\n",
        "    Hereby, '0' indicates an pruned codeword position\n",
        "    '1' indicates an codeword position\n",
        "    '2' indicates a shortened position.\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    decoder: LDPC5GDecoder\n",
        "        An instance of the decoder object.\n",
        "\n",
        "    n: int\n",
        "        The codeword lengths including rate-matching.\n",
        "\n",
        "    verbose: Boolean\n",
        "        Defaults to True. If True, status information during pruning is\n",
        "        provided.\n",
        "    \"\"\"\n",
        "\n",
        "    enc = decoder._encoder\n",
        "\n",
        "    # transmitted positions\n",
        "    pos_tx = np.ones(n)\n",
        "\n",
        "    # undo puncturing of the first 2*z information bits\n",
        "    pos_punc = np.concatenate([np.zeros([2*enc.z]),pos_tx], axis=0)\n",
        "\n",
        "    # puncturing of the last positions\n",
        "    # total length must be n_ldpc, while pos_tx has length n\n",
        "    # first 2*z positions are already added\n",
        "    # -> add n_ldpc - n - 2Z punctured positions\n",
        "    k_short = enc.k_ldpc - enc.k # number of shortend bits\n",
        "    num_punc_bits = ((enc.n_ldpc - k_short) - enc.n - 2*enc.z)\n",
        "    pos_punc2 = np.concatenate(\n",
        "               [pos_punc, np.zeros([num_punc_bits - decoder._nb_pruned_nodes])])\n",
        "\n",
        "    # shortening (= add 0 positions after k bits, i.e. LLR=LLR_max)\n",
        "    # the first k positions are the systematic bits\n",
        "    pos_info = pos_punc2[0:enc.k]\n",
        "\n",
        "    # parity part\n",
        "    num_par_bits = (enc.n_ldpc-k_short-enc.k-decoder._nb_pruned_nodes)\n",
        "    pos_parity = pos_punc2[enc.k:enc.k+num_par_bits]\n",
        "    pos_short = 2 * np.ones([k_short]) # \"2\" indicates shortened position\n",
        "\n",
        "    # and concatenate final pattern\n",
        "    rm_pattern = np.concatenate([pos_info, pos_short, pos_parity], axis=0)\n",
        "\n",
        "    # and prune matrix (remove shortend positions from pcm)\n",
        "    pcm_pruned = np.copy(decoder.pcm.todense())\n",
        "    idx_short = np.where(rm_pattern==2)\n",
        "    idx_pruned = np.setdiff1d(np.arange(pcm_pruned.shape[1]), idx_short)\n",
        "    pcm_pruned = pcm_pruned[:,idx_pruned]\n",
        "    num_shortened = np.size(idx_short)\n",
        "\n",
        "    # print information if enabled\n",
        "    if verbose:\n",
        "        print(\"using bg: \", enc._bg)\n",
        "        print(\"# information bits:\", enc.k)\n",
        "        print(\"CW length after rate-matching:\", n)\n",
        "        print(\"CW length without rm (incl. first 2*Z info bits):\",\n",
        "                                    pcm_pruned.shape[1])\n",
        "        print(\"# punctured bits:\", num_punc_bits)\n",
        "        print(\"# pruned nodes:\", decoder._nb_pruned_nodes)\n",
        "        print(\"# parity bits\", num_par_bits)\n",
        "        print(\"# shortened bits\", num_shortened)\n",
        "        print(\"pruned pcm dimension:\", pcm_pruned.shape)\n",
        "    return pcm_pruned, rm_pattern[idx_pruned]"
      ],
      "metadata": {
        "id": "OpUoHC5GBvet"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sionna.fec.ldpc import LDPCBPDecoder, LDPC5GEncoder, LDPC5GDecoder\n",
        "k = 20\n",
        "n = 50\n",
        "\n",
        "encoder_5g = LDPC5GEncoder(k, n)\n",
        "decoder_5g = LDPC5GDecoder(encoder_5g,\n",
        "                            num_iter=10,#params[\"eval_num_iter\"],\n",
        "                            return_infobits=False,\n",
        "                            prune_pcm=True\n",
        "                            )\n",
        "\n",
        "pcm, rm_pattern = generate_pruned_pcm_5g(decoder_5g, n)\n",
        "\n",
        "n_no_rm = pcm.shape[1]\n",
        "k_no_rm = pcm.shape[1] - pcm.shape[0]\n",
        "\n",
        "# create encoder without rate-matching\n",
        "u_ref = np.eye(k)\n",
        "c_ref = encoder_5g(u_ref).numpy()\n",
        "gm = np.concatenate([u_ref[:,:2*encoder_5g._z], c_ref], axis=1)\n",
        "encoder_no_rm = LinearEncoder(gm, is_pcm=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YqKeApD-i48",
        "outputId": "c1f65880-3cf2-4c8f-ff3c-b16236910e02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using bg:  bg2\n",
            "# information bits: 20\n",
            "CW length after rate-matching: 50\n",
            "CW length without rm (incl. first 2*Z info bits): 58\n",
            "# punctured bits: 130\n",
            "# pruned nodes: 130\n",
            "# parity bits 38\n",
            "# shortened bits 20\n",
            "pruned pcm dimension: (38, 58)\n",
            "Warning: The alias fec.utils.LinearEncoder will not be included in Sionna 1.0. Please use fec.linear.LinearEncoder instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = tf.floor(tf.random.uniform((1, k), minval=0, maxval=2))\n",
        "c = encoder_no_rm(b)\n",
        "\n",
        "# print(pcm @ tf.transpose(c) % 2)\n",
        "b, c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqLU2p_XFKnj",
        "outputId": "8317e4b6-2355-4e58-ac80-3a7b5c130052"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
              " array([[0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         1., 1., 1., 0.]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 58), dtype=float32, numpy=\n",
              " array([[0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
              "         0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
              "         1., 1., 1., 0., 0., 1., 0., 1., 0., 0.]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# precompute pruned positions\n",
        "gather_ind = encoder_5g.n * np.ones(np.size(rm_pattern))\n",
        "gather_ind_inv = np.zeros(np.size(np.where(rm_pattern==1)))\n",
        "for idx, pos in enumerate(np.where(rm_pattern==1)[0]):\n",
        "    gather_ind[pos] = idx\n",
        "    gather_ind_inv[idx] = pos\n",
        "# rm indexes\n",
        "rm_ind = tf.constant(gather_ind, tf.int32)\n",
        "rm_inv_ind = tf.constant(gather_ind_inv, tf.int32)\n",
        "\n",
        "# cw\n",
        "c = encoder_5g(b)\n",
        "c_in = tf.concat([c, tf.zeros([1, 1], tf.float32)],\n",
        "                           axis=1)\n",
        "c_rm = tf.gather(c_in, rm_ind, axis=1)\n",
        "\n",
        "print(pcm)\n",
        "# syndrome\n",
        "pcm @ tf.transpose(c_rm) % 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNKiWgDww9NX",
        "outputId": "d68cf497-8a65-4942-9832-3a63dc40b4fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(38, 1), dtype=float32, numpy=\n",
              "array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for e2e model\n",
        "from sionna.utils import BinarySource, ebnodb2no\n",
        "from sionna.mapping import Mapper, Demapper\n",
        "from sionna.channel import AWGN\n",
        "# from sionna.fec.ldpc import LDPC5GDecoder, LDPC5GEncoder\n",
        "from tensorflow.keras.layers import Layer, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Args():\n",
        "    def __init__(self, model_type, code_type='LDPC', n_look_up=121, k_look_up=80, n=400, k=200,\n",
        "                       n_rings=2, ls_active=True, split_diff=True, sigma=0.1,\n",
        "                       t_layers=1, d_model=64, heads=8, lr=5e-4,\n",
        "                       batch_size=160, batch_size_eval = 150,\n",
        "                       eval_train_iter=50, save_weights_iter=100,\n",
        "                       ebno_db_eval=2.5,\n",
        "                       ebno_db_min=0., ebno_db_max=4., ebno_db_stepsize=0.25,\n",
        "                       traindata_len=500, testdata_len=250,\n",
        "                       mc_batch_size=200, mc_iters=500, epochs=1000000):\n",
        "        assert model_type in ['gen', 'dis'], \"Type must be: 'gen', Generator or 'dis', Discriminator.\"\n",
        "        assert code_type in ['POLAR', 'BCH', 'CCSDS', 'LDPC', 'MACKAY', 'LDPC5G', 'POLAR5G'], \"Invalid linear code type.\"\n",
        "\n",
        "        # model data\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.split_diff = split_diff\n",
        "        self.n_rings = n_rings # ring connectivity of mask\n",
        "        self.sigma = sigma\n",
        "        self.t_layers = t_layers\n",
        "        self.ls_active = ls_active\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "\n",
        "        # training data\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.traindata_len = traindata_len\n",
        "        self.testdata_len = testdata_len\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.ebno_db_min = ebno_db_min\n",
        "        self.ebno_db_max = ebno_db_max\n",
        "        self.ebno_db_stepsize = ebno_db_stepsize\n",
        "\n",
        "        self.ebno_db_eval = ebno_db_eval\n",
        "        self.eval_train_iter = eval_train_iter\n",
        "        self.save_weights_iter = save_weights_iter\n",
        "        self.batch_size_eval = batch_size_eval\n",
        "\n",
        "        # simulation\n",
        "        self.mc_batch_size = mc_batch_size\n",
        "        self.mc_iters = mc_iters\n",
        "\n",
        "        # code data\n",
        "        self.code_type = code_type\n",
        "        self.code = self.get_code(n_look_up, k_look_up) # n,k look up values in Get_Generator_and_Parity\n",
        "\n",
        "        # if self.code_type not in ['LDPC5G', 'POLAR5G']:\n",
        "        #     self.n, self.m, self.k = self.code.n, self.code.m, self.code.k\n",
        "        # else:\n",
        "        #     self.n, self.m, self.k = n, n-k, k\n",
        "\n",
        "        # self.n_steps = self.m + 5  # Number of diffusion steps\n",
        "\n",
        "    def get_code(self, n_look_up, k_look_up):\n",
        "        code = type('Code', (), {})() # class Code, no base class, no attributes/methods, () instantiate object\n",
        "        # code.n_look_up, code.k_look_up = n_look_up, k_look_up\n",
        "        # code.code_type = self.code_type\n",
        "\n",
        "        # if self.code_type not in ['LDPC5G', 'POLAR5G']:\n",
        "        #     G, H = Get_Generator_and_Parity(code)\n",
        "        #     code.G, code.H = tf.convert_to_tensor(G), csr_matrix( tf.convert_to_tensor(H) )\n",
        "\n",
        "        #     code.m, code.n = code.H.shape\n",
        "        #     code.k = code.n - code.m\n",
        "\n",
        "        return code\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Dense, LayerNormalization, Dropout\n",
        "from tensorflow import einsum, multiply\n",
        "\n",
        "class LinearMHAttention( Layer ):\n",
        "    def __init__(self, num_heads, key_dim, mask_shape, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        assert (key_dim % num_heads) == 0, 'dimension must be divisible by the number of heads'\n",
        "        self.dims = key_dim\n",
        "        self.heads = num_heads\n",
        "        self.dim_head = self.dims // self.heads\n",
        "\n",
        "        self.k_proj = self.get_k_proj(mask_shape) # n+m\n",
        "        self.proj_k = None\n",
        "        self.proj_v = None\n",
        "\n",
        "        self.to_q, self.to_k, self.to_v = [ Dense(self.dims, use_bias=False) for _ in range(3) ]\n",
        "        self.to_out = Dense(self.dims)\n",
        "        self.dropout = Dropout(dropout) # to d-dimentional embeddings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        n_value = input_shape[1] # (b, n, d)\n",
        "        # Creates shape (n,k_proj) proj matrices for key and value\n",
        "        self.proj_k = self.add_weight(\"proj_k\", shape=[n_value, self.k_proj], initializer=GlorotUniform())\n",
        "        self.proj_v = self.add_weight(\"proj_v\", shape=[n_value, self.k_proj], initializer=GlorotUniform())\n",
        "\n",
        "    def get_k_proj(self, mask_shape):\n",
        "        mask_length = mask_shape[1] # mask_shape (b, n+m, n+m)\n",
        "        # gets dimention for linear tranformer vector projection\n",
        "        for k_proj in range(mask_length // 2, 0, -1): # starts at half the mask length TO 0\n",
        "            if mask_length % k_proj == 0:\n",
        "                return tf.cast(k_proj, tf.int32)\n",
        "\n",
        "    def call(self, query, value, key=None, attention_mask=None, training=False): # O(n)\n",
        "        shape = tf.shape(query) # (b, n, d)\n",
        "        b = tf.cast(shape[0], tf.int32)\n",
        "        n = tf.cast(shape[1], tf.int32)\n",
        "\n",
        "        key = value if key is None else key\n",
        "\n",
        "        assert query.shape[-1] is not None, \"The last dimension of x is undefined.\"\n",
        "\n",
        "        query, key, val = self.to_q(query), self.to_k(key), self.to_v(value)\n",
        "\n",
        "        # Project key and val into k-dimentional space\n",
        "        key = tf.einsum('bnd,nk->bkd', key, self.proj_k)\n",
        "        val = tf.einsum('bnd,nk->bkd', val, self.proj_v)\n",
        "\n",
        "        # Reshape splitting for heads\n",
        "        query = tf.reshape(query, (b, n, self.heads, self.dim_head))\n",
        "        key = tf.reshape(key, (b, self.k_proj, self.heads, self.dim_head))\n",
        "        val = tf.reshape(val, (b, self.k_proj, self.heads, self.dim_head))\n",
        "        query, key, val = [ tf.transpose(x, [0, 2, 1, 3]) for x in [query, key, val] ]\n",
        "\n",
        "        # Low-rank mask (n,k_proj)\n",
        "        mask = tf.expand_dims(attention_mask, axis=-1)\n",
        "        mask = tf.image.resize(mask, [n, self.k_proj], method='nearest')\n",
        "        mask = tf.reshape(mask, (1, 1, n, self.k_proj))\n",
        "\n",
        "        # Main attn logic: sftmx( q@k / d**0.5 ) @ v\n",
        "        scores = tf.einsum('bhnd,bhkd->bhnk', query, key) / (tf.sqrt( tf.cast(self.dim_head, dtype=tf.float32) ))\n",
        "        scores += (mask * -1e9) if mask is not None else 0.\n",
        "        attn = tf.nn.softmax(scores, axis=-1) # (b,h,n,k_proj)\n",
        "        attn = self.dropout(attn) if training else attn\n",
        "        out = tf.einsum('bhnk,bhkd->bhnd', attn, val)\n",
        "\n",
        "        # Reshape and pass through out layer\n",
        "        out = tf.transpose(out, [0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (b, n, -1))\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, linear, mask_shape, dropout_rate=0.1):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.mha = (\n",
        "            LinearMHAttention(num_heads=num_heads, key_dim=d_model, mask_shape=mask_shape, dropout=dropout_rate)\n",
        "            if linear\n",
        "            else MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
        "        )\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(d_ff, activation='relu'),\n",
        "            Dense(d_model),\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, mask, training):\n",
        "        # Multi-Head Attention\n",
        "        attn_output = self.mha(x, x, attention_mask=mask, training=training)\n",
        "        # tf.print(\"attn_output\", attn_output.shape)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # Add & Normalize\n",
        "\n",
        "        # Feedforward Network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        # tf.print(\"ffn_output\", ffn_output.shape)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Add & Normalize\n",
        "        return out2\n",
        "\n",
        "\n",
        "class Decoder( Layer ):\n",
        "    def __init__(self, args, linear=True, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        code = args.code\n",
        "        self.pcm = tf.cast(code.H, dtype=tf.int32)\n",
        "\n",
        "        # shapes\n",
        "        self._m, self._n = self.pcm.shape\n",
        "        self._k = self._n - self._m\n",
        "        self.dims = args.d_model\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        # mask\n",
        "        self.mask = self.create_mask(self.pcm)\n",
        "        for matrix, title in zip([self.pcm, tf.squeeze(self.mask, axis=0)], [\"PCM Matrix\", \"Mask Matrix\"]):\n",
        "            plt.imshow(matrix, cmap='viridis'); plt.colorbar(); plt.title(title); plt.show()\n",
        "        # print(\"mask, pcm: \", self.mask, self.pcm)\n",
        "\n",
        "        # layers\n",
        "        self.node_embeddings = Dense(self.dims)\n",
        "        self.encoder_blocks = [\n",
        "            TransformerEncoderBlock(\n",
        "                d_model=args.d_model,\n",
        "                num_heads=args.heads,\n",
        "                d_ff=args.d_model * 4,\n",
        "                linear=linear,\n",
        "                mask_shape=self.mask.shape,\n",
        "                dropout_rate=dropout_rate,\n",
        "            )\n",
        "            for _ in range(args.t_layers)\n",
        "        ]\n",
        "        self.forward_channel = Dense(1)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.to_n = Dense(self._n)\n",
        "\n",
        "    def create_mask(self, H):\n",
        "        # Initialize diagonal identity mask\n",
        "        mask = tf.eye(2 * self._n - self._k, dtype=tf.float32)\n",
        "\n",
        "        # Get indices where H == 1\n",
        "        indices = tf.where(H == 1)  # Returns (row, col) pairs where H is 1\n",
        "        check_nodes, variable_nodes = indices[:, 0], indices[:, 1]\n",
        "\n",
        "        # Step 1: Update check node to variable node connections\n",
        "        mask = tf.tensor_scatter_nd_update(mask,\n",
        "                                          tf.stack([n + check_nodes, variable_nodes], axis=1),\n",
        "                                          tf.ones_like(check_nodes, dtype=tf.float32))\n",
        "        mask = tf.tensor_scatter_nd_update(mask,\n",
        "                                          tf.stack([variable_nodes, n + check_nodes], axis=1),\n",
        "                                          tf.ones_like(check_nodes, dtype=tf.float32))\n",
        "\n",
        "        # Step 2: Update variable node connections\n",
        "        for cn in tf.unique(check_nodes)[0]:  # Iterate over unique check nodes\n",
        "            related_vns = tf.boolean_mask(variable_nodes, check_nodes == cn)\n",
        "            indices = tf.stack(tf.meshgrid(related_vns, related_vns), axis=-1)\n",
        "            indices = tf.reshape(indices, [-1, 2])  # Flatten indices\n",
        "            mask = tf.tensor_scatter_nd_update(mask, indices, tf.ones_like(indices[:, 0], dtype=tf.float32))\n",
        "\n",
        "        # Tile mask across batch size for tf MHA\n",
        "        mask = tf.expand_dims(mask, axis=0)  # Shape: (1, n+m, n+m)\n",
        "        # mask = tf.tile(mask, [self.batch_size, 1, 1])  # Shape: (b, n+m, n+m)\n",
        "        return mask\n",
        "\n",
        "    def get_syndrome(self, vn_vector, from_llr=True):\n",
        "        \"\"\" Calculate syndrome (pcm @ r = 0) if r is correct in binary \"\"\"\n",
        "        vn_vector = tf.transpose(vn_vector) # (n,b)\n",
        "        bin_vector = llr_to_bin(vn_vector) if from_llr else vn_vector\n",
        "        return tf.cast( (self.pcm @ bin_vector) % 2, dtype=tf.float32) # (m,n)@(n,b)->(m,b)\n",
        "\n",
        "    def call(self, x_nodes, training=False):\n",
        "        # tf.print(\"DECODER CALL\", training)\n",
        "        # tf.print(\"x_nodes\", x_nodes.shape)\n",
        "        # Embed cn/vn nodes vector\n",
        "        x_nodes_embedded = self.node_embeddings( x_nodes ) # (b, n+m, hidden_dims)\n",
        "        # Pass through each encoder block\n",
        "        for block in self.encoder_blocks:\n",
        "            x_nodes = block(x_nodes_embedded,\n",
        "                            mask=self.mask,\n",
        "                            training=training)\n",
        "            # tf.print(\"x_nodes\", x_nodes.shape)\n",
        "        x_nodes = tf.squeeze( self.forward_channel(x_nodes), axis=-1 ) # (b, n+m, hidden_dims)->(b, n+m)\n",
        "        # tf.print(\"x_nodes\", x_nodes, x_nodes.shape)\n",
        "        x_nodes = self.dropout(x_nodes) if training else x_nodes\n",
        "        llr_hat = self.to_n(x_nodes) # (b, n+m)->(b,n)\n",
        "        # tf.print(\"Decoded output (llr_hat):\", llr_hat)\n",
        "        return llr_hat\n",
        "\n",
        "\n",
        "class E2EModel(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, k, n, return_infobits=False, es_no=False, is_5G=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self._n = n\n",
        "        self._k = k\n",
        "        self._m = n - k\n",
        "        self._is_5G = is_5G\n",
        "\n",
        "        self._binary_source = BinarySource(dtype=tf.int32)\n",
        "        self._num_bits_per_symbol = 2\n",
        "        self._mapper = Mapper(\"qam\", self._num_bits_per_symbol)\n",
        "        self._demapper = Demapper(\"app\", \"qam\", self._num_bits_per_symbol)\n",
        "        self._channel = AWGN()\n",
        "        self._decoder = decoder\n",
        "        self._encoder = encoder\n",
        "        self._return_infobits = return_infobits\n",
        "        self._es_no = es_no\n",
        "\n",
        "    @tf.function(jit_compile=False)\n",
        "    def call(self, batch_size, ebno_db, training=False):\n",
        "\n",
        "        # no rate-adjustment for uncoded transmission or es_no scenario\n",
        "        if self._decoder is not None and self._es_no==False:\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, self._k/self._n)\n",
        "        else: #for uncoded transmissions the rate is 1\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, 1)\n",
        "\n",
        "        b = self._binary_source([batch_size, self._k])\n",
        "        if self._encoder is not None:\n",
        "            c = self._encoder(b)\n",
        "        else:\n",
        "            c = b\n",
        "\n",
        "        # check that rate calculations are correct\n",
        "        assert self._n==c.shape[-1], \"Invalid value of n.\"\n",
        "\n",
        "        # zero padding to support odd codeword lengths\n",
        "        if self._n%2==1:\n",
        "            c_pad = tf.concat([c, tf.zeros([batch_size, 1])], axis=1)\n",
        "        else: # no padding\n",
        "            c_pad = c\n",
        "        x = self._mapper(c_pad)\n",
        "\n",
        "        y = self._channel([x, no])\n",
        "        llr = self._demapper([y, no])\n",
        "\n",
        "        # remove zero padded bit at the end\n",
        "        if self._n%2==1:\n",
        "            llr = llr[:,:-1]\n",
        "\n",
        "        # decoder input nodes\n",
        "        if not self._is_5G:\n",
        "            syndrome = tf.reshape( self._decoder.get_syndrome(llr),\n",
        "                                  (batch_size, self._m) ) # (m,n)@(n,b)->(m,b) check nodes\n",
        "            x_nodes = tf.concat([llr, syndrome], axis=1)[:, :, tf.newaxis] # (b, n+m, 1)\n",
        "        else:\n",
        "           x_nodes = llr # (b, n, 1)\n",
        "\n",
        "        # and run the decoder\n",
        "        if self._decoder is not None:\n",
        "            ############################\n",
        "            c_hat_logits = self._decoder(x_nodes, training=training)\n",
        "            ############################\n",
        "        c_hat = tf.cast(tf.greater(c_hat_logits, 0.0), tf.int32)\n",
        "\n",
        "        if self._return_infobits:\n",
        "            return b, c_hat, c_hat_logits, llr\n",
        "        else:\n",
        "            return c, c_hat, c_hat_logits, llr\n",
        "\n"
      ],
      "metadata": {
        "id": "Hbb3yTfrD-fE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Decoder5G( Decoder ):\n",
        "    def __init__(self,\n",
        "                 encoder,\n",
        "                 args,\n",
        "                 linear=False,\n",
        "                 dropout_rate=0.1,\n",
        "                 output_all_iter=False,\n",
        "                 return_infobits=False,\n",
        "                 use_bias=True,\n",
        "                 **kwargs):\n",
        "\n",
        "        self._encoder = encoder\n",
        "        self._return_infobits = return_infobits\n",
        "        self._llr_max = 20 # internal max value for LLR initialization\n",
        "        self._output_all_iter = output_all_iter\n",
        "\n",
        "        # instantiate internal decoder object to access pruned pcm\n",
        "        # Remark: this object is NOT used for decoding!\n",
        "        decoder = LDPC5GDecoder(encoder, prune_pcm=True)\n",
        "\n",
        "        # access pcm and code properties\n",
        "        self._n_pruned = decoder._n_pruned\n",
        "        self._num_pruned_nodes = decoder._nb_pruned_nodes\n",
        "        # prune and remove shortened positions\n",
        "        self._pcm, self._rm_pattern = generate_pruned_pcm_5g(decoder,\n",
        "                                                             encoder.n,\n",
        "                                                             verbose=False)\n",
        "        # precompute pruned positions\n",
        "        gather_ind = encoder.n * np.ones(np.size(self._rm_pattern))\n",
        "        gather_ind_inv = np.zeros(np.size(np.where(self._rm_pattern==1)))\n",
        "        for idx, pos in enumerate(np.where(self._rm_pattern==1)[0]):\n",
        "            gather_ind[pos] = idx\n",
        "            gather_ind_inv[idx] = pos\n",
        "\n",
        "        self._rm_ind = tf.constant(gather_ind, tf.int32)\n",
        "        self._rm_inv_ind = tf.constant(gather_ind_inv, tf.int32)\n",
        "\n",
        "        args.code.H = self._pcm\n",
        "        args.n, args.m = self._pcm.shape\n",
        "        args.k = k\n",
        "        args.n_steps = args.m + 5\n",
        "        # init Transformer decoder\n",
        "        super().__init__(args,\n",
        "                         linear=linear,\n",
        "                         dropout_rate=dropout_rate)\n",
        "\n",
        "    @property\n",
        "    def llr_max(self):\n",
        "        \"\"\"Max LLR value used for rate-matching.\"\"\"\n",
        "        return self._llr_max\n",
        "\n",
        "    @property\n",
        "    def encoder(self):\n",
        "        \"\"\"LDPC Encoder used for rate-matching/recovery.\"\"\"\n",
        "        return self._encoder\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        \"\"\"Iterative MPNN decoding function.\"\"\"\n",
        "\n",
        "        llr_ch = tf.cast(inputs, tf.float32)\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        # add punctured positions\n",
        "        # append one zero pos\n",
        "        llr_in = tf.concat([llr_ch, tf.zeros([batch_size, 1], tf.float32)],\n",
        "                           axis=1)\n",
        "        llr_rm = tf.gather(llr_in, self._rm_ind, axis=1)\n",
        "\n",
        "        syndrome_rm = tf.reshape( self.get_syndrome(llr_rm),\n",
        "                               (batch_size, self._m) )\n",
        "        x_nodes_rm = tf.concat([llr_rm, syndrome_rm], axis=1)[:, :, tf.newaxis] # (b, n+m, 1)\n",
        "\n",
        "        # and execute the decoder\n",
        "        x_hat_dec = super().call(x_nodes_rm, training=training) # (b, n)\n",
        "\n",
        "        # we need to de-ratematch for all iterations individually (for training)\n",
        "        if not self._output_all_iter:\n",
        "            x_hat_list = [x_hat_dec]\n",
        "        else:\n",
        "            x_hat_list = x_hat_dec\n",
        "\n",
        "        u_out = []\n",
        "        x_out =[]\n",
        "\n",
        "        for idx,x_hat in enumerate(x_hat_list):\n",
        "            if self._return_infobits: # return only info bits\n",
        "                # reconstruct u_hat # code is systematic\n",
        "                u_hat = tf.slice(x_hat, [0,0], [batch_size, self.encoder.k])\n",
        "                u_out.append(u_hat)\n",
        "\n",
        "            else: # return all codeword bits\n",
        "                x_short = tf.gather(x_hat, self._rm_inv_ind, axis=1)\n",
        "                x_out.append(x_short)\n",
        "\n",
        "        # return no list\n",
        "        if not self._output_all_iter:\n",
        "            if self._return_infobits:\n",
        "                return u_out[-1]\n",
        "            else:\n",
        "                return x_out[-1]\n",
        "\n",
        "        # return list of all iterations\n",
        "        if self._return_infobits:\n",
        "            return u_out\n",
        "        else:\n",
        "            return x_out\n",
        "\n",
        "\n",
        "k = 12\n",
        "n = 20\n",
        "\n",
        "encoder_5g = LDPC5GEncoder(k, n, dtype=tf.int32)\n",
        "decoder_5g = LDPC5GDecoder(encoder_5g,\n",
        "                            num_iter=10,#params[\"eval_num_iter\"],\n",
        "                            return_infobits=False,\n",
        "                            prune_pcm=True\n",
        "                            )\n",
        "\n",
        "# args for decoder\n",
        "args = Args(model_type='dis')\n",
        "\n",
        "transformer_decoder5G = Decoder5G( encoder_5g, args, linear=False) # Linear Transformer Diffusion (LTD) Decoder\n",
        "\n",
        "e2e_5G = E2EModel(encoder_5g, transformer_decoder5G, k, n, is_5G=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "oWKdQjDG8_Qv",
        "outputId": "59d2654d-23b4-40ce-8897-3d8279ca101f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: decoder uses tf.float32 for internal calculations.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtI6SibkFNWd",
        "outputId": "22044dca-5043-4b60-c6f7-a4d3e51f8cfc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@staticmethod\n",
        "def bin_to_llr(x):\n",
        "    \"\"\"\n",
        "    Converts binary values (0 or 1) to log-likelihood ratios (LLRs), clipping values to ±20 for numerical stability.\n",
        "    Args:\n",
        "        x (Tensor): Binary input tensor with values 0 or 1.\n",
        "    Returns:\n",
        "        Tensor: Tensor of LLR values with clipped range.\n",
        "    \"\"\"\n",
        "    llr_vector = tf.where(x == 0, -20, 20)\n",
        "    return llr_vector\n",
        "\n",
        "@staticmethod\n",
        "def llr_to_bin(c):\n",
        "    \"\"\"\n",
        "    Converts log-likelihood ratios (LLRs) to binary values based on their sign.\n",
        "    Args:\n",
        "        c (Tensor): Tensor of LLR values.\n",
        "    Returns:\n",
        "        Tensor: Binary tensor with values 0 or 1.\n",
        "    \"\"\"\n",
        "    return tf.cast(tf.greater(c, 0), tf.int32)\n",
        "\n",
        "def load_weights(model, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Loads the model's weights from a specified checkpoint directory.\n",
        "    Args:\n",
        "        model (tf.keras.Model): The model whose weights need to be restored.\n",
        "        checkpoint_path (str): File path where checkpoint files are stored.\n",
        "    \"\"\"\n",
        "    checkpoint = tf.train.Checkpoint(decoder=model._decoder)\n",
        "    try:\n",
        "        checkpoint.restore(checkpoint_path).assert_existing_objects_matched()\n",
        "        print(f\"Successfully restored weights from {checkpoint_path}\")\n",
        "    except AssertionError:\n",
        "        print(\"No checkpoint found. Starting training from scratch.\")\n",
        "\n",
        "def save_weights(model, checkpoint_dir):\n",
        "    \"\"\"\n",
        "    Saves the model's current weights to a specified checkpoint directory.\n",
        "    Args:\n",
        "        model (tf.keras.Model): The model whose weights need to be saved.\n",
        "        checkpoint_dir (str): Directory path where checkpoint files will be saved.\n",
        "    \"\"\"\n",
        "    checkpoint = tf.train.Checkpoint(decoder=model._decoder)\n",
        "    checkpoint.save(checkpoint_dir)\n",
        "    print(f\"Saved weights to {checkpoint_dir}\")\n",
        "\n",
        "def visualize_weights(model):\n",
        "    \"\"\"\n",
        "    Visualizes the trainable weights of the model as heatmaps, updating them in place during training.\n",
        "    Args:\n",
        "        model (tf.keras.Model): The model whose weights are visualized.\n",
        "    \"\"\"\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.ion()  # Turn on interactive mode\n",
        "    num_weights = len(model.trainable_weights)\n",
        "    fig, axs = plt.subplots(1, num_weights, figsize=(5 * num_weights, 5))\n",
        "\n",
        "    # Ensure axs is iterable (handles single axis case)\n",
        "    if num_weights == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    print(model.trainable_weights)\n",
        "    for i, var in enumerate(model.trainable_weights):\n",
        "        var_name = var.name\n",
        "        var_value = var.numpy()\n",
        "\n",
        "        axs[i].clear()  # Clear the axis to avoid overlapping\n",
        "\n",
        "        if len(var_value.shape) == 1:  # 1D tensor (e.g., bias)\n",
        "            axs[i].plot(var_value)\n",
        "            axs[i].set_title(f'{var_name} (1D)')\n",
        "        elif len(var_value.shape) == 2:  # 2D tensor (e.g., weights)\n",
        "            sns.heatmap(var_value, cmap=\"viridis\", ax=axs[i], cbar=True)\n",
        "            axs[i].set_title(f'{var_name} (2D)')\n",
        "        else:  # Higher-dimensional tensors\n",
        "            axs[i].text(0.5, 0.5, f\"{var_name}: Shape {var_value.shape} not visualizable\",\n",
        "                        ha='center', va='center', fontsize=10)\n",
        "            axs[i].set_title(f'{var_name} (Not 1D/2D)')\n",
        "\n",
        "    plt.draw()  # Update the figure with new data\n",
        "    plt.pause(0.5)  # Pause briefly to allow visualization updates\n",
        "\n",
        "    plt.ioff()  # Turn off interactive mode\n",
        "    plt.show()\n",
        "\n",
        "# SGD update iteration\n",
        "@tf.function(jit_compile=False)\n",
        "def train_step(model, loss_fn, optimizer, batch_size):\n",
        "    \"\"\"\n",
        "    Performs one training step with a batch of data, applying SGD updates to the model.\n",
        "    Args:\n",
        "        model (tf.keras.Model): The model to be trained.\n",
        "        loss_fn (tf.keras.losses.Loss): Loss function to calculate the error.\n",
        "        optimizer (tf.keras.optimizers.Optimizer): Optimizer to update the model weights.\n",
        "        batch_size (int): Number of samples in the training batch.\n",
        "    Returns:\n",
        "        Tuple: Ground truth binary labels and predicted LLR values.\n",
        "    \"\"\"\n",
        "    # train for random SNRs within a pre-defined interval\n",
        "    ebno_db = tf.random.uniform([batch_size, 1],\n",
        "                                minval=args.ebno_db_min,\n",
        "                                maxval=args.ebno_db_max)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        c, c_hat, c_hat_logits, llr_channel = model(batch_size, ebno_db, training=True)\n",
        "        # tf.print(c, c_hat)\n",
        "\n",
        "        # tf.print(\"c/c_hat\", c, c_hat_logits)\n",
        "        loss_value = loss_fn(c, c_hat_logits)\n",
        "\n",
        "    # and apply the SGD updates\n",
        "    weights = model.trainable_weights\n",
        "    grads = tape.gradient(loss_value, weights) # variables\n",
        "    optimizer.apply_gradients(zip(grads, weights))\n",
        "    return c, c_hat_logits\n",
        "\n",
        "def test_step(model, args, loss_fn, learning_rate, epoch):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a batch of data, calculating loss, bit error rate (BER), and timing.\n",
        "    Args:\n",
        "        model (tf.keras.Model): The model to be evaluated.\n",
        "        args (Namespace): Arguments containing evaluation parameters.\n",
        "        loss_fn (tf.keras.losses.Loss): Loss function to calculate the error.\n",
        "        learning_rate (float): Current learning rate of the optimizer.\n",
        "        epoch (int): Current epoch of training.\n",
        "    \"\"\"\n",
        "    ebno_db = tf.random.uniform([args.batch_size, 1],\n",
        "                                 minval=args.ebno_db_eval,\n",
        "                                 maxval=args.ebno_db_eval)\n",
        "    # measure time for call\n",
        "    time_start = time.time()\n",
        "    c, c_hat, c_hat_logits, llr_channel = model(args.batch_size, ebno_db)\n",
        "    duration = time.time() - time_start # in s\n",
        "\n",
        "    # loss\n",
        "    loss_value = loss_fn(c, c_hat_logits)\n",
        "    # ber pred\n",
        "    ber = compute_ber(c, c_hat).numpy()\n",
        "    bler = compute_bler(c, c_hat).numpy()\n",
        "    # ber original\n",
        "    c_channel = llr_to_bin(llr_channel)\n",
        "    channel_ber = compute_ber(c, c_channel).numpy()\n",
        "\n",
        "    print(f'Training epoch {epoch}/{args.epochs}, LR={learning_rate:.2e}, Loss={loss_value.numpy():.5e}, channel_BER={channel_ber}, BER={ber}, BLER={bler} duration per call: {duration:.2f}s')\n",
        "\n",
        "def train_dec(model, args, file_name, save_path='/content/drive/My Drive/ECC_weights/', load_decoder_weights=False, visualize_decoder_weights=False):\n",
        "    \"\"\"\n",
        "    Trains the model using a specified training process, evaluates periodically, and saves weights at intervals.\n",
        "    Args:\n",
        "        model (tf.keras.Model): The model to be trained.\n",
        "        args (Namespace): Training arguments including batch size, epochs, learning rate, etc.\n",
        "        file_name (str): Name of the file to save weights.\n",
        "        save_path (str): Directory path to save model checkpoints.\n",
        "        visualize_decoder_weights (bool): Whether to visualize model weights during training.\n",
        "    \"\"\"\n",
        "    # loss\n",
        "    loss_fn =  tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    # optimizer\n",
        "    scheduler = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=args.lr, decay_steps=args.epochs) # 1000 is size of trainloader\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler)\n",
        "\n",
        "    # Load weights if available\n",
        "    weights_path = os.path.join(save_path, file_name)\n",
        "    load_weights(model, weights_path) if load_decoder_weights else None\n",
        "\n",
        "    print(\"Training Model...\")\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_step(model,\n",
        "                   loss_fn,\n",
        "                   optimizer,\n",
        "                   args.batch_size)\n",
        "\n",
        "        # eval train iter\n",
        "        if epoch % args.eval_train_iter == 0:\n",
        "            test_step(model,\n",
        "                      args,\n",
        "                      loss_fn,\n",
        "                      learning_rate=optimizer.learning_rate.numpy(),\n",
        "                      epoch=epoch)\n",
        "            # break\n",
        "\n",
        "        # save weights iter\n",
        "        if epoch % args.save_weights_iter == 0:\n",
        "            save_weights(model, weights_path)\n",
        "\n",
        "        # visualize decoder weights\n",
        "        if visualize_decoder_weights:\n",
        "            visualize_weights(model)\n",
        "\n",
        "\n",
        "train_dec(e2e_5G, args, file_name='ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1', load_decoder_weights=False, visualize_decoder_weights=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn0yRQH_-ZKK",
        "outputId": "eb7b4c07-f975-4af5-aca8-c401418e444e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model...\n",
            "Training epoch 50/1000000, LR=5.00e-04, Loss=1.85264e-01, channel_BER=0.07375, BER=0.3084375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 100/1000000, LR=5.00e-04, Loss=1.74080e-01, channel_BER=0.070625, BER=0.278125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 150/1000000, LR=5.00e-04, Loss=1.82156e-01, channel_BER=0.074375, BER=0.3115625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 200/1000000, LR=5.00e-04, Loss=1.93383e-01, channel_BER=0.07875, BER=0.3065625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 250/1000000, LR=5.00e-04, Loss=1.89612e-01, channel_BER=0.069375, BER=0.4065625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 300/1000000, LR=5.00e-04, Loss=1.89045e-01, channel_BER=0.08, BER=0.3340625, BLER=1.0 duration per call: 0.02s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 350/1000000, LR=5.00e-04, Loss=1.80973e-01, channel_BER=0.06875, BER=0.4334375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 400/1000000, LR=5.00e-04, Loss=1.91657e-01, channel_BER=0.071875, BER=0.408125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 450/1000000, LR=5.00e-04, Loss=1.73000e-01, channel_BER=0.0709375, BER=0.3675, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 500/1000000, LR=5.00e-04, Loss=2.05417e-01, channel_BER=0.085625, BER=0.3246875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 550/1000000, LR=5.00e-04, Loss=1.84344e-01, channel_BER=0.0734375, BER=0.3353125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 600/1000000, LR=5.00e-04, Loss=2.44707e-01, channel_BER=0.08125, BER=0.335, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 650/1000000, LR=5.00e-04, Loss=1.83683e-01, channel_BER=0.0678125, BER=0.3175, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 700/1000000, LR=5.00e-04, Loss=1.77094e-01, channel_BER=0.07375, BER=0.3825, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 750/1000000, LR=5.00e-04, Loss=2.29328e-01, channel_BER=0.0684375, BER=0.4215625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 800/1000000, LR=5.00e-04, Loss=1.92596e-01, channel_BER=0.0709375, BER=0.27375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 850/1000000, LR=5.00e-04, Loss=1.82868e-01, channel_BER=0.06875, BER=0.3565625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 900/1000000, LR=5.00e-04, Loss=2.11638e-01, channel_BER=0.07625, BER=0.50375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 950/1000000, LR=5.00e-04, Loss=1.79443e-01, channel_BER=0.0703125, BER=0.4225, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 1000/1000000, LR=5.00e-04, Loss=1.92024e-01, channel_BER=0.0715625, BER=0.38875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 1050/1000000, LR=5.00e-04, Loss=1.94987e-01, channel_BER=0.0753125, BER=0.3684375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 1100/1000000, LR=5.00e-04, Loss=1.97928e-01, channel_BER=0.066875, BER=0.363125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 1150/1000000, LR=5.00e-04, Loss=1.95581e-01, channel_BER=0.074375, BER=0.34, BLER=1.0 duration per call: 0.02s\n",
            "Training epoch 1200/1000000, LR=5.00e-04, Loss=1.93188e-01, channel_BER=0.0796875, BER=0.325625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 1250/1000000, LR=5.00e-04, Loss=2.04526e-01, channel_BER=0.083125, BER=0.368125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 1300/1000000, LR=5.00e-04, Loss=1.82540e-01, channel_BER=0.075625, BER=0.318125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 1350/1000000, LR=5.00e-04, Loss=2.75483e-01, channel_BER=0.06625, BER=0.4640625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 1400/1000000, LR=5.00e-04, Loss=2.06094e-01, channel_BER=0.075, BER=0.2609375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 1450/1000000, LR=5.00e-04, Loss=1.76033e-01, channel_BER=0.065, BER=0.3371875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 1500/1000000, LR=5.00e-04, Loss=1.87615e-01, channel_BER=0.0696875, BER=0.3134375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 1550/1000000, LR=5.00e-04, Loss=1.79469e-01, channel_BER=0.0665625, BER=0.3034375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 1600/1000000, LR=5.00e-04, Loss=1.97443e-01, channel_BER=0.0725, BER=0.3096875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 1650/1000000, LR=5.00e-04, Loss=2.02504e-01, channel_BER=0.06875, BER=0.2846875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 1700/1000000, LR=5.00e-04, Loss=2.04475e-01, channel_BER=0.075625, BER=0.305, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 1750/1000000, LR=5.00e-04, Loss=1.96344e-01, channel_BER=0.0734375, BER=0.280625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 1800/1000000, LR=5.00e-04, Loss=1.96024e-01, channel_BER=0.07125, BER=0.4421875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 1850/1000000, LR=5.00e-04, Loss=1.78727e-01, channel_BER=0.075, BER=0.3584375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 1900/1000000, LR=5.00e-04, Loss=1.67819e-01, channel_BER=0.0621875, BER=0.3671875, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 1950/1000000, LR=5.00e-04, Loss=1.65091e-01, channel_BER=0.0640625, BER=0.33, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 2000/1000000, LR=5.00e-04, Loss=1.87032e-01, channel_BER=0.070625, BER=0.3621875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 2050/1000000, LR=5.00e-04, Loss=1.89546e-01, channel_BER=0.0759375, BER=0.3325, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 2100/1000000, LR=5.00e-04, Loss=1.94729e-01, channel_BER=0.078125, BER=0.333125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 2150/1000000, LR=5.00e-04, Loss=1.74618e-01, channel_BER=0.0703125, BER=0.3178125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 2200/1000000, LR=5.00e-04, Loss=1.74388e-01, channel_BER=0.070625, BER=0.3146875, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 2250/1000000, LR=5.00e-04, Loss=1.81218e-01, channel_BER=0.0725, BER=0.3409375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 2300/1000000, LR=5.00e-04, Loss=1.95098e-01, channel_BER=0.0746875, BER=0.3815625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 2350/1000000, LR=5.00e-04, Loss=1.79356e-01, channel_BER=0.0678125, BER=0.3840625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 2400/1000000, LR=5.00e-04, Loss=1.88896e-01, channel_BER=0.0728125, BER=0.3771875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 2450/1000000, LR=5.00e-04, Loss=1.89975e-01, channel_BER=0.078125, BER=0.3525, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 2500/1000000, LR=5.00e-04, Loss=1.77047e-01, channel_BER=0.06875, BER=0.320625, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 2550/1000000, LR=5.00e-04, Loss=1.77046e-01, channel_BER=0.0675, BER=0.32625, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 2600/1000000, LR=5.00e-04, Loss=1.83144e-01, channel_BER=0.0709375, BER=0.358125, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 2650/1000000, LR=5.00e-04, Loss=1.75309e-01, channel_BER=0.0628125, BER=0.353125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 2700/1000000, LR=5.00e-04, Loss=1.89956e-01, channel_BER=0.0771875, BER=0.3421875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 2750/1000000, LR=5.00e-04, Loss=1.88544e-01, channel_BER=0.0721875, BER=0.341875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 2800/1000000, LR=5.00e-04, Loss=1.86556e-01, channel_BER=0.0796875, BER=0.343125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 2850/1000000, LR=5.00e-04, Loss=1.81715e-01, channel_BER=0.0746875, BER=0.336875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 2900/1000000, LR=5.00e-04, Loss=2.02025e-01, channel_BER=0.0821875, BER=0.3334375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 2950/1000000, LR=5.00e-04, Loss=1.86584e-01, channel_BER=0.0703125, BER=0.31, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 3000/1000000, LR=5.00e-04, Loss=1.85618e-01, channel_BER=0.073125, BER=0.2890625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 3050/1000000, LR=5.00e-04, Loss=1.73582e-01, channel_BER=0.0703125, BER=0.301875, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 3100/1000000, LR=5.00e-04, Loss=1.82896e-01, channel_BER=0.0703125, BER=0.3221875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 3150/1000000, LR=5.00e-04, Loss=1.74068e-01, channel_BER=0.0628125, BER=0.30125, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 3200/1000000, LR=5.00e-04, Loss=1.97862e-01, channel_BER=0.0759375, BER=0.333125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 3250/1000000, LR=5.00e-04, Loss=1.95860e-01, channel_BER=0.0771875, BER=0.3703125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 3300/1000000, LR=5.00e-04, Loss=1.86058e-01, channel_BER=0.069375, BER=0.32375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 3350/1000000, LR=5.00e-04, Loss=1.89093e-01, channel_BER=0.073125, BER=0.345625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 3400/1000000, LR=5.00e-04, Loss=1.81453e-01, channel_BER=0.0715625, BER=0.325, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 3450/1000000, LR=5.00e-04, Loss=1.95348e-01, channel_BER=0.07625, BER=0.3584375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 3500/1000000, LR=5.00e-04, Loss=1.89368e-01, channel_BER=0.078125, BER=0.3509375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 3550/1000000, LR=5.00e-04, Loss=1.80702e-01, channel_BER=0.071875, BER=0.3565625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 3600/1000000, LR=5.00e-04, Loss=1.90658e-01, channel_BER=0.070625, BER=0.318125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 3650/1000000, LR=5.00e-04, Loss=1.91830e-01, channel_BER=0.0646875, BER=0.3146875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 3700/1000000, LR=5.00e-04, Loss=2.19272e-01, channel_BER=0.07375, BER=0.5146875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 3750/1000000, LR=5.00e-04, Loss=1.92432e-01, channel_BER=0.076875, BER=0.3034375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 3800/1000000, LR=5.00e-04, Loss=2.17849e-01, channel_BER=0.0753125, BER=0.274375, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 3850/1000000, LR=5.00e-04, Loss=1.87262e-01, channel_BER=0.0715625, BER=0.2890625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 3900/1000000, LR=5.00e-04, Loss=1.76828e-01, channel_BER=0.0625, BER=0.296875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 3950/1000000, LR=5.00e-04, Loss=1.79767e-01, channel_BER=0.064375, BER=0.3190625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 4000/1000000, LR=5.00e-04, Loss=1.80196e-01, channel_BER=0.07375, BER=0.3365625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 4050/1000000, LR=5.00e-04, Loss=1.91570e-01, channel_BER=0.0796875, BER=0.3228125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 4100/1000000, LR=5.00e-04, Loss=2.02037e-01, channel_BER=0.0740625, BER=0.2975, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 4150/1000000, LR=5.00e-04, Loss=1.82282e-01, channel_BER=0.07125, BER=0.2975, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 4200/1000000, LR=5.00e-04, Loss=2.02315e-01, channel_BER=0.079375, BER=0.3446875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 4250/1000000, LR=5.00e-04, Loss=1.76461e-01, channel_BER=0.0678125, BER=0.40875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 4300/1000000, LR=5.00e-04, Loss=1.67332e-01, channel_BER=0.0678125, BER=0.3715625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 4350/1000000, LR=5.00e-04, Loss=2.45676e-01, channel_BER=0.0725, BER=0.4975, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 4400/1000000, LR=5.00e-04, Loss=1.73749e-01, channel_BER=0.064375, BER=0.31875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 4450/1000000, LR=5.00e-04, Loss=2.71705e-01, channel_BER=0.07125, BER=0.49125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 4500/1000000, LR=5.00e-04, Loss=2.03467e-01, channel_BER=0.07, BER=0.3321875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 4550/1000000, LR=5.00e-04, Loss=1.77570e-01, channel_BER=0.0696875, BER=0.3221875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 4600/1000000, LR=5.00e-04, Loss=1.81448e-01, channel_BER=0.0740625, BER=0.2946875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 4650/1000000, LR=5.00e-04, Loss=1.85037e-01, channel_BER=0.0696875, BER=0.390625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 4700/1000000, LR=5.00e-04, Loss=1.97946e-01, channel_BER=0.071875, BER=0.351875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 4750/1000000, LR=5.00e-04, Loss=1.63538e-01, channel_BER=0.066875, BER=0.3459375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 4800/1000000, LR=5.00e-04, Loss=1.92190e-01, channel_BER=0.068125, BER=0.44, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 4850/1000000, LR=5.00e-04, Loss=1.88641e-01, channel_BER=0.0765625, BER=0.3825, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 4900/1000000, LR=5.00e-04, Loss=1.82114e-01, channel_BER=0.0690625, BER=0.34875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 4950/1000000, LR=5.00e-04, Loss=1.97691e-01, channel_BER=0.070625, BER=0.289375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 5000/1000000, LR=5.00e-04, Loss=1.93195e-01, channel_BER=0.0740625, BER=0.3290625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 5050/1000000, LR=5.00e-04, Loss=1.56911e-01, channel_BER=0.05625, BER=0.295, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 5100/1000000, LR=5.00e-04, Loss=1.82964e-01, channel_BER=0.0703125, BER=0.30625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 5150/1000000, LR=5.00e-04, Loss=1.92507e-01, channel_BER=0.0671875, BER=0.2965625, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 5200/1000000, LR=5.00e-04, Loss=2.17384e-01, channel_BER=0.0746875, BER=0.500625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 5250/1000000, LR=5.00e-04, Loss=1.83869e-01, channel_BER=0.071875, BER=0.46, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 5300/1000000, LR=5.00e-04, Loss=1.90341e-01, channel_BER=0.073125, BER=0.4121875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 5350/1000000, LR=5.00e-04, Loss=1.83662e-01, channel_BER=0.064375, BER=0.4665625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 5400/1000000, LR=5.00e-04, Loss=1.86460e-01, channel_BER=0.0678125, BER=0.3878125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 5450/1000000, LR=5.00e-04, Loss=1.89527e-01, channel_BER=0.0753125, BER=0.3740625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 5500/1000000, LR=5.00e-04, Loss=1.81749e-01, channel_BER=0.07125, BER=0.3303125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 5550/1000000, LR=5.00e-04, Loss=1.80810e-01, channel_BER=0.07125, BER=0.3428125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 5600/1000000, LR=5.00e-04, Loss=1.72615e-01, channel_BER=0.0671875, BER=0.311875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 5650/1000000, LR=5.00e-04, Loss=1.92954e-01, channel_BER=0.0678125, BER=0.3353125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 5700/1000000, LR=5.00e-04, Loss=1.77696e-01, channel_BER=0.065625, BER=0.3965625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 5750/1000000, LR=5.00e-04, Loss=2.38455e-01, channel_BER=0.0684375, BER=0.4953125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 5800/1000000, LR=5.00e-04, Loss=1.66578e-01, channel_BER=0.059375, BER=0.295, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 5850/1000000, LR=5.00e-04, Loss=1.90171e-01, channel_BER=0.0696875, BER=0.3059375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 5900/1000000, LR=5.00e-04, Loss=1.77604e-01, channel_BER=0.0709375, BER=0.29875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 5950/1000000, LR=5.00e-04, Loss=1.90239e-01, channel_BER=0.074375, BER=0.3740625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 6000/1000000, LR=5.00e-04, Loss=1.67943e-01, channel_BER=0.0634375, BER=0.341875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 6050/1000000, LR=5.00e-04, Loss=1.86764e-01, channel_BER=0.06625, BER=0.295625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 6100/1000000, LR=5.00e-04, Loss=1.94118e-01, channel_BER=0.0771875, BER=0.3353125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 6150/1000000, LR=5.00e-04, Loss=1.90852e-01, channel_BER=0.0659375, BER=0.473125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 6200/1000000, LR=5.00e-04, Loss=1.86931e-01, channel_BER=0.0725, BER=0.3521875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 6250/1000000, LR=5.00e-04, Loss=1.95658e-01, channel_BER=0.0759375, BER=0.331875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 6300/1000000, LR=5.00e-04, Loss=1.77334e-01, channel_BER=0.0725, BER=0.329375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 6350/1000000, LR=5.00e-04, Loss=1.68310e-01, channel_BER=0.065625, BER=0.3140625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 6400/1000000, LR=5.00e-04, Loss=1.78532e-01, channel_BER=0.0684375, BER=0.34375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 6450/1000000, LR=5.00e-04, Loss=1.69187e-01, channel_BER=0.068125, BER=0.343125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 6500/1000000, LR=5.00e-04, Loss=1.83025e-01, channel_BER=0.0709375, BER=0.361875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 6550/1000000, LR=5.00e-04, Loss=1.83232e-01, channel_BER=0.080625, BER=0.3734375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 6600/1000000, LR=5.00e-04, Loss=1.85469e-01, channel_BER=0.0628125, BER=0.3415625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 6650/1000000, LR=5.00e-04, Loss=1.85055e-01, channel_BER=0.0715625, BER=0.405, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 6700/1000000, LR=5.00e-04, Loss=1.95777e-01, channel_BER=0.075, BER=0.329375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 6750/1000000, LR=5.00e-04, Loss=2.05423e-01, channel_BER=0.0790625, BER=0.49125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 6800/1000000, LR=5.00e-04, Loss=1.74276e-01, channel_BER=0.0696875, BER=0.3471875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 6850/1000000, LR=5.00e-04, Loss=1.95036e-01, channel_BER=0.079375, BER=0.363125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 6900/1000000, LR=5.00e-04, Loss=1.77734e-01, channel_BER=0.0715625, BER=0.335625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 6950/1000000, LR=5.00e-04, Loss=1.80777e-01, channel_BER=0.07125, BER=0.3375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 7000/1000000, LR=5.00e-04, Loss=1.82938e-01, channel_BER=0.0659375, BER=0.3128125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 7050/1000000, LR=5.00e-04, Loss=2.14309e-01, channel_BER=0.0778125, BER=0.3515625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 7100/1000000, LR=5.00e-04, Loss=1.91874e-01, channel_BER=0.0771875, BER=0.398125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 7150/1000000, LR=5.00e-04, Loss=1.83324e-01, channel_BER=0.0703125, BER=0.3971875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 7200/1000000, LR=5.00e-04, Loss=1.95587e-01, channel_BER=0.0775, BER=0.3415625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 7250/1000000, LR=5.00e-04, Loss=3.46738e-01, channel_BER=0.07125, BER=0.49875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 7300/1000000, LR=5.00e-04, Loss=2.94922e-01, channel_BER=0.069375, BER=0.383125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 7350/1000000, LR=5.00e-04, Loss=2.15682e-01, channel_BER=0.0653125, BER=0.2328125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 7400/1000000, LR=5.00e-04, Loss=1.87994e-01, channel_BER=0.0703125, BER=0.2134375, BLER=0.9875 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 7450/1000000, LR=5.00e-04, Loss=1.90336e-01, channel_BER=0.0628125, BER=0.220625, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 7500/1000000, LR=5.00e-04, Loss=2.27524e-01, channel_BER=0.0778125, BER=0.2415625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 7550/1000000, LR=5.00e-04, Loss=1.92670e-01, channel_BER=0.075625, BER=0.2971875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 7600/1000000, LR=5.00e-04, Loss=1.79492e-01, channel_BER=0.06875, BER=0.3196875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 7650/1000000, LR=5.00e-04, Loss=1.85405e-01, channel_BER=0.0721875, BER=0.30125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 7700/1000000, LR=5.00e-04, Loss=1.93696e-01, channel_BER=0.0728125, BER=0.298125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 7750/1000000, LR=5.00e-04, Loss=1.75321e-01, channel_BER=0.0690625, BER=0.281875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 7800/1000000, LR=5.00e-04, Loss=1.83807e-01, channel_BER=0.0721875, BER=0.3071875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 7850/1000000, LR=5.00e-04, Loss=1.94840e-01, channel_BER=0.069375, BER=0.294375, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 7900/1000000, LR=5.00e-04, Loss=3.82989e-01, channel_BER=0.0703125, BER=0.3671875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 7950/1000000, LR=5.00e-04, Loss=3.97378e-01, channel_BER=0.068125, BER=0.4403125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 8000/1000000, LR=5.00e-04, Loss=3.63606e-01, channel_BER=0.07, BER=0.4725, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 8050/1000000, LR=5.00e-04, Loss=3.43268e-01, channel_BER=0.073125, BER=0.4659375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 8100/1000000, LR=5.00e-04, Loss=3.21131e-01, channel_BER=0.07375, BER=0.4771875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 8150/1000000, LR=5.00e-04, Loss=2.96695e-01, channel_BER=0.0759375, BER=0.4515625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 8200/1000000, LR=5.00e-04, Loss=2.73360e-01, channel_BER=0.069375, BER=0.444375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 8250/1000000, LR=5.00e-04, Loss=2.67749e-01, channel_BER=0.0659375, BER=0.4503125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 8300/1000000, LR=5.00e-04, Loss=2.64073e-01, channel_BER=0.0715625, BER=0.464375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 8350/1000000, LR=5.00e-04, Loss=2.43782e-01, channel_BER=0.0703125, BER=0.43625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 8400/1000000, LR=5.00e-04, Loss=2.45627e-01, channel_BER=0.075625, BER=0.4303125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 8450/1000000, LR=5.00e-04, Loss=2.47982e-01, channel_BER=0.0740625, BER=0.4221875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 8500/1000000, LR=5.00e-04, Loss=2.40125e-01, channel_BER=0.0671875, BER=0.446875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 8550/1000000, LR=5.00e-04, Loss=2.21183e-01, channel_BER=0.0703125, BER=0.455625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 8600/1000000, LR=5.00e-04, Loss=2.48883e-01, channel_BER=0.076875, BER=0.45125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 8650/1000000, LR=5.00e-04, Loss=2.29094e-01, channel_BER=0.0778125, BER=0.4503125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 8700/1000000, LR=5.00e-04, Loss=2.03637e-01, channel_BER=0.068125, BER=0.4675, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 8750/1000000, LR=5.00e-04, Loss=2.04081e-01, channel_BER=0.0721875, BER=0.4690625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 8800/1000000, LR=5.00e-04, Loss=2.65386e-01, channel_BER=0.0725, BER=0.5028125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 8850/1000000, LR=5.00e-04, Loss=2.39484e-01, channel_BER=0.06625, BER=0.4875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 8900/1000000, LR=5.00e-04, Loss=2.33478e-01, channel_BER=0.0696875, BER=0.451875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 8950/1000000, LR=5.00e-04, Loss=2.58711e-01, channel_BER=0.0803125, BER=0.4646875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 9000/1000000, LR=5.00e-04, Loss=2.51860e-01, channel_BER=0.081875, BER=0.4465625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 9050/1000000, LR=5.00e-04, Loss=2.07506e-01, channel_BER=0.0703125, BER=0.45625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 9100/1000000, LR=5.00e-04, Loss=1.98887e-01, channel_BER=0.0640625, BER=0.445, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 9150/1000000, LR=5.00e-04, Loss=2.01694e-01, channel_BER=0.0659375, BER=0.4475, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 9200/1000000, LR=5.00e-04, Loss=1.99226e-01, channel_BER=0.0728125, BER=0.4375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 9250/1000000, LR=5.00e-04, Loss=2.65249e-01, channel_BER=0.066875, BER=0.475, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 9300/1000000, LR=5.00e-04, Loss=2.35558e-01, channel_BER=0.081875, BER=0.476875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 9350/1000000, LR=5.00e-04, Loss=1.99220e-01, channel_BER=0.0665625, BER=0.4546875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 9400/1000000, LR=5.00e-04, Loss=1.82910e-01, channel_BER=0.0709375, BER=0.40625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 9450/1000000, LR=5.00e-04, Loss=1.91263e-01, channel_BER=0.0690625, BER=0.4425, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 9500/1000000, LR=5.00e-04, Loss=2.06993e-01, channel_BER=0.08, BER=0.4571875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 9550/1000000, LR=5.00e-04, Loss=4.48015e-01, channel_BER=0.0759375, BER=0.5128125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 9600/1000000, LR=5.00e-04, Loss=3.89796e-01, channel_BER=0.0675, BER=0.495, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 9650/1000000, LR=5.00e-04, Loss=3.75153e-01, channel_BER=0.07, BER=0.49, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 9700/1000000, LR=5.00e-04, Loss=3.24963e-01, channel_BER=0.07125, BER=0.508125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 9750/1000000, LR=5.00e-04, Loss=2.96507e-01, channel_BER=0.0740625, BER=0.49, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 9800/1000000, LR=5.00e-04, Loss=2.64317e-01, channel_BER=0.0725, BER=0.489375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 9850/1000000, LR=5.00e-04, Loss=2.65539e-01, channel_BER=0.07125, BER=0.505, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 9900/1000000, LR=5.00e-04, Loss=2.56787e-01, channel_BER=0.07625, BER=0.455625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 9950/1000000, LR=5.00e-04, Loss=2.07944e-01, channel_BER=0.06625, BER=0.5040625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 10000/1000000, LR=5.00e-04, Loss=2.25842e-01, channel_BER=0.0765625, BER=0.4746875, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 10050/1000000, LR=5.00e-04, Loss=2.05784e-01, channel_BER=0.065, BER=0.5028125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 10100/1000000, LR=5.00e-04, Loss=2.43492e-01, channel_BER=0.0728125, BER=0.47, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 10150/1000000, LR=5.00e-04, Loss=2.19201e-01, channel_BER=0.0721875, BER=0.4878125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 10200/1000000, LR=5.00e-04, Loss=2.18344e-01, channel_BER=0.0746875, BER=0.501875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 10250/1000000, LR=5.00e-04, Loss=2.01889e-01, channel_BER=0.0659375, BER=0.4621875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 10300/1000000, LR=5.00e-04, Loss=2.19106e-01, channel_BER=0.064375, BER=0.4640625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 10350/1000000, LR=5.00e-04, Loss=2.06164e-01, channel_BER=0.0646875, BER=0.435625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 10400/1000000, LR=5.00e-04, Loss=2.02842e-01, channel_BER=0.073125, BER=0.4471875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 10450/1000000, LR=5.00e-04, Loss=2.33071e-01, channel_BER=0.075, BER=0.4259375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 10500/1000000, LR=5.00e-04, Loss=2.17546e-01, channel_BER=0.07, BER=0.464375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 10550/1000000, LR=5.00e-04, Loss=2.11176e-01, channel_BER=0.0765625, BER=0.440625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 10600/1000000, LR=5.00e-04, Loss=2.01472e-01, channel_BER=0.070625, BER=0.4440625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 10650/1000000, LR=5.00e-04, Loss=2.17025e-01, channel_BER=0.0775, BER=0.386875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 10700/1000000, LR=5.00e-04, Loss=2.23027e-01, channel_BER=0.076875, BER=0.3821875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 10750/1000000, LR=5.00e-04, Loss=2.06431e-01, channel_BER=0.069375, BER=0.3515625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 10800/1000000, LR=5.00e-04, Loss=2.30116e-01, channel_BER=0.0765625, BER=0.4446875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 10850/1000000, LR=5.00e-04, Loss=2.17564e-01, channel_BER=0.07375, BER=0.4428125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 10900/1000000, LR=5.00e-04, Loss=1.98813e-01, channel_BER=0.0690625, BER=0.423125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 10950/1000000, LR=5.00e-04, Loss=2.37712e-01, channel_BER=0.079375, BER=0.4784375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 11000/1000000, LR=5.00e-04, Loss=2.23361e-01, channel_BER=0.068125, BER=0.45625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 11050/1000000, LR=5.00e-04, Loss=2.16564e-01, channel_BER=0.075, BER=0.4, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 11100/1000000, LR=5.00e-04, Loss=1.94930e-01, channel_BER=0.066875, BER=0.388125, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 11150/1000000, LR=5.00e-04, Loss=2.07382e-01, channel_BER=0.071875, BER=0.3978125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 11200/1000000, LR=5.00e-04, Loss=2.07902e-01, channel_BER=0.0709375, BER=0.4303125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 11250/1000000, LR=5.00e-04, Loss=2.04690e-01, channel_BER=0.074375, BER=0.4190625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 11300/1000000, LR=5.00e-04, Loss=1.95801e-01, channel_BER=0.0721875, BER=0.3840625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 11350/1000000, LR=5.00e-04, Loss=2.25400e-01, channel_BER=0.070625, BER=0.41375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 11400/1000000, LR=5.00e-04, Loss=2.12793e-01, channel_BER=0.0715625, BER=0.3875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 11450/1000000, LR=5.00e-04, Loss=2.01686e-01, channel_BER=0.0725, BER=0.44875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 11500/1000000, LR=5.00e-04, Loss=1.90217e-01, channel_BER=0.0671875, BER=0.4415625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 11550/1000000, LR=5.00e-04, Loss=2.06693e-01, channel_BER=0.0753125, BER=0.4028125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 11600/1000000, LR=5.00e-04, Loss=2.06122e-01, channel_BER=0.066875, BER=0.38625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 11650/1000000, LR=5.00e-04, Loss=2.09027e-01, channel_BER=0.06875, BER=0.3578125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 11700/1000000, LR=5.00e-04, Loss=2.23290e-01, channel_BER=0.08, BER=0.3640625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 11750/1000000, LR=5.00e-04, Loss=2.08873e-01, channel_BER=0.0625, BER=0.5003125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 11800/1000000, LR=5.00e-04, Loss=2.13878e-01, channel_BER=0.07875, BER=0.4296875, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 11850/1000000, LR=5.00e-04, Loss=1.95410e-01, channel_BER=0.0671875, BER=0.4403125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 11900/1000000, LR=5.00e-04, Loss=2.19108e-01, channel_BER=0.07125, BER=0.453125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 11950/1000000, LR=5.00e-04, Loss=2.16030e-01, channel_BER=0.07375, BER=0.384375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 12000/1000000, LR=5.00e-04, Loss=2.18864e-01, channel_BER=0.0771875, BER=0.4625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 12050/1000000, LR=5.00e-04, Loss=1.93056e-01, channel_BER=0.06875, BER=0.4678125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 12100/1000000, LR=5.00e-04, Loss=1.97852e-01, channel_BER=0.074375, BER=0.470625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 12150/1000000, LR=5.00e-04, Loss=2.05859e-01, channel_BER=0.069375, BER=0.446875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 12200/1000000, LR=5.00e-04, Loss=1.96444e-01, channel_BER=0.070625, BER=0.4453125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 12250/1000000, LR=5.00e-04, Loss=1.97302e-01, channel_BER=0.0671875, BER=0.430625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 12300/1000000, LR=5.00e-04, Loss=2.03792e-01, channel_BER=0.073125, BER=0.3890625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 12350/1000000, LR=5.00e-04, Loss=2.02364e-01, channel_BER=0.0615625, BER=0.3584375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 12400/1000000, LR=5.00e-04, Loss=2.07028e-01, channel_BER=0.0709375, BER=0.39125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 12450/1000000, LR=5.00e-04, Loss=2.30207e-01, channel_BER=0.0728125, BER=0.346875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 12500/1000000, LR=5.00e-04, Loss=2.03412e-01, channel_BER=0.0675, BER=0.4053125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 12550/1000000, LR=5.00e-04, Loss=2.16656e-01, channel_BER=0.0703125, BER=0.3853125, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 12600/1000000, LR=5.00e-04, Loss=2.12878e-01, channel_BER=0.0784375, BER=0.41, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 12650/1000000, LR=5.00e-04, Loss=2.07416e-01, channel_BER=0.0734375, BER=0.509375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 12700/1000000, LR=5.00e-04, Loss=2.27697e-01, channel_BER=0.0709375, BER=0.3784375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 12750/1000000, LR=5.00e-04, Loss=2.03587e-01, channel_BER=0.0715625, BER=0.368125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 12800/1000000, LR=5.00e-04, Loss=1.79051e-01, channel_BER=0.065625, BER=0.3209375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 12850/1000000, LR=5.00e-04, Loss=1.93013e-01, channel_BER=0.074375, BER=0.393125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 12900/1000000, LR=5.00e-04, Loss=1.90205e-01, channel_BER=0.0728125, BER=0.345625, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 12950/1000000, LR=5.00e-04, Loss=1.99465e-01, channel_BER=0.0675, BER=0.5021875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 13000/1000000, LR=5.00e-04, Loss=2.24002e-01, channel_BER=0.0803125, BER=0.465, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 13050/1000000, LR=5.00e-04, Loss=2.07211e-01, channel_BER=0.0778125, BER=0.395625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 13100/1000000, LR=5.00e-04, Loss=1.77871e-01, channel_BER=0.0621875, BER=0.334375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 13150/1000000, LR=5.00e-04, Loss=1.91615e-01, channel_BER=0.0715625, BER=0.3378125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 13200/1000000, LR=5.00e-04, Loss=1.78149e-01, channel_BER=0.0665625, BER=0.3584375, BLER=0.9875 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 13250/1000000, LR=5.00e-04, Loss=2.21455e-01, channel_BER=0.0746875, BER=0.3578125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 13300/1000000, LR=5.00e-04, Loss=1.80062e-01, channel_BER=0.060625, BER=0.3140625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 13350/1000000, LR=5.00e-04, Loss=1.84462e-01, channel_BER=0.075, BER=0.3253125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 13400/1000000, LR=5.00e-04, Loss=2.08577e-01, channel_BER=0.0740625, BER=0.3190625, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 13450/1000000, LR=5.00e-04, Loss=2.05909e-01, channel_BER=0.07625, BER=0.3290625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 13500/1000000, LR=5.00e-04, Loss=1.95811e-01, channel_BER=0.0753125, BER=0.3075, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 13550/1000000, LR=5.00e-04, Loss=1.87095e-01, channel_BER=0.071875, BER=0.425, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 13600/1000000, LR=5.00e-04, Loss=2.09498e-01, channel_BER=0.07875, BER=0.346875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 13650/1000000, LR=5.00e-04, Loss=1.86564e-01, channel_BER=0.069375, BER=0.42375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 13700/1000000, LR=5.00e-04, Loss=1.91535e-01, channel_BER=0.070625, BER=0.4665625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 13750/1000000, LR=5.00e-04, Loss=1.83013e-01, channel_BER=0.0725, BER=0.35125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 13800/1000000, LR=5.00e-04, Loss=1.93340e-01, channel_BER=0.0725, BER=0.336875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 13850/1000000, LR=5.00e-04, Loss=1.90148e-01, channel_BER=0.071875, BER=0.3475, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 13900/1000000, LR=5.00e-04, Loss=1.85404e-01, channel_BER=0.068125, BER=0.3665625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 13950/1000000, LR=5.00e-04, Loss=1.87582e-01, channel_BER=0.075, BER=0.2896875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 14000/1000000, LR=5.00e-04, Loss=2.07442e-01, channel_BER=0.079375, BER=0.44375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 14050/1000000, LR=5.00e-04, Loss=1.79119e-01, channel_BER=0.065625, BER=0.3459375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 14100/1000000, LR=5.00e-04, Loss=2.02327e-01, channel_BER=0.0740625, BER=0.334375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 14150/1000000, LR=5.00e-04, Loss=1.89924e-01, channel_BER=0.06625, BER=0.5025, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 14200/1000000, LR=5.00e-04, Loss=1.89759e-01, channel_BER=0.0715625, BER=0.3715625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 14250/1000000, LR=5.00e-04, Loss=2.21715e-01, channel_BER=0.0728125, BER=0.3046875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 14300/1000000, LR=5.00e-04, Loss=1.71329e-01, channel_BER=0.065625, BER=0.4215625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 14350/1000000, LR=5.00e-04, Loss=1.83411e-01, channel_BER=0.076875, BER=0.3190625, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 14400/1000000, LR=5.00e-04, Loss=2.00832e-01, channel_BER=0.0728125, BER=0.3228125, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 14450/1000000, LR=5.00e-04, Loss=2.01520e-01, channel_BER=0.0725, BER=0.355625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 14500/1000000, LR=5.00e-04, Loss=1.93596e-01, channel_BER=0.0715625, BER=0.3409375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 14550/1000000, LR=5.00e-04, Loss=1.98423e-01, channel_BER=0.0753125, BER=0.288125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 14600/1000000, LR=5.00e-04, Loss=1.92719e-01, channel_BER=0.0728125, BER=0.3284375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 14650/1000000, LR=5.00e-04, Loss=1.90714e-01, channel_BER=0.0671875, BER=0.335625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 14700/1000000, LR=5.00e-04, Loss=1.98020e-01, channel_BER=0.0753125, BER=0.385625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 14750/1000000, LR=5.00e-04, Loss=1.87051e-01, channel_BER=0.0728125, BER=0.33, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 14800/1000000, LR=5.00e-04, Loss=1.86038e-01, channel_BER=0.0715625, BER=0.3684375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 14850/1000000, LR=5.00e-04, Loss=3.06910e-01, channel_BER=0.0721875, BER=0.4771875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 14900/1000000, LR=5.00e-04, Loss=2.44835e-01, channel_BER=0.0815625, BER=0.3853125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 14950/1000000, LR=5.00e-04, Loss=1.90011e-01, channel_BER=0.0721875, BER=0.433125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 15000/1000000, LR=5.00e-04, Loss=1.83592e-01, channel_BER=0.0715625, BER=0.39375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 15050/1000000, LR=5.00e-04, Loss=1.85569e-01, channel_BER=0.0696875, BER=0.491875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 15100/1000000, LR=5.00e-04, Loss=2.06582e-01, channel_BER=0.066875, BER=0.300625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 15150/1000000, LR=5.00e-04, Loss=1.90361e-01, channel_BER=0.0753125, BER=0.39875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 15200/1000000, LR=5.00e-04, Loss=1.91608e-01, channel_BER=0.06875, BER=0.3065625, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 15250/1000000, LR=5.00e-04, Loss=1.92922e-01, channel_BER=0.074375, BER=0.3178125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 15300/1000000, LR=5.00e-04, Loss=2.32109e-01, channel_BER=0.083125, BER=0.3203125, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 15350/1000000, LR=5.00e-04, Loss=1.91224e-01, channel_BER=0.070625, BER=0.32375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 15400/1000000, LR=5.00e-04, Loss=1.97956e-01, channel_BER=0.0746875, BER=0.4525, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 15450/1000000, LR=5.00e-04, Loss=2.24111e-01, channel_BER=0.0821875, BER=0.3, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 15500/1000000, LR=5.00e-04, Loss=2.09761e-01, channel_BER=0.08, BER=0.356875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 15550/1000000, LR=5.00e-04, Loss=1.80760e-01, channel_BER=0.0703125, BER=0.288125, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 15600/1000000, LR=5.00e-04, Loss=1.89548e-01, channel_BER=0.07, BER=0.3034375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 15650/1000000, LR=5.00e-04, Loss=2.03925e-01, channel_BER=0.074375, BER=0.4621875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 15700/1000000, LR=5.00e-04, Loss=1.78508e-01, channel_BER=0.0684375, BER=0.4465625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 15750/1000000, LR=5.00e-04, Loss=1.76177e-01, channel_BER=0.0665625, BER=0.4225, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 15800/1000000, LR=5.00e-04, Loss=1.94738e-01, channel_BER=0.0753125, BER=0.32875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 15850/1000000, LR=5.00e-04, Loss=1.97486e-01, channel_BER=0.074375, BER=0.3578125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 15900/1000000, LR=5.00e-04, Loss=1.86998e-01, channel_BER=0.07125, BER=0.32875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 15950/1000000, LR=5.00e-04, Loss=1.90125e-01, channel_BER=0.06875, BER=0.3403125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 16000/1000000, LR=5.00e-04, Loss=1.77546e-01, channel_BER=0.0615625, BER=0.4596875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 16050/1000000, LR=5.00e-04, Loss=1.97927e-01, channel_BER=0.0734375, BER=0.2959375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 16100/1000000, LR=5.00e-04, Loss=2.11548e-01, channel_BER=0.079375, BER=0.3259375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 16150/1000000, LR=5.00e-04, Loss=1.80108e-01, channel_BER=0.066875, BER=0.273125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 16200/1000000, LR=5.00e-04, Loss=1.88288e-01, channel_BER=0.0703125, BER=0.3378125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 16250/1000000, LR=5.00e-04, Loss=1.78383e-01, channel_BER=0.068125, BER=0.28375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 16300/1000000, LR=5.00e-04, Loss=2.06699e-01, channel_BER=0.070625, BER=0.3453125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 16350/1000000, LR=5.00e-04, Loss=1.92347e-01, channel_BER=0.0740625, BER=0.304375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 16400/1000000, LR=5.00e-04, Loss=1.81086e-01, channel_BER=0.0721875, BER=0.321875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 16450/1000000, LR=5.00e-04, Loss=2.21217e-01, channel_BER=0.0821875, BER=0.2809375, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 16500/1000000, LR=5.00e-04, Loss=1.97125e-01, channel_BER=0.0725, BER=0.2746875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 16550/1000000, LR=5.00e-04, Loss=1.87158e-01, channel_BER=0.0725, BER=0.2971875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 16600/1000000, LR=5.00e-04, Loss=2.02378e-01, channel_BER=0.076875, BER=0.3303125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 16650/1000000, LR=5.00e-04, Loss=2.07832e-01, channel_BER=0.07875, BER=0.3228125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 16700/1000000, LR=5.00e-04, Loss=1.81954e-01, channel_BER=0.0675, BER=0.3690625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 16750/1000000, LR=5.00e-04, Loss=2.05122e-01, channel_BER=0.0775, BER=0.2953125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 16800/1000000, LR=5.00e-04, Loss=1.94787e-01, channel_BER=0.0753125, BER=0.325, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 16850/1000000, LR=5.00e-04, Loss=2.35095e-01, channel_BER=0.071875, BER=0.498125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 16900/1000000, LR=5.00e-04, Loss=2.15564e-01, channel_BER=0.07375, BER=0.3225, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 16950/1000000, LR=5.00e-04, Loss=1.89839e-01, channel_BER=0.063125, BER=0.3246875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 17000/1000000, LR=5.00e-04, Loss=1.91576e-01, channel_BER=0.07375, BER=0.3034375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 17050/1000000, LR=5.00e-04, Loss=3.36278e-01, channel_BER=0.0740625, BER=0.4959375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 17100/1000000, LR=5.00e-04, Loss=3.07852e-01, channel_BER=0.074375, BER=0.496875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 17150/1000000, LR=5.00e-04, Loss=2.22568e-01, channel_BER=0.076875, BER=0.5078125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 17200/1000000, LR=5.00e-04, Loss=2.31580e-01, channel_BER=0.0753125, BER=0.4034375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 17250/1000000, LR=5.00e-04, Loss=1.82324e-01, channel_BER=0.064375, BER=0.4378125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 17300/1000000, LR=5.00e-04, Loss=1.94505e-01, channel_BER=0.060625, BER=0.4865625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 17350/1000000, LR=5.00e-04, Loss=2.07893e-01, channel_BER=0.076875, BER=0.3790625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 17400/1000000, LR=5.00e-04, Loss=1.84394e-01, channel_BER=0.06875, BER=0.455625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 17450/1000000, LR=5.00e-04, Loss=1.97308e-01, channel_BER=0.0775, BER=0.3725, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 17500/1000000, LR=5.00e-04, Loss=2.02333e-01, channel_BER=0.0771875, BER=0.385, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 17550/1000000, LR=5.00e-04, Loss=2.43979e-01, channel_BER=0.0803125, BER=0.4909375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 17600/1000000, LR=5.00e-04, Loss=1.95564e-01, channel_BER=0.071875, BER=0.4434375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 17650/1000000, LR=5.00e-04, Loss=1.84646e-01, channel_BER=0.071875, BER=0.3303125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 17700/1000000, LR=5.00e-04, Loss=2.10238e-01, channel_BER=0.0778125, BER=0.3584375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 17750/1000000, LR=5.00e-04, Loss=2.06669e-01, channel_BER=0.071875, BER=0.33625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 17800/1000000, LR=5.00e-04, Loss=1.82085e-01, channel_BER=0.0621875, BER=0.3475, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 17850/1000000, LR=5.00e-04, Loss=1.95558e-01, channel_BER=0.0721875, BER=0.3415625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 17900/1000000, LR=5.00e-04, Loss=2.12061e-01, channel_BER=0.081875, BER=0.350625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 17950/1000000, LR=5.00e-04, Loss=2.12101e-01, channel_BER=0.0809375, BER=0.413125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 18000/1000000, LR=5.00e-04, Loss=4.00560e-01, channel_BER=0.071875, BER=0.47125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 18050/1000000, LR=5.00e-04, Loss=3.48012e-01, channel_BER=0.0715625, BER=0.4909375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 18100/1000000, LR=5.00e-04, Loss=2.92131e-01, channel_BER=0.0709375, BER=0.47, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 18150/1000000, LR=5.00e-04, Loss=2.07025e-01, channel_BER=0.0653125, BER=0.458125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 18200/1000000, LR=5.00e-04, Loss=1.91625e-01, channel_BER=0.0746875, BER=0.36875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 18250/1000000, LR=5.00e-04, Loss=2.16356e-01, channel_BER=0.06875, BER=0.403125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 18300/1000000, LR=5.00e-04, Loss=2.20036e-01, channel_BER=0.0728125, BER=0.431875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 18350/1000000, LR=5.00e-04, Loss=1.93821e-01, channel_BER=0.0725, BER=0.3753125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 18400/1000000, LR=5.00e-04, Loss=2.04759e-01, channel_BER=0.0778125, BER=0.424375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 18450/1000000, LR=5.00e-04, Loss=1.92217e-01, channel_BER=0.07, BER=0.406875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 18500/1000000, LR=5.00e-04, Loss=1.95992e-01, channel_BER=0.0725, BER=0.413125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 18550/1000000, LR=5.00e-04, Loss=1.93431e-01, channel_BER=0.0753125, BER=0.379375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 18600/1000000, LR=5.00e-04, Loss=2.07151e-01, channel_BER=0.0696875, BER=0.465, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 18650/1000000, LR=5.00e-04, Loss=1.94327e-01, channel_BER=0.0759375, BER=0.4496875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 18700/1000000, LR=5.00e-04, Loss=2.45806e-01, channel_BER=0.071875, BER=0.4678125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 18750/1000000, LR=5.00e-04, Loss=2.03693e-01, channel_BER=0.074375, BER=0.391875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 18800/1000000, LR=5.00e-04, Loss=1.82473e-01, channel_BER=0.070625, BER=0.36875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 18850/1000000, LR=5.00e-04, Loss=2.08308e-01, channel_BER=0.0753125, BER=0.3996875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 18900/1000000, LR=5.00e-04, Loss=1.86132e-01, channel_BER=0.069375, BER=0.2765625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 18950/1000000, LR=5.00e-04, Loss=2.01416e-01, channel_BER=0.07625, BER=0.408125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 19000/1000000, LR=5.00e-04, Loss=1.71159e-01, channel_BER=0.065, BER=0.3896875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 19050/1000000, LR=5.00e-04, Loss=1.75130e-01, channel_BER=0.0728125, BER=0.3984375, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 19100/1000000, LR=5.00e-04, Loss=1.91480e-01, channel_BER=0.071875, BER=0.394375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 19150/1000000, LR=5.00e-04, Loss=1.81857e-01, channel_BER=0.0690625, BER=0.441875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 19200/1000000, LR=5.00e-04, Loss=1.96065e-01, channel_BER=0.0765625, BER=0.4209375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 19250/1000000, LR=5.00e-04, Loss=1.83836e-01, channel_BER=0.07125, BER=0.34375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 19300/1000000, LR=5.00e-04, Loss=1.84105e-01, channel_BER=0.0778125, BER=0.319375, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 19350/1000000, LR=5.00e-04, Loss=2.15347e-01, channel_BER=0.0803125, BER=0.37875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 19400/1000000, LR=5.00e-04, Loss=1.81515e-01, channel_BER=0.074375, BER=0.38, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 19450/1000000, LR=5.00e-04, Loss=2.50773e-01, channel_BER=0.0746875, BER=0.5040625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 19500/1000000, LR=5.00e-04, Loss=2.35205e-01, channel_BER=0.08, BER=0.504375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 19550/1000000, LR=5.00e-04, Loss=1.88752e-01, channel_BER=0.0625, BER=0.4778125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 19600/1000000, LR=5.00e-04, Loss=1.87023e-01, channel_BER=0.07375, BER=0.369375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 19650/1000000, LR=5.00e-04, Loss=2.09814e-01, channel_BER=0.08125, BER=0.4353125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 19700/1000000, LR=5.00e-04, Loss=1.91941e-01, channel_BER=0.0703125, BER=0.3853125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 19750/1000000, LR=5.00e-04, Loss=1.94647e-01, channel_BER=0.075, BER=0.370625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 19800/1000000, LR=5.00e-04, Loss=1.93803e-01, channel_BER=0.0621875, BER=0.370625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 19850/1000000, LR=5.00e-04, Loss=1.83455e-01, channel_BER=0.0696875, BER=0.319375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 19900/1000000, LR=5.00e-04, Loss=1.80797e-01, channel_BER=0.07, BER=0.3446875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 19950/1000000, LR=5.00e-04, Loss=1.99362e-01, channel_BER=0.0721875, BER=0.316875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 20000/1000000, LR=5.00e-04, Loss=1.94793e-01, channel_BER=0.075625, BER=0.445, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 20050/1000000, LR=5.00e-04, Loss=2.66109e-01, channel_BER=0.0746875, BER=0.5153125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 20100/1000000, LR=5.00e-04, Loss=2.14889e-01, channel_BER=0.0746875, BER=0.4925, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 20150/1000000, LR=4.99e-04, Loss=1.85962e-01, channel_BER=0.0696875, BER=0.4684375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 20200/1000000, LR=4.99e-04, Loss=2.12740e-01, channel_BER=0.07625, BER=0.3415625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 20250/1000000, LR=4.99e-04, Loss=1.94176e-01, channel_BER=0.075625, BER=0.4525, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 20300/1000000, LR=4.99e-04, Loss=1.89858e-01, channel_BER=0.0715625, BER=0.3803125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 20350/1000000, LR=4.99e-04, Loss=2.02645e-01, channel_BER=0.0728125, BER=0.4084375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 20400/1000000, LR=4.99e-04, Loss=1.91789e-01, channel_BER=0.0734375, BER=0.3484375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 20450/1000000, LR=4.99e-04, Loss=2.04498e-01, channel_BER=0.08125, BER=0.32875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 20500/1000000, LR=4.99e-04, Loss=1.85098e-01, channel_BER=0.0753125, BER=0.36875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 20550/1000000, LR=4.99e-04, Loss=2.15961e-01, channel_BER=0.0640625, BER=0.3646875, BLER=0.99375 duration per call: 0.01s\n",
            "Training epoch 20600/1000000, LR=4.99e-04, Loss=2.08604e-01, channel_BER=0.08, BER=0.3578125, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 20650/1000000, LR=4.99e-04, Loss=1.78676e-01, channel_BER=0.0703125, BER=0.3525, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 20700/1000000, LR=4.99e-04, Loss=2.09690e-01, channel_BER=0.0778125, BER=0.39, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 20750/1000000, LR=4.99e-04, Loss=1.96208e-01, channel_BER=0.0778125, BER=0.4515625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 20800/1000000, LR=4.99e-04, Loss=2.16780e-01, channel_BER=0.0778125, BER=0.5, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 20850/1000000, LR=4.99e-04, Loss=2.08470e-01, channel_BER=0.0825, BER=0.4925, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 20900/1000000, LR=4.99e-04, Loss=1.87315e-01, channel_BER=0.0678125, BER=0.4009375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 20950/1000000, LR=4.99e-04, Loss=1.86541e-01, channel_BER=0.0771875, BER=0.4528125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 21000/1000000, LR=4.99e-04, Loss=1.79847e-01, channel_BER=0.07125, BER=0.389375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 21050/1000000, LR=4.99e-04, Loss=2.02394e-01, channel_BER=0.0815625, BER=0.359375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 21100/1000000, LR=4.99e-04, Loss=1.76162e-01, channel_BER=0.0709375, BER=0.373125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 21150/1000000, LR=4.99e-04, Loss=1.84450e-01, channel_BER=0.0684375, BER=0.361875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 21200/1000000, LR=4.99e-04, Loss=1.83668e-01, channel_BER=0.0690625, BER=0.3609375, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 21250/1000000, LR=4.99e-04, Loss=1.89238e-01, channel_BER=0.0671875, BER=0.335625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 21300/1000000, LR=4.99e-04, Loss=2.00854e-01, channel_BER=0.078125, BER=0.30875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 21350/1000000, LR=4.99e-04, Loss=1.97992e-01, channel_BER=0.060625, BER=0.4925, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 21400/1000000, LR=4.99e-04, Loss=2.04174e-01, channel_BER=0.0753125, BER=0.381875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 21450/1000000, LR=4.99e-04, Loss=1.79447e-01, channel_BER=0.06375, BER=0.3771875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 21500/1000000, LR=4.99e-04, Loss=2.09878e-01, channel_BER=0.0771875, BER=0.49625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 21550/1000000, LR=4.99e-04, Loss=1.90386e-01, channel_BER=0.065625, BER=0.4959375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 21600/1000000, LR=4.99e-04, Loss=1.95529e-01, channel_BER=0.079375, BER=0.393125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 21650/1000000, LR=4.99e-04, Loss=1.83754e-01, channel_BER=0.07125, BER=0.366875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 21700/1000000, LR=4.99e-04, Loss=1.94187e-01, channel_BER=0.0709375, BER=0.364375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 21750/1000000, LR=4.99e-04, Loss=1.92245e-01, channel_BER=0.0746875, BER=0.4053125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 21800/1000000, LR=4.99e-04, Loss=1.90836e-01, channel_BER=0.074375, BER=0.3296875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 21850/1000000, LR=4.99e-04, Loss=1.83040e-01, channel_BER=0.0684375, BER=0.3809375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 21900/1000000, LR=4.99e-04, Loss=2.01206e-01, channel_BER=0.075625, BER=0.356875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 21950/1000000, LR=4.99e-04, Loss=2.08632e-01, channel_BER=0.0728125, BER=0.5128125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 22000/1000000, LR=4.99e-04, Loss=1.89740e-01, channel_BER=0.0734375, BER=0.4165625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 22050/1000000, LR=4.99e-04, Loss=1.96713e-01, channel_BER=0.0784375, BER=0.4340625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 22100/1000000, LR=4.99e-04, Loss=1.85950e-01, channel_BER=0.07375, BER=0.38875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 22150/1000000, LR=4.99e-04, Loss=1.97375e-01, channel_BER=0.076875, BER=0.3725, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 22200/1000000, LR=4.99e-04, Loss=1.81580e-01, channel_BER=0.0721875, BER=0.388125, BLER=0.99375 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 22250/1000000, LR=4.99e-04, Loss=2.01276e-01, channel_BER=0.0765625, BER=0.371875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 22300/1000000, LR=4.99e-04, Loss=1.92404e-01, channel_BER=0.0734375, BER=0.3596875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 22350/1000000, LR=4.99e-04, Loss=1.84975e-01, channel_BER=0.076875, BER=0.4028125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 22400/1000000, LR=4.99e-04, Loss=1.78455e-01, channel_BER=0.07125, BER=0.3415625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 22450/1000000, LR=4.99e-04, Loss=1.88528e-01, channel_BER=0.068125, BER=0.3740625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 22500/1000000, LR=4.99e-04, Loss=1.89000e-01, channel_BER=0.07, BER=0.4125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 22550/1000000, LR=4.99e-04, Loss=1.73356e-01, channel_BER=0.065, BER=0.34, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 22600/1000000, LR=4.99e-04, Loss=2.09493e-01, channel_BER=0.0825, BER=0.3528125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 22650/1000000, LR=4.99e-04, Loss=1.89948e-01, channel_BER=0.07125, BER=0.37125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 22700/1000000, LR=4.99e-04, Loss=1.86924e-01, channel_BER=0.0721875, BER=0.364375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 22750/1000000, LR=4.99e-04, Loss=1.78168e-01, channel_BER=0.070625, BER=0.444375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 22800/1000000, LR=4.99e-04, Loss=1.93380e-01, channel_BER=0.0734375, BER=0.3503125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 22850/1000000, LR=4.99e-04, Loss=2.09894e-01, channel_BER=0.0659375, BER=0.45875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 22900/1000000, LR=4.99e-04, Loss=1.86356e-01, channel_BER=0.069375, BER=0.393125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 22950/1000000, LR=4.99e-04, Loss=1.78706e-01, channel_BER=0.0715625, BER=0.3684375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 23000/1000000, LR=4.99e-04, Loss=2.17794e-01, channel_BER=0.0771875, BER=0.451875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 23050/1000000, LR=4.99e-04, Loss=1.98333e-01, channel_BER=0.0728125, BER=0.5115625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 23100/1000000, LR=4.99e-04, Loss=2.03311e-01, channel_BER=0.0803125, BER=0.3903125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 23150/1000000, LR=4.99e-04, Loss=2.44263e-01, channel_BER=0.084375, BER=0.4996875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 23200/1000000, LR=4.99e-04, Loss=2.74591e-01, channel_BER=0.0665625, BER=0.4825, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 23250/1000000, LR=4.99e-04, Loss=2.22582e-01, channel_BER=0.0734375, BER=0.4903125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 23300/1000000, LR=4.99e-04, Loss=1.85166e-01, channel_BER=0.0709375, BER=0.3834375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 23350/1000000, LR=4.99e-04, Loss=2.07413e-01, channel_BER=0.075625, BER=0.34625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 23400/1000000, LR=4.99e-04, Loss=1.96170e-01, channel_BER=0.0725, BER=0.39125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 23450/1000000, LR=4.99e-04, Loss=1.81732e-01, channel_BER=0.0653125, BER=0.3121875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 23500/1000000, LR=4.99e-04, Loss=1.81738e-01, channel_BER=0.0728125, BER=0.3946875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 23550/1000000, LR=4.99e-04, Loss=1.76804e-01, channel_BER=0.0728125, BER=0.3259375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 23600/1000000, LR=4.99e-04, Loss=1.76982e-01, channel_BER=0.0728125, BER=0.320625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 23650/1000000, LR=4.99e-04, Loss=1.95216e-01, channel_BER=0.0740625, BER=0.3353125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 23700/1000000, LR=4.99e-04, Loss=2.58478e-01, channel_BER=0.0746875, BER=0.4915625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 23750/1000000, LR=4.99e-04, Loss=2.29370e-01, channel_BER=0.0778125, BER=0.499375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 23800/1000000, LR=4.99e-04, Loss=2.06802e-01, channel_BER=0.0796875, BER=0.43875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 23850/1000000, LR=4.99e-04, Loss=2.01890e-01, channel_BER=0.0746875, BER=0.3909375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 23900/1000000, LR=4.99e-04, Loss=4.72136e-01, channel_BER=0.071875, BER=0.490625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 23950/1000000, LR=4.99e-04, Loss=4.14458e-01, channel_BER=0.074375, BER=0.48375, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 24000/1000000, LR=4.99e-04, Loss=3.57591e-01, channel_BER=0.071875, BER=0.47625, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 24050/1000000, LR=4.99e-04, Loss=3.36008e-01, channel_BER=0.0771875, BER=0.4771875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 24100/1000000, LR=4.99e-04, Loss=2.37299e-01, channel_BER=0.065, BER=0.46375, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 24150/1000000, LR=4.99e-04, Loss=2.19513e-01, channel_BER=0.0703125, BER=0.3265625, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 24200/1000000, LR=4.99e-04, Loss=2.32358e-01, channel_BER=0.073125, BER=0.298125, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 24250/1000000, LR=4.99e-04, Loss=2.11564e-01, channel_BER=0.069375, BER=0.29125, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 24300/1000000, LR=4.99e-04, Loss=1.88747e-01, channel_BER=0.06625, BER=0.30875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 24350/1000000, LR=4.99e-04, Loss=1.94892e-01, channel_BER=0.069375, BER=0.3246875, BLER=1.0 duration per call: 0.01s\n",
            "Training epoch 24400/1000000, LR=4.99e-04, Loss=2.11313e-01, channel_BER=0.078125, BER=0.3646875, BLER=1.0 duration per call: 0.01s\n",
            "Saved weights to /content/drive/My Drive/ECC_weights/ECCT_5G_pcm_11_24_Tlayers1_dims64/weights-1\n",
            "Training epoch 24450/1000000, LR=4.99e-04, Loss=4.09534e-01, channel_BER=0.06625, BER=0.5034375, BLER=1.0 duration per call: 0.01s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ber_plot = PlotBER(f\"Transformer-based Decoding - LDPC, (k,n)=({e2e_ltd._k},{e2e_ltd._n})\")\n",
        "ebno_dbs = np.arange(args.ebno_db_min,\n",
        "                     args.ebno_db_max,\n",
        "                     args.ebno_db_stepsize)"
      ],
      "metadata": {
        "id": "vQLjPlH9FB2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and run the BER simulations\n",
        "ber_plot.simulate(e2e_ltd,\n",
        "                  ebno_dbs=ebno_dbs,\n",
        "                  batch_size=100,#args.mc_batch_size,\n",
        "                  num_target_block_errors=500,\n",
        "                  legend=f\"Transformer dims={e2e_ltd._decoder.dims} n={e2e_ltd._n}\",\n",
        "                  soft_estimates=False,\n",
        "                  max_mc_iter=100,#args.mc_iters,\n",
        "                  forward_keyboard_interrupt=False,\n",
        "                  show_fig=True);"
      ],
      "metadata": {
        "id": "V4p44uj6BC48"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}